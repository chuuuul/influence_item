#!/usr/bin/env python3
"""
í™˜ê²½ ì„¤ì • ìë™í™” ìŠ¤í¬ë¦½íŠ¸
T10_S01_M04: Environment Setup Automation

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ê°œë°œ, í…ŒìŠ¤íŠ¸, í”„ë¡œë•ì…˜ í™˜ê²½ì„ ìë™ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
"""

import os
import sys
import json
import shutil
import subprocess
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import secrets
import string

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class EnvironmentConfig:
    """í™˜ê²½ ì„¤ì •"""
    name: str
    database_url: str
    redis_url: str
    debug: bool
    log_level: str
    api_rate_limit: int
    cache_ttl: int
    backup_enabled: bool
    monitoring_enabled: bool
    ssl_enabled: bool


class EnvironmentSetup:
    """í™˜ê²½ ì„¤ì • ìë™í™”"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.project_root = project_root
        self.environments = {
            "development": EnvironmentConfig(
                name="development",
                database_url="sqlite:///influence_item_dev.db",
                redis_url="redis://localhost:6379/0",
                debug=True,
                log_level="DEBUG",
                api_rate_limit=1000,
                cache_ttl=300,
                backup_enabled=False,
                monitoring_enabled=False,
                ssl_enabled=False
            ),
            "testing": EnvironmentConfig(
                name="testing",
                database_url="sqlite:///influence_item_test.db",
                redis_url="redis://localhost:6379/1",
                debug=False,
                log_level="INFO",
                api_rate_limit=500,
                cache_ttl=600,
                backup_enabled=True,
                monitoring_enabled=True,
                ssl_enabled=False
            ),
            "production": EnvironmentConfig(
                name="production",
                database_url="postgresql://user:pass@localhost:5432/influence_item",
                redis_url="redis://localhost:6379/2",
                debug=False,
                log_level="WARNING",
                api_rate_limit=100,
                cache_ttl=3600,
                backup_enabled=True,
                monitoring_enabled=True,
                ssl_enabled=True
            )
        }
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬
        self.required_directories = [
            "logs",
            "temp",
            "backups",
            "ssl",
            "screenshots",
            "channel_discovery_results",
            "daily_reports",
            "credentials"
        ]
        
        # í™˜ê²½ë³„ ì„¤ì • íŒŒì¼ í…œí”Œë¦¿
        self.config_templates = {
            ".env": self._get_env_template(),
            "docker-compose.override.yml": self._get_docker_override_template(),
            "nginx.conf": self._get_nginx_template(),
            "monitoring.yml": self._get_monitoring_template()
        }
    
    def setup_environment(self, env_name: str) -> bool:
        """íŠ¹ì • í™˜ê²½ ì„¤ì •"""
        try:
            if env_name not in self.environments:
                raise ValueError(f"Unknown environment: {env_name}")
            
            config = self.environments[env_name]
            logger.info(f"ğŸš€ {env_name} í™˜ê²½ ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # 1. ë””ë ‰í† ë¦¬ ìƒì„±
            self._create_directories()
            
            # 2. í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±
            self._create_env_files(config)
            
            # 3. ì„¤ì • íŒŒì¼ ìƒì„±
            self._create_config_files(config)
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            self._initialize_database(config)
            
            # 5. SSL ì¸ì¦ì„œ ì„¤ì • (í”„ë¡œë•ì…˜ë§Œ)
            if config.ssl_enabled:
                self._setup_ssl_certificates()
            
            # 6. ëª¨ë‹ˆí„°ë§ ì„¤ì •
            if config.monitoring_enabled:
                self._setup_monitoring(config)
            
            # 7. ë°±ì—… ì„¤ì •
            if config.backup_enabled:
                self._setup_backup_system(config)
            
            # 8. ì„œë¹„ìŠ¤ ì„¤ì • ê²€ì¦
            self._validate_setup(config)
            
            logger.info(f"âœ… {env_name} í™˜ê²½ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
            
        except Exception as e:
            logger.error(f"âŒ {env_name} í™˜ê²½ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _create_directories(self):
        """í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„±"""
        logger.info("ğŸ“ í•„ìˆ˜ ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘...")
        
        for directory in self.required_directories:
            dir_path = self.project_root / directory
            dir_path.mkdir(exist_ok=True, parents=True)
            logger.debug(f"ë””ë ‰í† ë¦¬ ìƒì„±: {dir_path}")
        
        # íŠ¹ë³„í•œ ê¶Œí•œì´ í•„ìš”í•œ ë””ë ‰í† ë¦¬
        sensitive_dirs = ["credentials", "ssl", "backups"]
        for directory in sensitive_dirs:
            dir_path = self.project_root / directory
            if dir_path.exists():
                os.chmod(dir_path, 0o700)  # rwx------
    
    def _create_env_files(self, config: EnvironmentConfig):
        """í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ”§ í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ .env íŒŒì¼
        env_content = self._generate_env_content(config)
        env_path = self.project_root / f".env.{config.name}"
        
        with open(env_path, 'w') as f:
            f.write(env_content)
        
        # ë©”ì¸ .env íŒŒì¼ì— ë§í¬ (ê°œë°œ í™˜ê²½ë§Œ)
        if config.name == "development":
            main_env_path = self.project_root / ".env"
            if not main_env_path.exists():
                shutil.copy2(env_path, main_env_path)
        
        # íŒŒì¼ ê¶Œí•œ ì„¤ì •
        os.chmod(env_path, 0o600)  # rw-------
        
        logger.info(f"í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ ìƒì„±: {env_path}")
    
    def _generate_env_content(self, config: EnvironmentConfig) -> str:
        """í™˜ê²½ ë³€ìˆ˜ ë‚´ìš© ìƒì„±"""
        # API í‚¤ ìƒì„± (ì‹¤ì œë¡œëŠ” ì‹¤ì œ ê°’ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•¨)
        api_keys = {
            "YOUTUBE_API_KEY": self._generate_secure_key(40),
            "OPENAI_API_KEY": self._generate_secure_key(32),
            "GOOGLE_API_KEY": self._generate_secure_key(39),
            "SLACK_BOT_TOKEN": self._generate_secure_key(55),
            "SLACK_WEBHOOK_URL": f"https://hooks.slack.com/services/{self._generate_secure_key(11)}/{self._generate_secure_key(11)}/{self._generate_secure_key(24)}"
        }
        
        content = f"""# {config.name.upper()} Environment Configuration
# Generated on {datetime.now().isoformat()}

# Environment
ENVIRONMENT={config.name}
DEBUG={str(config.debug).lower()}
LOG_LEVEL={config.log_level}

# Database
DATABASE_URL={config.database_url}
DB_POOL_SIZE=20
DB_TIMEOUT=30

# Cache
REDIS_URL={config.redis_url}
CACHE_TTL={config.cache_ttl}
CACHE_MAX_CONNECTIONS=100

# API Configuration
API_RATE_LIMIT={config.api_rate_limit}
API_TIMEOUT=30
MAX_CONCURRENT_REQUESTS=100

# Security
SECRET_KEY={self._generate_secure_key(32)}
JWT_SECRET={self._generate_secure_key(32)}
ENCRYPTION_KEY={self._generate_secure_key(32)}

# API Keys (Replace with actual values)
{chr(10).join(f'{key}={value}' for key, value in api_keys.items())}

# Google Sheets
GOOGLE_SHEETS_SPREADSHEET_ID=your_spreadsheet_id_here
GOOGLE_APPLICATION_CREDENTIALS=./credentials/google_sheets_credentials.json

# Monitoring
MONITORING_ENABLED={str(config.monitoring_enabled).lower()}
HEALTH_CHECK_INTERVAL=30
METRICS_COLLECTION_INTERVAL=60

# Backup
BACKUP_ENABLED={str(config.backup_enabled).lower()}
BACKUP_RETENTION_DAYS=7
BACKUP_SCHEDULE=0 3 * * *

# SSL
SSL_ENABLED={str(config.ssl_enabled).lower()}
SSL_CERT_PATH=./ssl/cert.pem
SSL_KEY_PATH=./ssl/key.pem

# Performance
WORKER_PROCESSES=4
WORKER_THREADS=2
MAX_MEMORY_PER_WORKER=512MB

# Feature Flags
ENABLE_GPU_ACCELERATION=true
ENABLE_ADVANCED_FILTERING=true
ENABLE_REAL_TIME_MONITORING=true
ENABLE_AUTO_SCALING=false
"""
        return content
    
    def _generate_secure_key(self, length: int) -> str:
        """ì•ˆì „í•œ í‚¤ ìƒì„±"""
        alphabet = string.ascii_letters + string.digits
        return ''.join(secrets.choice(alphabet) for _ in range(length))
    
    def _create_config_files(self, config: EnvironmentConfig):
        """ì„¤ì • íŒŒì¼ ìƒì„±"""
        logger.info("ğŸ“„ ì„¤ì • íŒŒì¼ ìƒì„± ì¤‘...")
        
        # Docker Compose override íŒŒì¼
        self._create_docker_override(config)
        
        # Nginx ì„¤ì • íŒŒì¼
        if config.ssl_enabled:
            self._create_nginx_config(config)
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì • íŒŒì¼
        if config.monitoring_enabled:
            self._create_monitoring_config(config)
        
        # ë¡œê¹… ì„¤ì • íŒŒì¼
        self._create_logging_config(config)
    
    def _create_docker_override(self, config: EnvironmentConfig):
        """Docker Compose override íŒŒì¼ ìƒì„±"""
        override_content = f"""version: '3.8'

services:
  cpu-server:
    environment:
      - ENVIRONMENT={config.name}
      - DEBUG={str(config.debug).lower()}
      - LOG_LEVEL={config.log_level}
    {"restart: unless-stopped" if config.name == "production" else ""}
    
  gpu-server:
    environment:
      - ENVIRONMENT={config.name}
      - DEBUG={str(config.debug).lower()}
      - LOG_LEVEL={config.log_level}
    {"restart: unless-stopped" if config.name == "production" else ""}
    
  {"redis:" if config.name == "production" else "# redis:"}
  {"  image: redis:7-alpine" if config.name == "production" else ""}
  {"  restart: unless-stopped" if config.name == "production" else ""}
  {"  ports:" if config.name == "production" else ""}
  {"    - '6379:6379'" if config.name == "production" else ""}
  {"  volumes:" if config.name == "production" else ""}
  {"    - redis_data:/data" if config.name == "production" else ""}

{"volumes:" if config.name == "production" else ""}
{"  redis_data:" if config.name == "production" else ""}
"""
        
        override_path = self.project_root / f"docker-compose.{config.name}.yml"
        with open(override_path, 'w') as f:
            f.write(override_content)
    
    def _create_nginx_config(self, config: EnvironmentConfig):
        """Nginx ì„¤ì • íŒŒì¼ ìƒì„±"""
        nginx_content = f"""events {{
    worker_connections 1024;
}}

http {{
    upstream cpu_backend {{
        server cpu-server:8501;
    }}
    
    upstream gpu_backend {{
        server gpu-server:8001;
    }}
    
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate={config.api_rate_limit}r/h;
    
    server {{
        listen 80;
        server_name localhost;
        
        {"# Redirect HTTP to HTTPS" if config.ssl_enabled else ""}
        {"return 301 https://$server_name$request_uri;" if config.ssl_enabled else ""}
    }}
    
    {"server {" if config.ssl_enabled else ""}
    {"    listen 443 ssl http2;" if config.ssl_enabled else ""}
    {"    server_name localhost;" if config.ssl_enabled else ""}
    {"" if config.ssl_enabled else ""}
    {"    ssl_certificate /etc/nginx/ssl/cert.pem;" if config.ssl_enabled else ""}
    {"    ssl_certificate_key /etc/nginx/ssl/key.pem;" if config.ssl_enabled else ""}
    {"    ssl_protocols TLSv1.2 TLSv1.3;" if config.ssl_enabled else ""}
    {"    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;" if config.ssl_enabled else ""}
    {"" if config.ssl_enabled else ""}
        # Security headers
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        {"add_header Strict-Transport-Security 'max-age=31536000; includeSubDomains; preload';" if config.ssl_enabled else ""}
        
        # API endpoints
        location /api/ {{
            limit_req zone=api burst=20 nodelay;
            proxy_pass http://gpu_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            {"proxy_set_header X-Forwarded-Proto https;" if config.ssl_enabled else ""}
        }}
        
        # Dashboard
        location / {{
            proxy_pass http://cpu_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            {"proxy_set_header X-Forwarded-Proto https;" if config.ssl_enabled else ""}
            
            # WebSocket support for Streamlit
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_read_timeout 86400;
        }}
    {"}" if config.ssl_enabled else ""}
}}
"""
        
        nginx_path = self.project_root / f"nginx.{config.name}.conf"
        with open(nginx_path, 'w') as f:
            f.write(nginx_content)
    
    def _create_monitoring_config(self, config: EnvironmentConfig):
        """ëª¨ë‹ˆí„°ë§ ì„¤ì • íŒŒì¼ ìƒì„±"""
        monitoring_content = f"""# Monitoring Configuration for {config.name}
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'influence-item-cpu'
    static_configs:
      - targets: ['cpu-server:8501']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'influence-item-gpu'
    static_configs:
      - targets: ['gpu-server:8001']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# Alert rules
groups:
  - name: influence-item-alerts
    rules:
      - alert: HighCPUUsage
        expr: cpu_usage_percent > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          
      - alert: HighMemoryUsage
        expr: memory_usage_percent > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
"""
        
        monitoring_path = self.project_root / f"monitoring.{config.name}.yml"
        with open(monitoring_path, 'w') as f:
            f.write(monitoring_content)
    
    def _create_logging_config(self, config: EnvironmentConfig):
        """ë¡œê¹… ì„¤ì • íŒŒì¼ ìƒì„±"""
        logging_config = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "standard": {
                    "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
                },
                "detailed": {
                    "format": "%(asctime)s [%(levelname)s] %(name)s [%(filename)s:%(lineno)d]: %(message)s"
                }
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "level": config.log_level,
                    "formatter": "standard",
                    "stream": "ext://sys.stdout"
                },
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "level": config.log_level,
                    "formatter": "detailed",
                    "filename": f"logs/{config.name}.log",
                    "maxBytes": 10485760,  # 10MB
                    "backupCount": 5
                }
            },
            "loggers": {
                "": {
                    "level": config.log_level,
                    "handlers": ["console", "file"],
                    "propagate": False
                }
            }
        }
        
        logging_path = self.project_root / f"logging.{config.name}.json"
        with open(logging_path, 'w') as f:
            json.dump(logging_config, f, indent=2)
    
    def _initialize_database(self, config: EnvironmentConfig):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        try:
            # SQLite ë°ì´í„°ë² ì´ìŠ¤ì¸ ê²½ìš°
            if "sqlite" in config.database_url:
                db_path = config.database_url.split("///")[1]
                db_file = self.project_root / db_path
                
                # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
                if not db_file.exists():
                    db_file.touch()
                    logger.info(f"SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±: {db_file}")
                
                # ê¸°ë³¸ í…Œì´ë¸” ìƒì„± (ì‹¤ì œë¡œëŠ” ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰)
                # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”ëœ ë²„ì „ìœ¼ë¡œ êµ¬í˜„
                
            # PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ì¸ ê²½ìš°
            elif "postgresql" in config.database_url:
                logger.info("PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸...")
                # ì‹¤ì œë¡œëŠ” psycopg2ë‚˜ SQLAlchemyë¥¼ ì‚¬ìš©í•˜ì—¬ ì—°ê²° í…ŒìŠ¤íŠ¸
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _setup_ssl_certificates(self):
        """SSL ì¸ì¦ì„œ ì„¤ì •"""
        logger.info("ğŸ”’ SSL ì¸ì¦ì„œ ì„¤ì • ì¤‘...")
        
        ssl_dir = self.project_root / "ssl"
        cert_path = ssl_dir / "cert.pem"
        key_path = ssl_dir / "key.pem"
        
        # ìì²´ ì„œëª… ì¸ì¦ì„œ ìƒì„± (ê°œë°œìš©)
        if not cert_path.exists() or not key_path.exists():
            try:
                subprocess.run([
                    "openssl", "req", "-x509", "-newkey", "rsa:4096",
                    "-keyout", str(key_path),
                    "-out", str(cert_path),
                    "-days", "365",
                    "-nodes",
                    "-subj", "/C=KR/ST=Seoul/L=Seoul/O=InfluenceItem/OU=IT/CN=localhost"
                ], check=True, capture_output=True)
                
                # íŒŒì¼ ê¶Œí•œ ì„¤ì •
                os.chmod(cert_path, 0o644)
                os.chmod(key_path, 0o600)
                
                logger.info("ìì²´ ì„œëª… SSL ì¸ì¦ì„œ ìƒì„± ì™„ë£Œ")
                
            except subprocess.CalledProcessError as e:
                logger.error(f"SSL ì¸ì¦ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
        else:
            logger.info("ê¸°ì¡´ SSL ì¸ì¦ì„œ ì‚¬ìš©")
    
    def _setup_monitoring(self, config: EnvironmentConfig):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì •"""
        logger.info("ğŸ“Š ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        
        # Prometheus ì„¤ì • íŒŒì¼ ìƒì„±
        prometheus_config = f"""global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'influence-item'
    static_configs:
      - targets: ['localhost:8501', 'localhost:8001']
    scrape_interval: 30s
"""
        
        prometheus_path = self.project_root / "prometheus.yml"
        with open(prometheus_path, 'w') as f:
            f.write(prometheus_config)
        
        # Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì • ìƒì„±
        grafana_dashboard = {
            "dashboard": {
                "title": f"Influence Item - {config.name.title()}",
                "panels": [
                    {
                        "title": "CPU Usage",
                        "type": "graph",
                        "targets": [{"expr": "cpu_usage_percent"}]
                    },
                    {
                        "title": "Memory Usage",
                        "type": "graph",
                        "targets": [{"expr": "memory_usage_percent"}]
                    },
                    {
                        "title": "Request Rate",
                        "type": "graph",
                        "targets": [{"expr": "http_requests_per_second"}]
                    }
                ]
            }
        }
        
        grafana_path = self.project_root / f"grafana-dashboard-{config.name}.json"
        with open(grafana_path, 'w') as f:
            json.dump(grafana_dashboard, f, indent=2)
    
    def _setup_backup_system(self, config: EnvironmentConfig):
        """ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì •"""
        logger.info("ğŸ’¾ ë°±ì—… ì‹œìŠ¤í…œ ì„¤ì • ì¤‘...")
        
        # ë°±ì—… ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        backup_script = f"""#!/bin/bash
# Backup script for {config.name} environment
# Generated on {datetime.now().isoformat()}

set -euo pipefail

BACKUP_DIR="{self.project_root}/backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/backup_$DATE.tar.gz"

echo "Starting backup for {config.name} environment..."

# Create backup directory
mkdir -p "$BACKUP_DIR"

# Database backup
if [[ "{config.database_url}" == sqlite* ]]; then
    DB_FILE=$(echo "{config.database_url}" | sed 's/sqlite:\/\/\///')
    if [[ -f "$DB_FILE" ]]; then
        cp "$DB_FILE" "$BACKUP_DIR/database_$DATE.db"
        echo "Database backup completed"
    fi
fi

# Configuration backup
tar -czf "$BACKUP_FILE" \\
    --exclude="*.log" \\
    --exclude="temp/*" \\
    --exclude="node_modules/*" \\
    --exclude="venv/*" \\
    --exclude=".git/*" \\
    config/ \\
    credentials/ \\
    ssl/ \\
    .env.{config.name} \\
    2>/dev/null || true

echo "Backup completed: $BACKUP_FILE"

# Cleanup old backups (keep last 7 days)
find "$BACKUP_DIR" -name "backup_*.tar.gz" -mtime +7 -delete 2>/dev/null || true

echo "Old backups cleaned up"
"""
        
        backup_script_path = self.project_root / f"backup_{config.name}.sh"
        with open(backup_script_path, 'w') as f:
            f.write(backup_script)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        os.chmod(backup_script_path, 0o755)
        
        # Cron ì‘ì—… ì„¤ì • (í”„ë¡œë•ì…˜ë§Œ)
        if config.name == "production":
            crontab_entry = f"0 3 * * * {backup_script_path}\n"
            crontab_path = self.project_root / "crontab.txt"
            with open(crontab_path, 'a') as f:
                f.write(crontab_entry)
    
    def _validate_setup(self, config: EnvironmentConfig):
        """ì„¤ì • ê²€ì¦"""
        logger.info("âœ… ì„¤ì • ê²€ì¦ ì¤‘...")
        
        validations = []
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = [
            f".env.{config.name}",
            f"docker-compose.{config.name}.yml",
            f"logging.{config.name}.json"
        ]
        
        for file_name in required_files:
            file_path = self.project_root / file_name
            if file_path.exists():
                validations.append(f"âœ… {file_name} ì¡´ì¬")
            else:
                validations.append(f"âŒ {file_name} ëˆ„ë½")
        
        # SSL ì¸ì¦ì„œ í™•ì¸ (í•„ìš”í•œ ê²½ìš°)
        if config.ssl_enabled:
            ssl_files = ["ssl/cert.pem", "ssl/key.pem"]
            for ssl_file in ssl_files:
                ssl_path = self.project_root / ssl_file
                if ssl_path.exists():
                    validations.append(f"âœ… {ssl_file} ì¡´ì¬")
                else:
                    validations.append(f"âŒ {ssl_file} ëˆ„ë½")
        
        # ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
        sensitive_dirs = ["credentials", "ssl", "backups"]
        for directory in sensitive_dirs:
            dir_path = self.project_root / directory
            if dir_path.exists():
                stat = dir_path.stat()
                mode = oct(stat.st_mode)[-3:]
                if mode == "700":
                    validations.append(f"âœ… {directory} ê¶Œí•œ ì˜¬ë°”ë¦„")
                else:
                    validations.append(f"âš ï¸ {directory} ê¶Œí•œ í™•ì¸ í•„ìš”: {mode}")
        
        for validation in validations:
            logger.info(validation)
    
    def _get_env_template(self):
        """í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿"""
        return "# Environment template will be generated dynamically"
    
    def _get_docker_override_template(self):
        """Docker override í…œí”Œë¦¿"""
        return "# Docker override template will be generated dynamically"
    
    def _get_nginx_template(self):
        """Nginx í…œí”Œë¦¿"""
        return "# Nginx template will be generated dynamically"
    
    def _get_monitoring_template(self):
        """ëª¨ë‹ˆí„°ë§ í…œí”Œë¦¿"""
        return "# Monitoring template will be generated dynamically"
    
    def setup_all_environments(self) -> Dict[str, bool]:
        """ëª¨ë“  í™˜ê²½ ì„¤ì •"""
        results = {}
        
        for env_name in self.environments.keys():
            logger.info(f"ğŸ”§ {env_name} í™˜ê²½ ì„¤ì • ì¤‘...")
            results[env_name] = self.setup_environment(env_name)
        
        return results
    
    def cleanup_environment(self, env_name: str):
        """í™˜ê²½ ì •ë¦¬"""
        logger.info(f"ğŸ§¹ {env_name} í™˜ê²½ ì •ë¦¬ ì¤‘...")
        
        files_to_remove = [
            f".env.{env_name}",
            f"docker-compose.{env_name}.yml",
            f"nginx.{env_name}.conf",
            f"monitoring.{env_name}.yml",
            f"logging.{env_name}.json",
            f"grafana-dashboard-{env_name}.json",
            f"backup_{env_name}.sh"
        ]
        
        for file_name in files_to_remove:
            file_path = self.project_root / file_name
            if file_path.exists():
                file_path.unlink()
                logger.info(f"ì‚­ì œë¨: {file_name}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í™˜ê²½ ì„¤ì • ìë™í™”")
    parser.add_argument("action", choices=["setup", "cleanup", "all"], help="ì‹¤í–‰í•  ì•¡ì…˜")
    parser.add_argument("--env", choices=["development", "testing", "production"], help="ëŒ€ìƒ í™˜ê²½")
    
    args = parser.parse_args()
    
    setup = EnvironmentSetup()
    
    if args.action == "setup":
        if args.env:
            success = setup.setup_environment(args.env)
            return 0 if success else 1
        else:
            print("âŒ --env ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")
            return 1
    
    elif args.action == "cleanup":
        if args.env:
            setup.cleanup_environment(args.env)
            return 0
        else:
            print("âŒ --env ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤")
            return 1
    
    elif args.action == "all":
        results = setup.setup_all_environments()
        success_count = sum(results.values())
        total_count = len(results)
        
        print(f"\nğŸ“Š ê²°ê³¼: {success_count}/{total_count} í™˜ê²½ ì„¤ì • ì™„ë£Œ")
        for env, success in results.items():
            status = "âœ…" if success else "âŒ"
            print(f"  {status} {env}")
        
        return 0 if success_count == total_count else 1


if __name__ == "__main__":
    sys.exit(main())