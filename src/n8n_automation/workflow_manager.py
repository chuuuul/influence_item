#!/usr/bin/env python3
"""
n8n ì›Œí¬í”Œë¡œìš° ì™„ì „ ìë™í™” ë° ì—ëŸ¬ ë³µêµ¬ ì‹œìŠ¤í…œ

ì›Œí¬í”Œë¡œìš° ëª¨ë‹ˆí„°ë§, ìë™ ë³µêµ¬, ì„±ëŠ¥ ìµœì í™”, ìŠ¤ì¼€ì¤„ë§ ê´€ë¦¬
"""

import os
import json
import asyncio
import logging
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path

import requests
from src.notifications.alert_manager import send_warning_alert, send_critical_alert, send_info_alert
from src.config.config import Config

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WorkflowStatus(Enum):
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    ACTIVE = "active"
    INACTIVE = "inactive"
    ERROR = "error"
    WAITING = "waiting"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"

class WorkflowType(Enum):
    """ì›Œí¬í”Œë¡œìš° íƒ€ì…"""
    RSS_COLLECTION = "rss_collection"
    CHANNEL_DISCOVERY = "channel_discovery"
    AI_ANALYSIS = "ai_analysis"
    HISTORICAL_ANALYSIS = "historical_analysis"
    NOTIFICATION = "notification"
    MONITORING = "monitoring"
    BACKUP = "backup"

@dataclass
class WorkflowExecution:
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë°ì´í„°"""
    workflow_id: str
    execution_id: str
    status: WorkflowStatus
    start_time: datetime
    end_time: Optional[datetime]
    duration: Optional[float]
    error_message: Optional[str]
    input_data: Dict[str, Any]
    output_data: Dict[str, Any]
    retry_count: int = 0
    
    @property
    def is_successful(self) -> bool:
        return self.status == WorkflowStatus.SUCCESS
    
    @property
    def is_failed(self) -> bool:
        return self.status == WorkflowStatus.FAILED
    
    @property
    def is_running(self) -> bool:
        return self.status in [WorkflowStatus.RUNNING, WorkflowStatus.WAITING]

@dataclass
class WorkflowMetrics:
    """ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­"""
    workflow_id: str
    workflow_name: str
    workflow_type: WorkflowType
    total_executions: int
    successful_executions: int
    failed_executions: int
    average_duration: float
    last_execution: Optional[datetime]
    last_success: Optional[datetime]
    last_failure: Optional[datetime]
    success_rate: float
    
    @property
    def failure_rate(self) -> float:
        return 100.0 - self.success_rate

class N8nAPIClient:
    """n8n API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.base_url = os.getenv("N8N_API_URL", "http://n8n:5678")
        self.auth_token = os.getenv("N8N_API_TOKEN")
        self.username = os.getenv("N8N_USER")
        self.password = os.getenv("N8N_PASSWORD")
        self.session = None
    
    async def get_session(self) -> aiohttp.ClientSession:
        """HTTP ì„¸ì…˜ ìƒì„±"""
        if self.session is None:
            headers = {}
            
            if self.auth_token:
                headers["Authorization"] = f"Bearer {self.auth_token}"
            
            self.session = aiohttp.ClientSession(
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=30)
            )
            
            # Basic Auth ë¡œê·¸ì¸ (í† í°ì´ ì—†ëŠ” ê²½ìš°)
            if not self.auth_token and self.username and self.password:
                await self.login()
        
        return self.session
    
    async def login(self):
        """n8n ë¡œê·¸ì¸"""
        try:
            login_data = {
                "email": self.username,
                "password": self.password
            }
            
            async with self.session.post(f"{self.base_url}/rest/login", json=login_data) as response:
                if response.status == 200:
                    logger.info("n8n ë¡œê·¸ì¸ ì„±ê³µ")
                else:
                    logger.error(f"n8n ë¡œê·¸ì¸ ì‹¤íŒ¨: {response.status}")
        except Exception as e:
            logger.error(f"n8n ë¡œê·¸ì¸ ì˜ˆì™¸: {e}")
    
    async def get_workflows(self) -> List[Dict[str, Any]]:
        """ì›Œí¬í”Œë¡œìš° ëª©ë¡ ì¡°íšŒ"""
        try:
            session = await self.get_session()
            async with session.get(f"{self.base_url}/rest/workflows") as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("data", [])
                else:
                    logger.error(f"ì›Œí¬í”Œë¡œìš° ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ëª©ë¡ ì¡°íšŒ ì˜ˆì™¸: {e}")
            return []
    
    async def get_workflow_executions(self, workflow_id: str, limit: int = 100) -> List[Dict[str, Any]]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë‚´ì—­ ì¡°íšŒ"""
        try:
            session = await self.get_session()
            params = {
                "filter": json.dumps({"workflowId": workflow_id}),
                "limit": limit,
                "includeData": "true"
            }
            
            async with session.get(f"{self.base_url}/rest/executions", params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return data.get("data", [])
                else:
                    logger.error(f"ì‹¤í–‰ ë‚´ì—­ ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"ì‹¤í–‰ ë‚´ì—­ ì¡°íšŒ ì˜ˆì™¸: {e}")
            return []
    
    async def trigger_workflow(self, workflow_id: str, input_data: Dict[str, Any] = None) -> Optional[str]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ íŠ¸ë¦¬ê±°"""
        try:
            session = await self.get_session()
            
            trigger_data = input_data or {}
            
            async with session.post(
                f"{self.base_url}/rest/workflows/{workflow_id}/activate",
                json=trigger_data
            ) as response:
                if response.status in [200, 201]:
                    data = await response.json()
                    return data.get("data", {}).get("id")
                else:
                    logger.error(f"ì›Œí¬í”Œë¡œìš° íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {response.status}")
                    return None
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° íŠ¸ë¦¬ê±° ì˜ˆì™¸: {e}")
            return None
    
    async def stop_execution(self, execution_id: str) -> bool:
        """ì‹¤í–‰ ì¤‘ì§€"""
        try:
            session = await self.get_session()
            
            async with session.post(f"{self.base_url}/rest/executions/{execution_id}/stop") as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"ì‹¤í–‰ ì¤‘ì§€ ì˜ˆì™¸: {e}")
            return False
    
    async def retry_execution(self, execution_id: str) -> Optional[str]:
        """ì‹¤í–‰ ì¬ì‹œë„"""
        try:
            session = await self.get_session()
            
            async with session.post(f"{self.base_url}/rest/executions/{execution_id}/retry") as response:
                if response.status in [200, 201]:
                    data = await response.json()
                    return data.get("data", {}).get("id")
                else:
                    logger.error(f"ì‹¤í–‰ ì¬ì‹œë„ ì‹¤íŒ¨: {response.status}")
                    return None
        except Exception as e:
            logger.error(f"ì‹¤í–‰ ì¬ì‹œë„ ì˜ˆì™¸: {e}")
            return None
    
    async def get_workflow_status(self, workflow_id: str) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ"""
        try:
            session = await self.get_session()
            
            async with session.get(f"{self.base_url}/rest/workflows/{workflow_id}") as response:
                if response.status == 200:
                    return await response.json()
                else:
                    logger.error(f"ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {response.status}")
                    return {}
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì¡°íšŒ ì˜ˆì™¸: {e}")
            return {}
    
    async def activate_workflow(self, workflow_id: str) -> bool:
        """ì›Œí¬í”Œë¡œìš° í™œì„±í™”"""
        try:
            session = await self.get_session()
            
            async with session.patch(
                f"{self.base_url}/rest/workflows/{workflow_id}",
                json={"active": True}
            ) as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° í™œì„±í™” ì˜ˆì™¸: {e}")
            return False
    
    async def deactivate_workflow(self, workflow_id: str) -> bool:
        """ì›Œí¬í”Œë¡œìš° ë¹„í™œì„±í™”"""
        try:
            session = await self.get_session()
            
            async with session.patch(
                f"{self.base_url}/rest/workflows/{workflow_id}",
                json={"active": False}
            ) as response:
                return response.status == 200
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ë¹„í™œì„±í™” ì˜ˆì™¸: {e}")
            return False
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self.session:
            await self.session.close()

class WorkflowMonitor:
    """ì›Œí¬í”Œë¡œìš° ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.client = N8nAPIClient()
        self.metrics_file = Path("data/workflow_metrics.json")
        self.metrics_file.parent.mkdir(exist_ok=True)
        
        # ì›Œí¬í”Œë¡œìš° íƒ€ì… ë§¤í•‘
        self.workflow_types = {
            "daily_rss_collection": WorkflowType.RSS_COLLECTION,
            "channel_discovery": WorkflowType.CHANNEL_DISCOVERY,
            "ai_analysis_pipeline": WorkflowType.AI_ANALYSIS,
            "historical_video_analysis": WorkflowType.HISTORICAL_ANALYSIS,
            "notification_routing": WorkflowType.NOTIFICATION,
            "monitoring_workflow": WorkflowType.MONITORING,
            "backup_workflow": WorkflowType.BACKUP
        }
    
    def load_metrics(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ"""
        if not self.metrics_file.exists():
            return {"workflows": {}, "executions": []}
        
        try:
            with open(self.metrics_file, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {"workflows": {}, "executions": []}
    
    def save_metrics(self, data: Dict[str, Any]):
        """ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥"""
        try:
            with open(self.metrics_file, 'w') as f:
                json.dump(data, f, indent=2, default=str)
        except Exception as e:
            logger.error(f"ë©”íŠ¸ë¦­ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def collect_workflow_metrics(self) -> List[WorkflowMetrics]:
        """ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        workflows = await self.client.get_workflows()
        metrics_list = []
        
        for workflow in workflows:
            workflow_id = workflow.get("id")
            workflow_name = workflow.get("name", "Unknown")
            
            if not workflow_id:
                continue
            
            # ì‹¤í–‰ ë‚´ì—­ ì¡°íšŒ
            executions = await self.client.get_workflow_executions(workflow_id)
            
            if not executions:
                continue
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            total_executions = len(executions)
            successful_executions = len([e for e in executions if e.get("finished") and not e.get("stoppedAt")])
            failed_executions = total_executions - successful_executions
            
            success_rate = (successful_executions / total_executions * 100) if total_executions > 0 else 0
            
            # í‰ê·  ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            durations = []
            for execution in executions:
                if execution.get("startedAt") and execution.get("stoppedAt"):
                    start = datetime.fromisoformat(execution["startedAt"].replace("Z", "+00:00"))
                    end = datetime.fromisoformat(execution["stoppedAt"].replace("Z", "+00:00"))
                    durations.append((end - start).total_seconds())
            
            average_duration = sum(durations) / len(durations) if durations else 0
            
            # ìµœê·¼ ì‹¤í–‰ ì‹œê°„
            last_execution = None
            last_success = None
            last_failure = None
            
            for execution in executions:
                exec_time = datetime.fromisoformat(execution["startedAt"].replace("Z", "+00:00"))
                
                if last_execution is None or exec_time > last_execution:
                    last_execution = exec_time
                
                if execution.get("finished") and not execution.get("stoppedAt"):
                    if last_success is None or exec_time > last_success:
                        last_success = exec_time
                else:
                    if last_failure is None or exec_time > last_failure:
                        last_failure = exec_time
            
            # ì›Œí¬í”Œë¡œìš° íƒ€ì… ê²°ì •
            workflow_type = WorkflowType.MONITORING  # ê¸°ë³¸ê°’
            for name_pattern, wf_type in self.workflow_types.items():
                if name_pattern.lower() in workflow_name.lower():
                    workflow_type = wf_type
                    break
            
            metrics = WorkflowMetrics(
                workflow_id=workflow_id,
                workflow_name=workflow_name,
                workflow_type=workflow_type,
                total_executions=total_executions,
                successful_executions=successful_executions,
                failed_executions=failed_executions,
                average_duration=average_duration,
                last_execution=last_execution,
                last_success=last_success,
                last_failure=last_failure,
                success_rate=success_rate
            )
            
            metrics_list.append(metrics)
        
        return metrics_list
    
    async def analyze_workflow_health(self, metrics: List[WorkflowMetrics]) -> List[Dict[str, Any]]:
        """ì›Œí¬í”Œë¡œìš° ê±´ê°• ìƒíƒœ ë¶„ì„"""
        issues = []
        
        for metric in metrics:
            # ì„±ê³µë¥  ì²´í¬
            if metric.success_rate < 50:
                issues.append({
                    "type": "low_success_rate",
                    "severity": "critical",
                    "workflow_id": metric.workflow_id,
                    "workflow_name": metric.workflow_name,
                    "success_rate": metric.success_rate,
                    "message": f"ì›Œí¬í”Œë¡œìš° ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤: {metric.success_rate:.1f}%"
                })
            elif metric.success_rate < 80:
                issues.append({
                    "type": "moderate_success_rate",
                    "severity": "warning",
                    "workflow_id": metric.workflow_id,
                    "workflow_name": metric.workflow_name,
                    "success_rate": metric.success_rate,
                    "message": f"ì›Œí¬í”Œë¡œìš° ì„±ê³µë¥ ì´ ë‚®ìŠµë‹ˆë‹¤: {metric.success_rate:.1f}%"
                })
            
            # ìµœê·¼ ì‹¤í–‰ ì²´í¬
            if metric.last_execution:
                time_since_last = datetime.now() - metric.last_execution.replace(tzinfo=None)
                if time_since_last > timedelta(hours=24):
                    issues.append({
                        "type": "stale_workflow",
                        "severity": "warning",
                        "workflow_id": metric.workflow_id,
                        "workflow_name": metric.workflow_name,
                        "last_execution": metric.last_execution,
                        "message": f"ì›Œí¬í”Œë¡œìš°ê°€ 24ì‹œê°„ ì´ìƒ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"
                    })
            
            # ì‹¤í–‰ ì‹œê°„ ì²´í¬
            if metric.average_duration > 300:  # 5ë¶„ ì´ìƒ
                issues.append({
                    "type": "slow_execution",
                    "severity": "info",
                    "workflow_id": metric.workflow_id,
                    "workflow_name": metric.workflow_name,
                    "average_duration": metric.average_duration,
                    "message": f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ìŠµë‹ˆë‹¤: {metric.average_duration:.1f}ì´ˆ"
                })
            
            # ìµœê·¼ ì‹¤íŒ¨ ì²´í¬
            if metric.last_failure and metric.last_success:
                if metric.last_failure > metric.last_success:
                    issues.append({
                        "type": "recent_failure",
                        "severity": "warning",
                        "workflow_id": metric.workflow_id,
                        "workflow_name": metric.workflow_name,
                        "last_failure": metric.last_failure,
                        "message": f"ìµœê·¼ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤"
                    })
        
        return issues

class WorkflowRecovery:
    """ì›Œí¬í”Œë¡œìš° ìë™ ë³µêµ¬"""
    
    def __init__(self):
        self.client = N8nAPIClient()
        self.recovery_strategies = {
            "low_success_rate": self.recover_low_success_rate,
            "stale_workflow": self.recover_stale_workflow,
            "recent_failure": self.recover_recent_failure,
            "execution_timeout": self.recover_execution_timeout
        }
        self.max_retry_attempts = 3
        self.retry_delay = 60  # ì´ˆ
    
    async def execute_recovery(self, issues: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """ë³µêµ¬ ì‹¤í–‰"""
        recovery_results = {
            "successful": [],
            "failed": [],
            "skipped": []
        }
        
        for issue in issues:
            issue_type = issue.get("type")
            severity = issue.get("severity", "info")
            
            # ì‹¬ê°ë„ê°€ ë‚®ì€ ì´ìŠˆëŠ” ê±´ë„ˆë›°ê¸°
            if severity == "info":
                recovery_results["skipped"].append(f"{issue['workflow_name']}: {issue['message']}")
                continue
            
            if issue_type in self.recovery_strategies:
                try:
                    success = await self.recovery_strategies[issue_type](issue)
                    if success:
                        recovery_results["successful"].append(f"{issue['workflow_name']}: {issue_type} ë³µêµ¬ ì„±ê³µ")
                    else:
                        recovery_results["failed"].append(f"{issue['workflow_name']}: {issue_type} ë³µêµ¬ ì‹¤íŒ¨")
                except Exception as e:
                    logger.error(f"ë³µêµ¬ ì‹¤í–‰ ì˜¤ë¥˜ ({issue_type}): {e}")
                    recovery_results["failed"].append(f"{issue['workflow_name']}: {issue_type} ë³µêµ¬ ì˜ˆì™¸ - {e}")
            else:
                recovery_results["skipped"].append(f"{issue['workflow_name']}: {issue_type} - ë³µêµ¬ ì „ëµ ì—†ìŒ")
        
        return recovery_results
    
    async def recover_low_success_rate(self, issue: Dict[str, Any]) -> bool:
        """ë‚®ì€ ì„±ê³µë¥  ë³µêµ¬"""
        workflow_id = issue["workflow_id"]
        
        try:
            # 1. ì›Œí¬í”Œë¡œìš° ë¹„í™œì„±í™”
            await self.client.deactivate_workflow(workflow_id)
            await asyncio.sleep(5)
            
            # 2. ë‹¤ì‹œ í™œì„±í™”
            success = await self.client.activate_workflow(workflow_id)
            
            if success:
                logger.info(f"ì›Œí¬í”Œë¡œìš° ì¬ì‹œì‘ ì™„ë£Œ: {issue['workflow_name']}")
                
                await send_info_alert(
                    "ì›Œí¬í”Œë¡œìš° ìë™ ë³µêµ¬",
                    f"ì„±ê³µë¥ ì´ ë‚®ì€ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¬ì‹œì‘í–ˆìŠµë‹ˆë‹¤: {issue['workflow_name']}",
                    service="n8n_automation",
                    workflow_id=workflow_id,
                    recovery_action="restart"
                )
                
                return True
            
        except Exception as e:
            logger.error(f"ë‚®ì€ ì„±ê³µë¥  ë³µêµ¬ ì‹¤íŒ¨: {e}")
        
        return False
    
    async def recover_stale_workflow(self, issue: Dict[str, Any]) -> bool:
        """ì˜¤ë˜ëœ ì›Œí¬í”Œë¡œìš° ë³µêµ¬"""
        workflow_id = issue["workflow_id"]
        
        try:
            # ì›Œí¬í”Œë¡œìš° ìƒíƒœ í™•ì¸
            status = await self.client.get_workflow_status(workflow_id)
            
            if not status.get("active", False):
                # ë¹„í™œì„±í™”ëœ ê²½ìš° í™œì„±í™”
                success = await self.client.activate_workflow(workflow_id)
                
                if success:
                    logger.info(f"ë¹„í™œì„±í™”ëœ ì›Œí¬í”Œë¡œìš° í™œì„±í™”: {issue['workflow_name']}")
                    
                    await send_info_alert(
                        "ì›Œí¬í”Œë¡œìš° ìë™ í™œì„±í™”",
                        f"ë¹„í™œì„±í™”ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ í™œì„±í™”í–ˆìŠµë‹ˆë‹¤: {issue['workflow_name']}",
                        service="n8n_automation",
                        workflow_id=workflow_id,
                        recovery_action="activate"
                    )
                    
                    return True
            else:
                # ìˆ˜ë™ íŠ¸ë¦¬ê±° ì‹œë„
                execution_id = await self.client.trigger_workflow(workflow_id)
                
                if execution_id:
                    logger.info(f"ì›Œí¬í”Œë¡œìš° ìˆ˜ë™ íŠ¸ë¦¬ê±° ì™„ë£Œ: {issue['workflow_name']}")
                    
                    await send_info_alert(
                        "ì›Œí¬í”Œë¡œìš° ìˆ˜ë™ íŠ¸ë¦¬ê±°",
                        f"ì˜¤ë˜ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ìˆ˜ë™ìœ¼ë¡œ íŠ¸ë¦¬ê±°í–ˆìŠµë‹ˆë‹¤: {issue['workflow_name']}",
                        service="n8n_automation",
                        workflow_id=workflow_id,
                        execution_id=execution_id,
                        recovery_action="manual_trigger"
                    )
                    
                    return True
        
        except Exception as e:
            logger.error(f"ì˜¤ë˜ëœ ì›Œí¬í”Œë¡œìš° ë³µêµ¬ ì‹¤íŒ¨: {e}")
        
        return False
    
    async def recover_recent_failure(self, issue: Dict[str, Any]) -> bool:
        """ìµœê·¼ ì‹¤íŒ¨ ë³µêµ¬"""
        workflow_id = issue["workflow_id"]
        
        try:
            # ìµœê·¼ ì‹¤í–‰ ë‚´ì—­ ì¡°íšŒ
            executions = await self.client.get_workflow_executions(workflow_id, limit=1)
            
            if executions:
                last_execution = executions[0]
                execution_id = last_execution.get("id")
                
                if execution_id and not last_execution.get("finished"):
                    # ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ì¤‘ì§€ í›„ ì¬ì‹œë„
                    await self.client.stop_execution(execution_id)
                    await asyncio.sleep(10)
                
                # ì¬ì‹œë„
                new_execution_id = await self.client.retry_execution(execution_id)
                
                if new_execution_id:
                    logger.info(f"ì›Œí¬í”Œë¡œìš° ì¬ì‹œë„ ì™„ë£Œ: {issue['workflow_name']}")
                    
                    await send_info_alert(
                        "ì›Œí¬í”Œë¡œìš° ìë™ ì¬ì‹œë„",
                        f"ì‹¤íŒ¨í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¬ì‹œë„í–ˆìŠµë‹ˆë‹¤: {issue['workflow_name']}",
                        service="n8n_automation",
                        workflow_id=workflow_id,
                        execution_id=new_execution_id,
                        recovery_action="retry"
                    )
                    
                    return True
        
        except Exception as e:
            logger.error(f"ìµœê·¼ ì‹¤íŒ¨ ë³µêµ¬ ì‹¤íŒ¨: {e}")
        
        return False
    
    async def recover_execution_timeout(self, issue: Dict[str, Any]) -> bool:
        """ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ë³µêµ¬"""
        workflow_id = issue["workflow_id"]
        execution_id = issue.get("execution_id")
        
        if not execution_id:
            return False
        
        try:
            # ì‹¤í–‰ ì¤‘ì§€
            stopped = await self.client.stop_execution(execution_id)
            
            if stopped:
                await asyncio.sleep(5)
                
                # ìƒˆë¡œìš´ ì‹¤í–‰ ì‹œì‘
                new_execution_id = await self.client.trigger_workflow(workflow_id)
                
                if new_execution_id:
                    logger.info(f"íƒ€ì„ì•„ì›ƒ ì›Œí¬í”Œë¡œìš° ë³µêµ¬ ì™„ë£Œ: {issue['workflow_name']}")
                    
                    await send_warning_alert(
                        "ì›Œí¬í”Œë¡œìš° íƒ€ì„ì•„ì›ƒ ë³µêµ¬",
                        f"íƒ€ì„ì•„ì›ƒëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¤‘ì§€í•˜ê³  ì¬ì‹œì‘í–ˆìŠµë‹ˆë‹¤: {issue['workflow_name']}",
                        service="n8n_automation",
                        workflow_id=workflow_id,
                        old_execution_id=execution_id,
                        new_execution_id=new_execution_id,
                        recovery_action="timeout_recovery"
                    )
                    
                    return True
        
        except Exception as e:
            logger.error(f"ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ë³µêµ¬ ì‹¤íŒ¨: {e}")
        
        return False

class WorkflowOptimizer:
    """ì›Œí¬í”Œë¡œìš° ìµœì í™”"""
    
    def __init__(self):
        self.client = N8nAPIClient()
    
    async def optimize_workflows(self, metrics: List[WorkflowMetrics]) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ìµœì í™”"""
        optimization_results = {
            "optimized": [],
            "suggestions": []
        }
        
        for metric in metrics:
            # ì‹¤í–‰ ì‹œê°„ ìµœì í™”
            if metric.average_duration > 180:  # 3ë¶„ ì´ìƒ
                optimization_results["suggestions"].append({
                    "workflow_id": metric.workflow_id,
                    "workflow_name": metric.workflow_name,
                    "type": "performance",
                    "suggestion": "ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ì´ ê¹ë‹ˆë‹¤. ë³‘ë ¬ ì²˜ë¦¬ë‚˜ ë°°ì¹˜ í¬ê¸° ì¡°ì •ì„ ê³ ë ¤í•˜ì„¸ìš”.",
                    "current_duration": metric.average_duration
                })
            
            # ì‹¤íŒ¨ìœ¨ ìµœì í™”
            if metric.failure_rate > 20:
                optimization_results["suggestions"].append({
                    "workflow_id": metric.workflow_id,
                    "workflow_name": metric.workflow_name,
                    "type": "reliability",
                    "suggestion": "ì›Œí¬í”Œë¡œìš° ì‹¤íŒ¨ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ì¬ì‹œë„ ë¡œì§ì„ ê°•í™”í•˜ì„¸ìš”.",
                    "failure_rate": metric.failure_rate
                })
            
            # ì‹¤í–‰ ë¹ˆë„ ìµœì í™”
            if metric.workflow_type == WorkflowType.RSS_COLLECTION:
                if metric.total_executions > 100:  # ì¼ì¼ ì‹¤í–‰ ê¸°ì¤€
                    optimization_results["suggestions"].append({
                        "workflow_id": metric.workflow_id,
                        "workflow_name": metric.workflow_name,
                        "type": "frequency",
                        "suggestion": "RSS ìˆ˜ì§‘ ë¹ˆë„ê°€ ë†’ìŠµë‹ˆë‹¤. ìŠ¤ì¼€ì¤„ì„ ì¡°ì •í•˜ì—¬ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•˜ì„¸ìš”.",
                        "daily_executions": metric.total_executions
                    })
        
        return optimization_results

class N8nAutomationManager:
    """n8n ìë™í™” í†µí•© ê´€ë¦¬ì"""
    
    def __init__(self):
        self.monitor = WorkflowMonitor()
        self.recovery = WorkflowRecovery()
        self.optimizer = WorkflowOptimizer()
        self.client = N8nAPIClient()
    
    async def run_automation_cycle(self):
        """ìë™í™” ì‚¬ì´í´ ì‹¤í–‰"""
        logger.info("n8n ìë™í™” ì‚¬ì´í´ ì‹œì‘")
        
        try:
            # 1. ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            metrics = await self.monitor.collect_workflow_metrics()
            
            if not metrics:
                logger.warning("ìˆ˜ì§‘ëœ ì›Œí¬í”Œë¡œìš° ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤")
                return
            
            # 2. ê±´ê°• ìƒíƒœ ë¶„ì„
            issues = await self.monitor.analyze_workflow_health(metrics)
            
            # 3. ìë™ ë³µêµ¬ ì‹¤í–‰
            if issues:
                recovery_results = await self.recovery.execute_recovery(issues)
                await self.report_recovery_results(recovery_results)
            
            # 4. ìµœì í™” ì œì•ˆ
            optimization_results = await self.optimizer.optimize_workflows(metrics)
            await self.report_optimization_suggestions(optimization_results)
            
            # 5. ì „ì²´ ìƒíƒœ ë¦¬í¬íŠ¸
            await self.generate_status_report(metrics, issues)
            
            logger.info("n8n ìë™í™” ì‚¬ì´í´ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"n8n ìë™í™” ì‚¬ì´í´ ì‹¤íŒ¨: {e}")
            await send_critical_alert(
                "n8n ìë™í™” ì‹œìŠ¤í…œ ì˜¤ë¥˜",
                f"n8n ìë™í™” ì‹œìŠ¤í…œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                service="n8n_automation"
            )
        finally:
            await self.client.close()
    
    async def report_recovery_results(self, results: Dict[str, List[str]]):
        """ë³µêµ¬ ê²°ê³¼ ë¦¬í¬íŠ¸"""
        if results["successful"]:
            await send_info_alert(
                "ì›Œí¬í”Œë¡œìš° ìë™ ë³µêµ¬ ì„±ê³µ",
                f"ë‹¤ìŒ ì›Œí¬í”Œë¡œìš°ê°€ ìë™ìœ¼ë¡œ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤:\n" + "\n".join(results["successful"]),
                service="n8n_automation",
                recovery_count=len(results["successful"])
            )
        
        if results["failed"]:
            await send_warning_alert(
                "ì›Œí¬í”Œë¡œìš° ë³µêµ¬ ì‹¤íŒ¨",
                f"ë‹¤ìŒ ì›Œí¬í”Œë¡œìš° ë³µêµ¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤:\n" + "\n".join(results["failed"]),
                service="n8n_automation",
                failed_recovery_count=len(results["failed"])
            )
    
    async def report_optimization_suggestions(self, results: Dict[str, Any]):
        """ìµœì í™” ì œì•ˆ ë¦¬í¬íŠ¸"""
        if results["suggestions"]:
            suggestions_text = "\n".join([
                f"- {s['workflow_name']}: {s['suggestion']}" 
                for s in results["suggestions"]
            ])
            
            await send_info_alert(
                "ì›Œí¬í”Œë¡œìš° ìµœì í™” ì œì•ˆ",
                f"ë‹¤ìŒ ì›Œí¬í”Œë¡œìš°ë“¤ì˜ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”:\n{suggestions_text}",
                service="n8n_automation",
                optimization_count=len(results["suggestions"])
            )
    
    async def generate_status_report(self, metrics: List[WorkflowMetrics], issues: List[Dict[str, Any]]):
        """ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
        total_workflows = len(metrics)
        total_executions = sum(m.total_executions for m in metrics)
        avg_success_rate = sum(m.success_rate for m in metrics) / total_workflows if total_workflows > 0 else 0
        
        critical_issues = len([i for i in issues if i.get("severity") == "critical"])
        warning_issues = len([i for i in issues if i.get("severity") == "warning"])
        
        report = f"""
ğŸ¤– n8n ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë¦¬í¬íŠ¸
================================

ğŸ“Š ì „ì²´ í˜„í™©:
- ì´ ì›Œí¬í”Œë¡œìš°: {total_workflows}ê°œ
- ì´ ì‹¤í–‰ íšŸìˆ˜: {total_executions}íšŒ
- í‰ê·  ì„±ê³µë¥ : {avg_success_rate:.1f}%

âš ï¸ ì´ìŠˆ í˜„í™©:
- ì‹¬ê°: {critical_issues}ê°œ
- ê²½ê³ : {warning_issues}ê°œ

ğŸ¯ ìƒíƒœ: {"ğŸ”´ ì£¼ì˜ í•„ìš”" if critical_issues > 0 else "ğŸŸ¡ ì–‘í˜¸" if warning_issues > 0 else "ğŸŸ¢ ì •ìƒ"}
"""
        
        await send_info_alert(
            "n8n ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë¦¬í¬íŠ¸",
            report,
            service="n8n_automation",
            total_workflows=total_workflows,
            total_executions=total_executions,
            avg_success_rate=avg_success_rate,
            critical_issues=critical_issues,
            warning_issues=warning_issues
        )

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    automation_manager = N8nAutomationManager()
    
    # ì§€ì†ì  ìë™í™” ëª¨ë‹ˆí„°ë§
    while True:
        try:
            await automation_manager.run_automation_cycle()
            await asyncio.sleep(1800)  # 30ë¶„ ê°„ê²©
        except KeyboardInterrupt:
            logger.info("n8n ìë™í™” ì‹œìŠ¤í…œ ì¤‘ë‹¨")
            break
        except Exception as e:
            logger.error(f"n8n ìë™í™” ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            await asyncio.sleep(300)  # 5ë¶„ í›„ ì¬ì‹œë„

if __name__ == "__main__":
    asyncio.run(main())