"""
GPU ì„œë²„ API ì—”ë“œí¬ì¸íŠ¸
Whisper, YOLO, OCR ëª¨ë¸ ì„œë¹™ ë° ë°°ì¹˜ ì²˜ë¦¬
"""
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import torch
import logging
import asyncio
import base64
import io
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import Config
from src.whisper_processor.whisper_processor import WhisperProcessor
from src.visual_processor.object_detector import ObjectDetector
from src.visual_processor.ocr_processor import OCRProcessor
from src.gpu_optimizer.gpu_optimizer import GPUOptimizer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Influence Item GPU Server", 
    version="1.0.0",
    description="AI ëª¨ë¸ ì„œë¹™ API (Whisper, YOLO, OCR)"
)

# ê¸€ë¡œë²Œ ëª¨ë¸ ê°ì²´ë“¤
config = Config()
gpu_optimizer = None
whisper_processor = None
object_detector = None
ocr_processor = None

# ëª¨ë¸ ë¡œë”© ìƒíƒœ
models_loaded = {
    "whisper": False,
    "yolo": False,
    "ocr": False
}

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ì´ˆê¸°í™”"""
    global gpu_optimizer, whisper_processor, object_detector, ocr_processor
    
    logger.info("ğŸš€ GPU ì„œë²„ ì‹œì‘ ì¤‘...")
    logger.info(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    try:
        # GPU ìµœì í™” ì´ˆê¸°í™”
        logger.info("GPU ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        gpu_optimizer = GPUOptimizer(config)
        
        # Whisper ëª¨ë¸ ë¡œë”©
        logger.info("Whisper ëª¨ë¸ ë¡œë”© ì¤‘...")
        whisper_processor = WhisperProcessor(config)
        if whisper_processor.model is not None:
            models_loaded["whisper"] = True
            logger.info("âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # YOLO ëª¨ë¸ ë¡œë”©
        logger.info("YOLO ëª¨ë¸ ë¡œë”© ì¤‘...")
        object_detector = ObjectDetector(config, gpu_optimizer=gpu_optimizer)
        if object_detector.model is not None:
            models_loaded["yolo"] = True
            logger.info("âœ… YOLO ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # OCR ëª¨ë¸ ë¡œë”©
        logger.info("EasyOCR ëª¨ë¸ ë¡œë”© ì¤‘...")
        ocr_processor = OCRProcessor(config)
        if ocr_processor.reader is not None:
            models_loaded["ocr"] = True
            logger.info("âœ… EasyOCR ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        logger.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        if gpu_optimizer and gpu_optimizer.is_gpu_available:
            memory_info = gpu_optimizer.get_memory_info()
            logger.info(f"GPU ë©”ëª¨ë¦¬ ìƒíƒœ: ì´ {memory_info['total']}MB, ì‚¬ìš© ì¤‘ {memory_info['used']}MB")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ë¶€ë¶„ì ìœ¼ë¡œë¼ë„ ì„œë¹„ìŠ¤ ì œê³µ
        logger.warning("ì¼ë¶€ ëª¨ë¸ë§Œìœ¼ë¡œ ì„œë¹„ìŠ¤ ì‹œì‘")

class AudioRequest(BaseModel):
    audio_url: str
    language: Optional[str] = "ko"

class VideoFrameRequest(BaseModel):
    frames: List[str]  # Base64 encoded frames
    target_objects: Optional[List[str]] = None

class OCRRequest(BaseModel):
    images: List[str]  # Base64 encoded images
    languages: Optional[List[str]] = ["ko", "en"]

# ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/process/whisper/batch")
async def process_whisper_batch(request: List[AudioRequest]):
    """Whisper ë°°ì¹˜ ì²˜ë¦¬"""
    if not models_loaded["whisper"]:
        raise HTTPException(status_code=503, detail="Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        def process_audio_batch(audio_requests):
            results = []
            for req in audio_requests:
                # ì‹¤ì œ Whisper ì²˜ë¦¬ ë¡œì§
                result = whisper_processor.transcribe_from_url(req.audio_url)
                results.append(result)
            return results
        
        # GPU ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
        if gpu_optimizer:
            results = gpu_optimizer.process_batch(request, process_audio_batch)
        else:
            results = process_audio_batch(request)
        
        return {"results": results, "processed_count": len(request)}
    
    except Exception as e:
        logger.error(f"Whisper ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.post("/process/yolo/batch")
async def process_yolo_batch(request: List[VideoFrameRequest]):
    """YOLO ë°°ì¹˜ ì²˜ë¦¬"""
    if not models_loaded["yolo"]:
        raise HTTPException(status_code=503, detail="YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        def process_frames_batch(frame_requests):
            results = []
            for req in frame_requests:
                # ì‹¤ì œ YOLO ì²˜ë¦¬ ë¡œì§
                batch_result = []
                for frame_b64 in req.frames:
                    # Base64 ë””ì½”ë”© ë° ê°ì²´ íƒì§€
                    result = object_detector.detect_objects_from_base64(frame_b64)
                    batch_result.append(result)
                results.append(batch_result)
            return results
        
        # GPU ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
        if gpu_optimizer:
            results = gpu_optimizer.process_batch(request, process_frames_batch)
        else:
            results = process_frames_batch(request)
        
        return {"results": results, "processed_count": len(request)}
    
    except Exception as e:
        logger.error(f"YOLO ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.post("/process/ocr/batch")
async def process_ocr_batch(request: List[OCRRequest]):
    """OCR ë°°ì¹˜ ì²˜ë¦¬"""
    if not models_loaded["ocr"]:
        raise HTTPException(status_code=503, detail="OCR ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    try:
        def process_images_batch(ocr_requests):
            results = []
            for req in ocr_requests:
                # ì‹¤ì œ OCR ì²˜ë¦¬ ë¡œì§
                batch_result = []
                for image_b64 in req.images:
                    # Base64 ë””ì½”ë”© ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    result = ocr_processor.extract_text_from_base64(image_b64)
                    batch_result.append(result)
                results.append(batch_result)
            return results
        
        # GPU ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
        if gpu_optimizer:
            results = gpu_optimizer.process_batch(request, process_images_batch)
        else:
            results = process_images_batch(request)
        
        return {"results": results, "processed_count": len(request)}
    
    except Exception as e:
        logger.error(f"OCR ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    gpu_available = torch.cuda.is_available()
    return {
        "status": "healthy",
        "gpu_available": gpu_available,
        "models_loaded": models_loaded
    }

@app.post("/api/whisper/transcribe")
async def transcribe_audio(request: AudioRequest):
    """Whisper ìŒì„± ì¸ì‹"""
    if not models_loaded["whisper"]:
        raise HTTPException(status_code=503, detail="Whisper ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    # TODO: ì‹¤ì œ Whisper ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
    return {
        "transcript": "Sample transcript",
        "segments": [
            {"start": 0.0, "end": 5.0, "text": "Sample segment"}
        ]
    }

@app.post("/api/yolo/detect")
async def detect_objects(request: VideoFrameRequest):
    """YOLO ê°ì²´ íƒì§€"""
    if not models_loaded["yolo"]:
        raise HTTPException(status_code=503, detail="YOLO ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    # TODO: ì‹¤ì œ YOLO ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
    return {
        "detections": [
            {"class": "person", "confidence": 0.9, "bbox": [100, 100, 200, 200]}
        ]
    }

@app.post("/api/ocr/extract")
async def extract_text(request: OCRRequest):
    """OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not models_loaded["ocr"]:
        raise HTTPException(status_code=503, detail="OCR ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
    
    # TODO: ì‹¤ì œ OCR ì²˜ë¦¬ ë¡œì§ êµ¬í˜„
    return {
        "texts": [
            {"text": "Sample text", "confidence": 0.95, "bbox": [50, 50, 150, 80]}
        ]
    }

@app.get("/api/models/status")
async def get_models_status():
    """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
    return {
        "models": models_loaded,
        "gpu_info": {
            "available": torch.cuda.is_available(),
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)