#!/usr/bin/env python3
"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
AI íŒŒì´í”„ë¼ì¸, ë°ì´í„° ì²˜ë¦¬, API ì‘ë‹µ ì‹œê°„ ë“±ì„ ë²¤ì¹˜ë§ˆí¬í•˜ì—¬ ì„±ëŠ¥ ìµœì í™” ì§€ì ì„ ì‹ë³„
"""

import time
import asyncio
import aiohttp
import requests
import json
import statistics
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import sys
import sqlite3
import subprocess
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    test_name: str
    timestamp: str
    average_time: float
    min_time: float
    max_time: float
    median_time: float
    success_rate: float
    throughput: float  # requests per second
    details: Dict[str, Any]

class PerformanceBenchmarker:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.project_root = project_root
        self.results = []
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "performance_benchmark.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def benchmark_api_endpoints(self, iterations: int = 10) -> BenchmarkResult:
        """API ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"API ì—”ë“œí¬ì¸íŠ¸ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        endpoints = [
            "http://localhost:8501",  # Streamlit ëŒ€ì‹œë³´ë“œ
            "http://localhost:8000/docs",  # API ì„œë²„ ë¬¸ì„œ
        ]
        
        times = []
        success_count = 0
        start_total = time.time()
        
        async with aiohttp.ClientSession() as session:
            for i in range(iterations):
                for endpoint in endpoints:
                    try:
                        start_time = time.time()
                        async with session.get(endpoint, timeout=10) as response:
                            await response.text()
                            end_time = time.time()
                            
                        response_time = end_time - start_time
                        times.append(response_time)
                        
                        if response.status == 200:
                            success_count += 1
                            
                    except Exception as e:
                        self.logger.warning(f"API ìš”ì²­ ì‹¤íŒ¨ {endpoint}: {e}")
                        times.append(10.0)  # íƒ€ì„ì•„ì›ƒê°’ìœ¼ë¡œ ì„¤ì •
        
        total_time = time.time() - start_total
        total_requests = iterations * len(endpoints)
        
        if times:
            return BenchmarkResult(
                test_name="API Endpoints",
                timestamp=datetime.now().isoformat(),
                average_time=round(statistics.mean(times), 3),
                min_time=round(min(times), 3),
                max_time=round(max(times), 3),
                median_time=round(statistics.median(times), 3),
                success_rate=round((success_count / total_requests) * 100, 2),
                throughput=round(total_requests / total_time, 2),
                details={
                    "endpoints_tested": endpoints,
                    "total_requests": total_requests,
                    "successful_requests": success_count,
                    "total_time": round(total_time, 3)
                }
            )
        else:
            return None

    def benchmark_database_operations(self, iterations: int = 100) -> BenchmarkResult:
        """ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        db_path = self.project_root / "influence_item.db"
        if not db_path.exists():
            self.logger.error("ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return None
        
        times = []
        success_count = 0
        start_total = time.time()
        
        queries = [
            "SELECT COUNT(*) FROM rss_channels",
            "SELECT COUNT(*) FROM candidates",
            "SELECT COUNT(*) FROM scraped_videos",
            "SELECT * FROM rss_channels LIMIT 10",
            "SELECT * FROM candidates LIMIT 10",
        ]
        
        for i in range(iterations):
            for query in queries:
                try:
                    start_time = time.time()
                    with sqlite3.connect(str(db_path)) as conn:
                        cursor = conn.cursor()
                        cursor.execute(query)
                        cursor.fetchall()
                    end_time = time.time()
                    
                    response_time = end_time - start_time
                    times.append(response_time)
                    success_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì‹¤íŒ¨ {query}: {e}")
                    times.append(1.0)  # ê¸°ë³¸ê°’
        
        total_time = time.time() - start_total
        total_queries = iterations * len(queries)
        
        if times:
            return BenchmarkResult(
                test_name="Database Operations",
                timestamp=datetime.now().isoformat(),
                average_time=round(statistics.mean(times), 3),
                min_time=round(min(times), 3),
                max_time=round(max(times), 3),
                median_time=round(statistics.median(times), 3),
                success_rate=round((success_count / total_queries) * 100, 2),
                throughput=round(total_queries / total_time, 2),
                details={
                    "queries_tested": queries,
                    "total_queries": total_queries,
                    "successful_queries": success_count,
                    "total_time": round(total_time, 3)
                }
            )
        else:
            return None

    def benchmark_file_operations(self, iterations: int = 50) -> BenchmarkResult:
        """íŒŒì¼ I/O ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"íŒŒì¼ I/O ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        times = []
        success_count = 0
        start_total = time.time()
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_dir = self.project_root / "temp_benchmark"
        test_dir.mkdir(exist_ok=True)
        
        try:
            for i in range(iterations):
                try:
                    # íŒŒì¼ ì“°ê¸° ë²¤ì¹˜ë§ˆí¬
                    start_time = time.time()
                    test_file = test_dir / f"test_{i}.txt"
                    test_data = "í…ŒìŠ¤íŠ¸ ë°ì´í„° " * 1000  # ì•½ 13KB
                    
                    with open(test_file, 'w', encoding='utf-8') as f:
                        f.write(test_data)
                    
                    # íŒŒì¼ ì½ê¸° ë²¤ì¹˜ë§ˆí¬
                    with open(test_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # íŒŒì¼ ì‚­ì œ
                    test_file.unlink()
                    
                    end_time = time.time()
                    response_time = end_time - start_time
                    times.append(response_time)
                    success_count += 1
                    
                except Exception as e:
                    self.logger.warning(f"íŒŒì¼ I/O ì‘ì—… ì‹¤íŒ¨ {i}: {e}")
                    times.append(0.1)  # ê¸°ë³¸ê°’
            
            # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if test_dir.exists():
                import shutil
                shutil.rmtree(test_dir)
        
        except Exception as e:
            self.logger.error(f"íŒŒì¼ I/O ë²¤ì¹˜ë§ˆí¬ ì˜¤ë¥˜: {e}")
            return None
        
        total_time = time.time() - start_total
        
        if times:
            return BenchmarkResult(
                test_name="File I/O Operations",
                timestamp=datetime.now().isoformat(),
                average_time=round(statistics.mean(times), 3),
                min_time=round(min(times), 3),
                max_time=round(max(times), 3),
                median_time=round(statistics.median(times), 3),
                success_rate=round((success_count / iterations) * 100, 2),
                throughput=round(iterations / total_time, 2),
                details={
                    "operations_tested": ["write", "read", "delete"],
                    "total_operations": iterations,
                    "successful_operations": success_count,
                    "total_time": round(total_time, 3),
                    "data_size_kb": 13
                }
            )
        else:
            return None

    def benchmark_cpu_intensive_tasks(self, iterations: int = 10) -> BenchmarkResult:
        """CPU ì§‘ì•½ì  ì‘ì—… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"CPU ì§‘ì•½ì  ì‘ì—… ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        times = []
        success_count = 0
        start_total = time.time()
        
        def cpu_intensive_task():
            """CPU ì§‘ì•½ì  ê³„ì‚° ì‘ì—…"""
            result = 0
            for i in range(100000):
                result += i ** 0.5
            return result
        
        for i in range(iterations):
            try:
                start_time = time.time()
                result = cpu_intensive_task()
                end_time = time.time()
                
                response_time = end_time - start_time
                times.append(response_time)
                success_count += 1
                
            except Exception as e:
                self.logger.warning(f"CPU ì§‘ì•½ì  ì‘ì—… ì‹¤íŒ¨ {i}: {e}")
                times.append(1.0)  # ê¸°ë³¸ê°’
        
        total_time = time.time() - start_total
        
        if times:
            return BenchmarkResult(
                test_name="CPU Intensive Tasks",
                timestamp=datetime.now().isoformat(),
                average_time=round(statistics.mean(times), 3),
                min_time=round(min(times), 3),
                max_time=round(max(times), 3),
                median_time=round(statistics.median(times), 3),
                success_rate=round((success_count / iterations) * 100, 2),
                throughput=round(iterations / total_time, 2),
                details={
                    "task_type": "mathematical_computation",
                    "operations_per_task": 100000,
                    "total_tasks": iterations,
                    "successful_tasks": success_count,
                    "total_time": round(total_time, 3)
                }
            )
        else:
            return None

    def benchmark_memory_operations(self, iterations: int = 20) -> BenchmarkResult:
        """ë©”ëª¨ë¦¬ ì‘ì—… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        self.logger.info(f"ë©”ëª¨ë¦¬ ì‘ì—… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ ({iterations}íšŒ ë°˜ë³µ)")
        
        times = []
        success_count = 0
        start_total = time.time()
        
        for i in range(iterations):
            try:
                start_time = time.time()
                
                # í° ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì¡°ì‘
                large_list = list(range(100000))
                large_list.sort(reverse=True)
                filtered_list = [x for x in large_list if x % 2 == 0]
                
                # ë”•ì…”ë„ˆë¦¬ ìƒì„± ë° ì¡°ì‘
                large_dict = {str(i): i for i in range(10000)}
                sorted_keys = sorted(large_dict.keys())
                
                # ë©”ëª¨ë¦¬ í•´ì œ
                del large_list, filtered_list, large_dict, sorted_keys
                
                end_time = time.time()
                response_time = end_time - start_time
                times.append(response_time)
                success_count += 1
                
            except Exception as e:
                self.logger.warning(f"ë©”ëª¨ë¦¬ ì‘ì—… ì‹¤íŒ¨ {i}: {e}")
                times.append(1.0)  # ê¸°ë³¸ê°’
        
        total_time = time.time() - start_total
        
        if times:
            return BenchmarkResult(
                test_name="Memory Operations",
                timestamp=datetime.now().isoformat(),
                average_time=round(statistics.mean(times), 3),
                min_time=round(min(times), 3),
                max_time=round(max(times), 3),
                median_time=round(statistics.median(times), 3),
                success_rate=round((success_count / iterations) * 100, 2),
                throughput=round(iterations / total_time, 2),
                details={
                    "operations_tested": ["list_creation", "sorting", "filtering", "dict_creation"],
                    "list_size": 100000,
                    "dict_size": 10000,
                    "total_operations": iterations,
                    "successful_operations": success_count,
                    "total_time": round(total_time, 3)
                }
            )
        else:
            return None

    async def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        self.logger.info("ì „ì²´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹œì‘")
        
        start_time = time.time()
        benchmark_results = {}
        
        # API ì—”ë“œí¬ì¸íŠ¸ ë²¤ì¹˜ë§ˆí¬
        try:
            api_result = await self.benchmark_api_endpoints(iterations=5)
            if api_result:
                benchmark_results["api_endpoints"] = asdict(api_result)
                self.results.append(api_result)
        except Exception as e:
            self.logger.error(f"API ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì‘ì—… ë²¤ì¹˜ë§ˆí¬
        try:
            db_result = self.benchmark_database_operations(iterations=50)
            if db_result:
                benchmark_results["database_operations"] = asdict(db_result)
                self.results.append(db_result)
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        # íŒŒì¼ I/O ë²¤ì¹˜ë§ˆí¬
        try:
            file_result = self.benchmark_file_operations(iterations=20)
            if file_result:
                benchmark_results["file_operations"] = asdict(file_result)
                self.results.append(file_result)
        except Exception as e:
            self.logger.error(f"íŒŒì¼ I/O ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        # CPU ì§‘ì•½ì  ì‘ì—… ë²¤ì¹˜ë§ˆí¬
        try:
            cpu_result = self.benchmark_cpu_intensive_tasks(iterations=5)
            if cpu_result:
                benchmark_results["cpu_intensive"] = asdict(cpu_result)
                self.results.append(cpu_result)
        except Exception as e:
            self.logger.error(f"CPU ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        # ë©”ëª¨ë¦¬ ì‘ì—… ë²¤ì¹˜ë§ˆí¬
        try:
            memory_result = self.benchmark_memory_operations(iterations=10)
            if memory_result:
                benchmark_results["memory_operations"] = asdict(memory_result)
                self.results.append(memory_result)
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        
        total_time = time.time() - start_time
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            "timestamp": datetime.now().isoformat(),
            "total_benchmark_time": round(total_time, 3),
            "benchmarks_completed": len(benchmark_results),
            "results": benchmark_results,
            "performance_summary": self.generate_performance_summary(benchmark_results),
            "recommendations": self.generate_recommendations(benchmark_results)
        }
        
        # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        self.save_benchmark_results(report)
        
        return report

    def generate_performance_summary(self, results: Dict[str, Any]) -> Dict[str, str]:
        """ì„±ëŠ¥ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        for test_name, result in results.items():
            avg_time = result.get('average_time', 0)
            success_rate = result.get('success_rate', 0)
            throughput = result.get('throughput', 0)
            
            # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
            if test_name == "api_endpoints":
                if avg_time < 0.1 and success_rate > 95:
                    grade = "ğŸŸ¢ ìš°ìˆ˜"
                elif avg_time < 0.5 and success_rate > 90:
                    grade = "ğŸŸ¡ ì–‘í˜¸"
                else:
                    grade = "ğŸ”´ ê°œì„  í•„ìš”"
            elif test_name == "database_operations":
                if avg_time < 0.01 and success_rate > 98:
                    grade = "ğŸŸ¢ ìš°ìˆ˜"
                elif avg_time < 0.05 and success_rate > 95:
                    grade = "ğŸŸ¡ ì–‘í˜¸"
                else:
                    grade = "ğŸ”´ ê°œì„  í•„ìš”"
            else:
                if success_rate > 95:
                    grade = "ğŸŸ¢ ìš°ìˆ˜"
                elif success_rate > 85:
                    grade = "ğŸŸ¡ ì–‘í˜¸"
                else:
                    grade = "ğŸ”´ ê°œì„  í•„ìš”"
            
            summary[test_name] = f"{grade} (í‰ê· : {avg_time}ì´ˆ, ì„±ê³µë¥ : {success_rate}%, ì²˜ë¦¬ëŸ‰: {throughput}/ì´ˆ)"
        
        return summary

    def generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for test_name, result in results.items():
            avg_time = result.get('average_time', 0)
            success_rate = result.get('success_rate', 0)
            max_time = result.get('max_time', 0)
            
            if test_name == "api_endpoints":
                if avg_time > 0.5:
                    recommendations.append("ğŸ”§ API ì‘ë‹µ ì‹œê°„ ê°œì„ : ìºì‹±, ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” ê²€í† ")
                if success_rate < 95:
                    recommendations.append("ğŸ”§ API ì•ˆì •ì„± ê°œì„ : ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë¡œì§ ê°•í™”")
                if max_time > 2.0:
                    recommendations.append("ğŸ”§ API íƒ€ì„ì•„ì›ƒ ìµœì í™”: ëŠë¦° ìš”ì²­ ì›ì¸ ë¶„ì„ í•„ìš”")
            
            elif test_name == "database_operations":
                if avg_time > 0.05:
                    recommendations.append("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ ê°œì„ : ì¸ë±ìŠ¤ ì¶”ê°€, ì¿¼ë¦¬ ìµœì í™” ê²€í† ")
                if success_rate < 98:
                    recommendations.append("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ì•ˆì •ì„± ê°œì„ : ì—°ê²° í’€ë§, íŠ¸ëœì­ì…˜ ìµœì í™”")
            
            elif test_name == "file_operations":
                if avg_time > 0.1:
                    recommendations.append("ğŸ”§ íŒŒì¼ I/O ì„±ëŠ¥ ê°œì„ : SSD ì‚¬ìš©, ë¹„ë™ê¸° I/O ì ìš© ê²€í† ")
            
            elif test_name == "cpu_intensive":
                if avg_time > 0.5:
                    recommendations.append("ğŸ”§ CPU ì„±ëŠ¥ ê°œì„ : ë©€í‹°í”„ë¡œì„¸ì‹±, ì•Œê³ ë¦¬ì¦˜ ìµœì í™” ê²€í† ")
            
            elif test_name == "memory_operations":
                if avg_time > 0.5:
                    recommendations.append("ğŸ”§ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì í™”: ì œë„ˆë ˆì´í„° ì‚¬ìš©, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìë£Œêµ¬ì¡° ê²€í† ")
        
        if not recommendations:
            recommendations.append("âœ… ëª¨ë“  ì„±ëŠ¥ ì§€í‘œê°€ ì–‘í˜¸í•©ë‹ˆë‹¤")
        
        return recommendations

    def save_benchmark_results(self, report: Dict[str, Any]):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            benchmark_dir = self.project_root / "monitoring_data"
            benchmark_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            with open(benchmark_dir / f"benchmark_report_{timestamp}.json", 'w') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: benchmark_report_{timestamp}.json")
                
        except Exception as e:
            self.logger.error(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    benchmarker = PerformanceBenchmarker()
    
    print("ğŸš€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤ìœ„íŠ¸ ì‹œì‘...")
    print("ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
    
    try:
        report = await benchmarker.run_full_benchmark_suite()
        
        print(f"\n=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë¦¬í¬íŠ¸ ===")
        print(f"ì‹¤í–‰ ì‹œê°„: {report['total_benchmark_time']}ì´ˆ")
        print(f"ì™„ë£Œëœ ë²¤ì¹˜ë§ˆí¬: {report['benchmarks_completed']}ê°œ")
        
        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        for test_name, summary in report['performance_summary'].items():
            print(f"  {test_name}: {summary}")
        
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for recommendation in report['recommendations']:
            print(f"  {recommendation}")
        
        print(f"\nìì„¸í•œ ê²°ê³¼ëŠ” monitoring_data/ í´ë”ì˜ JSON íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
    except Exception as e:
        print(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())