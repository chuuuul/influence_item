#!/usr/bin/env python3
"""
ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë„êµ¬
ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤, API ì„±ëŠ¥, AI íŒŒì´í”„ë¼ì¸, ë°ì´í„° ì²˜ë¦¬ ìƒíƒœë¥¼ ì¢…í•©ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
"""

import psutil
import time
import json
import sqlite3
import requests
import logging
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any
import asyncio
import threading
from pathlib import Path
import subprocess
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

@dataclass
class SystemMetrics:
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_gb: float
    memory_total_gb: float
    disk_percent: float
    disk_used_gb: float
    disk_total_gb: float
    network_sent_mb: float
    network_recv_mb: float
    process_count: int
    
@dataclass
class APIMetrics:
    """API ì„œë²„ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    streamlit_status: bool
    streamlit_response_time: float
    api_server_status: bool
    api_server_response_time: float
    database_connection_status: bool
    database_response_time: float
    error_count: int
    
@dataclass
class AIMetrics:
    """AI íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    gpu_available: bool
    gpu_utilization: float
    gpu_memory_used: float
    gpu_memory_total: float
    gemini_api_status: bool
    gemini_response_time: float
    whisper_processing_time: float
    video_analysis_queue_size: int
    memory_leak_detected: bool
    
@dataclass
class DataMetrics:
    """ë°ì´í„° ì²˜ë¦¬ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    timestamp: str
    rss_collection_success_rate: float
    rss_collection_count: int
    channel_discovery_success_rate: float
    channel_discovery_count: int
    ppl_filtering_accuracy: float
    ppl_filtering_count: int
    database_size_mb: float
    last_data_update: str
    
class SystemMonitor:
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.project_root = project_root
        self.monitoring_active = False
        self.alert_thresholds = {
            'cpu_percent': 80.0,
            'memory_percent': 85.0,
            'disk_percent': 90.0,
            'api_response_time': 5.0,
            'database_response_time': 2.0,
            'error_rate': 0.1,
        }
        
        # ë¡œê¹… ì„¤ì •
        self.setup_logging()
        
        # ë©”íŠ¸ë¦­ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        self.system_metrics_history = []
        self.api_metrics_history = []
        self.ai_metrics_history = []
        self.data_metrics_history = []
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "system_monitor.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
    def get_system_metrics(self) -> SystemMetrics:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        try:
            # CPU ì‚¬ìš©ë¥ 
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            memory = psutil.virtual_memory()
            memory_used_gb = memory.used / (1024**3)
            memory_total_gb = memory.total / (1024**3)
            
            # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
            disk = psutil.disk_usage('/')
            disk_used_gb = disk.used / (1024**3)
            disk_total_gb = disk.total / (1024**3)
            
            # ë„¤íŠ¸ì›Œí¬ ì‚¬ìš©ëŸ‰
            network = psutil.net_io_counters()
            network_sent_mb = network.bytes_sent / (1024**2)
            network_recv_mb = network.bytes_recv / (1024**2)
            
            # í”„ë¡œì„¸ìŠ¤ ìˆ˜
            process_count = len(psutil.pids())
            
            return SystemMetrics(
                timestamp=datetime.now().isoformat(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_gb=round(memory_used_gb, 2),
                memory_total_gb=round(memory_total_gb, 2),
                disk_percent=disk.percent,
                disk_used_gb=round(disk_used_gb, 2),
                disk_total_gb=round(disk_total_gb, 2),
                network_sent_mb=round(network_sent_mb, 2),
                network_recv_mb=round(network_recv_mb, 2),
                process_count=process_count
            )
        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return None
    
    def get_api_metrics(self) -> APIMetrics:
        """API ì„œë²„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now().isoformat()
        
        # Streamlit ëŒ€ì‹œë³´ë“œ ìƒíƒœ í™•ì¸
        streamlit_status, streamlit_response_time = self.check_endpoint_health("http://localhost:8501")
        
        # API ì„œë²„ ìƒíƒœ í™•ì¸
        api_status, api_response_time = self.check_endpoint_health("http://localhost:8000/docs")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
        db_status, db_response_time = self.check_database_health()
        
        # ì—ëŸ¬ ì¹´ìš´íŠ¸ í™•ì¸ (ë¡œê·¸ íŒŒì¼ì—ì„œ)
        error_count = self.count_recent_errors()
        
        return APIMetrics(
            timestamp=timestamp,
            streamlit_status=streamlit_status,
            streamlit_response_time=streamlit_response_time,
            api_server_status=api_status,
            api_server_response_time=api_response_time,
            database_connection_status=db_status,
            database_response_time=db_response_time,
            error_count=error_count
        )
    
    def check_endpoint_health(self, url: str) -> tuple[bool, float]:
        """ì—”ë“œí¬ì¸íŠ¸ ìƒíƒœ í™•ì¸"""
        try:
            start_time = time.time()
            response = requests.get(url, timeout=10)
            response_time = time.time() - start_time
            
            if response.status_code == 200:
                return True, round(response_time, 3)
            else:
                return False, round(response_time, 3)
        except Exception as e:
            self.logger.warning(f"ì—”ë“œí¬ì¸íŠ¸ í™•ì¸ ì‹¤íŒ¨ {url}: {e}")
            return False, 0.0
    
    def check_database_health(self) -> tuple[bool, float]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸"""
        try:
            start_time = time.time()
            db_path = self.project_root / "influence_item.db"
            
            with sqlite3.connect(str(db_path), timeout=5) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT 1")
                cursor.fetchone()
                
            response_time = time.time() - start_time
            return True, round(response_time, 3)
        except Exception as e:
            self.logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
            return False, 0.0
    
    def count_recent_errors(self) -> int:
        """ìµœê·¼ ì—ëŸ¬ ìˆ˜ ì¹´ìš´íŠ¸ (ìµœê·¼ 10ë¶„)"""
        try:
            log_file = self.project_root / "logs" / "system_monitor.log"
            if not log_file.exists():
                return 0
                
            error_count = 0
            cutoff_time = datetime.now() - timedelta(minutes=10)
            
            with open(log_file, 'r') as f:
                for line in f:
                    if 'ERROR' in line:
                        try:
                            # ë¡œê·¸ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì‹± (ê¸°ë³¸ ë¡œê¹… í˜•ì‹ ê°€ì •)
                            timestamp_str = line.split(' - ')[0]
                            log_time = datetime.fromisoformat(timestamp_str)
                            if log_time > cutoff_time:
                                error_count += 1
                        except:
                            continue
            
            return error_count
        except Exception as e:
            self.logger.warning(f"ì—ëŸ¬ ì¹´ìš´íŠ¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return 0
    
    def get_ai_metrics(self) -> AIMetrics:
        """AI íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now().isoformat()
        
        # GPU ìƒíƒœ í™•ì¸
        gpu_available, gpu_util, gpu_mem_used, gpu_mem_total = self.check_gpu_status()
        
        # Gemini API ìƒíƒœ í™•ì¸
        gemini_status, gemini_response_time = self.check_gemini_api()
        
        # Whisper ì²˜ë¦¬ ì‹œê°„ (ì˜ˆì‹œê°’)
        whisper_processing_time = 0.0
        
        # ë¹„ë””ì˜¤ ë¶„ì„ í í¬ê¸° (ì˜ˆì‹œê°’)
        video_analysis_queue_size = 0
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€
        memory_leak_detected = self.detect_memory_leak()
        
        return AIMetrics(
            timestamp=timestamp,
            gpu_available=gpu_available,
            gpu_utilization=gpu_util,
            gpu_memory_used=gpu_mem_used,
            gpu_memory_total=gpu_mem_total,
            gemini_api_status=gemini_status,
            gemini_response_time=gemini_response_time,
            whisper_processing_time=whisper_processing_time,
            video_analysis_queue_size=video_analysis_queue_size,
            memory_leak_detected=memory_leak_detected
        )
    
    def check_gpu_status(self) -> tuple[bool, float, float, float]:
        """GPU ìƒíƒœ í™•ì¸"""
        try:
            # nvidia-smi ëª…ë ¹ì–´ë¡œ GPU ìƒíƒœ í™•ì¸
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=5)
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if lines:
                    values = lines[0].split(', ')
                    if len(values) >= 3:
                        gpu_util = float(values[0])
                        gpu_mem_used = float(values[1])
                        gpu_mem_total = float(values[2])
                        return True, gpu_util, gpu_mem_used, gpu_mem_total
            
            return False, 0.0, 0.0, 0.0
        except Exception as e:
            # GPUê°€ ì—†ê±°ë‚˜ nvidia-smiê°€ ì—†ëŠ” ê²½ìš°
            return False, 0.0, 0.0, 0.0
    
    def check_gemini_api(self) -> tuple[bool, float]:
        """Gemini API ìƒíƒœ í™•ì¸"""
        try:
            # Gemini API í‚¤ í™•ì¸
            config_path = self.project_root / "config" / "config.py"
            if not config_path.exists():
                return False, 0.0
            
            # ì‹¤ì œ API í˜¸ì¶œ ëŒ€ì‹  ì„¤ì • íŒŒì¼ ì¡´ì¬ í™•ì¸
            start_time = time.time()
            with open(config_path, 'r') as f:
                content = f.read()
                if 'GEMINI_API_KEY' in content:
                    response_time = time.time() - start_time
                    return True, round(response_time, 3)
            
            return False, 0.0
        except Exception as e:
            self.logger.warning(f"Gemini API í™•ì¸ ì‹¤íŒ¨: {e}")
            return False, 0.0
    
    def detect_memory_leak(self) -> bool:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸
            if len(self.system_metrics_history) >= 10:
                recent_memory = [m.memory_percent for m in self.system_metrics_history[-10:]]
                # ìµœê·¼ 10ê°œ ì¸¡ì •ê°’ì´ ëª¨ë‘ ì¦ê°€ ì¶”ì„¸ì¸ì§€ í™•ì¸
                increasing = all(recent_memory[i] <= recent_memory[i+1] for i in range(len(recent_memory)-1))
                if increasing and recent_memory[-1] - recent_memory[0] > 10:  # 10% ì´ìƒ ì¦ê°€
                    return True
            return False
        except Exception:
            return False
    
    def get_data_metrics(self) -> DataMetrics:
        """ë°ì´í„° ì²˜ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        timestamp = datetime.now().isoformat()
        
        try:
            db_path = self.project_root / "influence_item.db"
            db_size_mb = db_path.stat().st_size / (1024**2) if db_path.exists() else 0
            
            with sqlite3.connect(str(db_path)) as conn:
                cursor = conn.cursor()
                
                # RSS ìˆ˜ì§‘ ì„±ê³µë¥  (ì‹¤ì œ í…Œì´ë¸” ì‚¬ìš©)
                cursor.execute("SELECT COUNT(*) FROM rss_collection_logs WHERE collection_status = 'completed'")
                rss_success = cursor.fetchone()[0] or 0
                cursor.execute("SELECT COUNT(*) FROM rss_collection_logs")
                rss_total = cursor.fetchone()[0] or 1
                rss_success_rate = (rss_success / rss_total) * 100
                
                # ì±„ë„ íƒìƒ‰ ì„±ê³µë¥ 
                cursor.execute("SELECT COUNT(*) FROM rss_channels WHERE is_active = 1")
                channel_success = cursor.fetchone()[0] or 0
                cursor.execute("SELECT COUNT(*) FROM rss_channels")
                channel_total = cursor.fetchone()[0] or 1
                channel_success_rate = (channel_success / channel_total) * 100
                
                # PPL í•„í„°ë§ ì •í™•ë„ (candidates í…Œì´ë¸”ì—ì„œ JSON ë°ì´í„° ê¸°ë°˜)
                cursor.execute("SELECT COUNT(*) FROM candidates")
                ppl_count = cursor.fetchone()[0] or 0
                ppl_accuracy = 90.0  # ê¸°ë³¸ê°’ ì„¤ì • (ì‹¤ì œ ë¶„ì„ ê²°ê³¼ê°€ JSONì— í¬í•¨)
                
                # ë§ˆì§€ë§‰ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œê°„
                cursor.execute("SELECT MAX(scraped_at) FROM scraped_videos")
                last_update = cursor.fetchone()[0] or timestamp
                
            return DataMetrics(
                timestamp=timestamp,
                rss_collection_success_rate=round(rss_success_rate, 2),
                rss_collection_count=rss_total,
                channel_discovery_success_rate=round(channel_success_rate, 2),
                channel_discovery_count=channel_total,
                ppl_filtering_accuracy=ppl_accuracy,
                ppl_filtering_count=ppl_count,
                database_size_mb=round(db_size_mb, 2),
                last_data_update=last_update
            )
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            return DataMetrics(
                timestamp=timestamp,
                rss_collection_success_rate=0,
                rss_collection_count=0,
                channel_discovery_success_rate=0,
                channel_discovery_count=0,
                ppl_filtering_accuracy=0,
                ppl_filtering_count=0,
                database_size_mb=0,
                last_data_update=timestamp
            )
    
    def check_alerts(self, system_metrics: SystemMetrics, api_metrics: APIMetrics) -> List[str]:
        """ì•Œë¦¼ ì¡°ê±´ í™•ì¸"""
        alerts = []
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì•Œë¦¼
        if system_metrics.cpu_percent > self.alert_thresholds['cpu_percent']:
            alerts.append(f"ğŸš¨ CPU ì‚¬ìš©ë¥  ë†’ìŒ: {system_metrics.cpu_percent}%")
        
        if system_metrics.memory_percent > self.alert_thresholds['memory_percent']:
            alerts.append(f"ğŸš¨ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {system_metrics.memory_percent}%")
        
        if system_metrics.disk_percent > self.alert_thresholds['disk_percent']:
            alerts.append(f"ğŸš¨ ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ë†’ìŒ: {system_metrics.disk_percent}%")
        
        # API ì‘ë‹µ ì‹œê°„ ì•Œë¦¼
        if api_metrics.streamlit_response_time > self.alert_thresholds['api_response_time']:
            alerts.append(f"ğŸš¨ Streamlit ì‘ë‹µ ì‹œê°„ ì§€ì—°: {api_metrics.streamlit_response_time}ì´ˆ")
        
        if api_metrics.api_server_response_time > self.alert_thresholds['api_response_time']:
            alerts.append(f"ğŸš¨ API ì„œë²„ ì‘ë‹µ ì‹œê°„ ì§€ì—°: {api_metrics.api_server_response_time}ì´ˆ")
        
        # ì„œë¹„ìŠ¤ ë‹¤ìš´ ì•Œë¦¼
        if not api_metrics.streamlit_status:
            alerts.append("ğŸš¨ Streamlit ëŒ€ì‹œë³´ë“œ ì ‘ì† ë¶ˆê°€")
        
        if not api_metrics.api_server_status:
            alerts.append("ğŸš¨ API ì„œë²„ ì ‘ì† ë¶ˆê°€")
        
        if not api_metrics.database_connection_status:
            alerts.append("ğŸš¨ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
        
        # ì—ëŸ¬ìœ¨ ì•Œë¦¼
        if api_metrics.error_count > 10:  # 10ë¶„ê°„ 10ê°œ ì´ìƒ ì—ëŸ¬
            alerts.append(f"ğŸš¨ ë†’ì€ ì—ëŸ¬ìœ¨: {api_metrics.error_count}ê°œ ì—ëŸ¬ (ìµœê·¼ 10ë¶„)")
        
        return alerts
    
    def save_metrics_to_file(self):
        """ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            monitoring_dir = self.project_root / "monitoring_data"
            monitoring_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            metrics_data = {
                'timestamp': timestamp,
                'system_metrics': [asdict(m) for m in self.system_metrics_history[-100:]],  # ìµœê·¼ 100ê°œ
                'api_metrics': [asdict(m) for m in self.api_metrics_history[-100:]],
                'ai_metrics': [asdict(m) for m in self.ai_metrics_history[-100:]],
                'data_metrics': [asdict(m) for m in self.data_metrics_history[-100:]]
            }
            
            with open(monitoring_dir / f"metrics_{timestamp}.json", 'w') as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            self.logger.error(f"ë©”íŠ¸ë¦­ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def generate_status_report(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not (self.system_metrics_history and self.api_metrics_history):
            return {"error": "ë©”íŠ¸ë¦­ ë°ì´í„° ì—†ìŒ"}
        
        latest_system = self.system_metrics_history[-1]
        latest_api = self.api_metrics_history[-1]
        latest_ai = self.ai_metrics_history[-1] if self.ai_metrics_history else None
        latest_data = self.data_metrics_history[-1] if self.data_metrics_history else None
        
        alerts = self.check_alerts(latest_system, latest_api)
        
        status = "ğŸŸ¢ ì •ìƒ" if not alerts else "ğŸŸ¡ ì£¼ì˜" if len(alerts) <= 2 else "ğŸ”´ ìœ„í—˜"
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": status,
            "alerts": alerts,
            "system_health": {
                "cpu_percent": latest_system.cpu_percent,
                "memory_percent": latest_system.memory_percent,
                "disk_percent": latest_system.disk_percent,
                "process_count": latest_system.process_count
            },
            "service_health": {
                "streamlit_status": "ğŸŸ¢ ì •ìƒ" if latest_api.streamlit_status else "ğŸ”´ ì˜¤ë¥˜",
                "streamlit_response_time": f"{latest_api.streamlit_response_time}ì´ˆ",
                "api_server_status": "ğŸŸ¢ ì •ìƒ" if latest_api.api_server_status else "ğŸ”´ ì˜¤ë¥˜",
                "api_server_response_time": f"{latest_api.api_server_response_time}ì´ˆ",
                "database_status": "ğŸŸ¢ ì •ìƒ" if latest_api.database_connection_status else "ğŸ”´ ì˜¤ë¥˜",
                "database_response_time": f"{latest_api.database_response_time}ì´ˆ"
            }
        }
        
        if latest_ai:
            report["ai_pipeline_health"] = {
                "gpu_available": latest_ai.gpu_available,
                "gpu_utilization": f"{latest_ai.gpu_utilization}%",
                "gemini_api_status": "ğŸŸ¢ ì •ìƒ" if latest_ai.gemini_api_status else "ğŸ”´ ì˜¤ë¥˜",
                "memory_leak_detected": "ğŸ”´ ê°ì§€ë¨" if latest_ai.memory_leak_detected else "ğŸŸ¢ ì •ìƒ"
            }
        
        if latest_data:
            report["data_processing_health"] = {
                "rss_success_rate": f"{latest_data.rss_collection_success_rate}%",
                "channel_discovery_success_rate": f"{latest_data.channel_discovery_success_rate}%",
                "ppl_filtering_accuracy": f"{latest_data.ppl_filtering_accuracy}%",
                "database_size": f"{latest_data.database_size_mb} MB",
                "last_update": latest_data.last_data_update
            }
        
        return report
    
    def monitor_cycle(self):
        """ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì‹¤í–‰"""
        self.logger.info("ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì‹œì‘")
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        system_metrics = self.get_system_metrics()
        if system_metrics:
            self.system_metrics_history.append(system_metrics)
        
        # API ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        api_metrics = self.get_api_metrics()
        self.api_metrics_history.append(api_metrics)
        
        # AI ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        ai_metrics = self.get_ai_metrics()
        self.ai_metrics_history.append(ai_metrics)
        
        # ë°ì´í„° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        data_metrics = self.get_data_metrics()
        self.data_metrics_history.append(data_metrics)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        max_history = 1000
        if len(self.system_metrics_history) > max_history:
            self.system_metrics_history = self.system_metrics_history[-max_history:]
        if len(self.api_metrics_history) > max_history:
            self.api_metrics_history = self.api_metrics_history[-max_history:]
        if len(self.ai_metrics_history) > max_history:
            self.ai_metrics_history = self.ai_metrics_history[-max_history:]
        if len(self.data_metrics_history) > max_history:
            self.data_metrics_history = self.data_metrics_history[-max_history:]
        
        # ì•Œë¦¼ í™•ì¸
        if system_metrics:
            alerts = self.check_alerts(system_metrics, api_metrics)
            if alerts:
                self.logger.warning("ì•Œë¦¼ ë°œìƒ:")
                for alert in alerts:
                    self.logger.warning(f"  {alert}")
        
        # ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_status_report()
        self.logger.info(f"ì‹œìŠ¤í…œ ìƒíƒœ: {report['overall_status']}")
        
        # íŒŒì¼ë¡œ ì €ì¥ (30ë¶„ë§ˆë‹¤)
        if len(self.system_metrics_history) % 3 == 0:  # 10ë¶„ * 3 = 30ë¶„
            self.save_metrics_to_file()
        
        return report
    
    def start_monitoring(self, interval_minutes: int = 10):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring_active = True
        self.logger.info(f"ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘ (ê°„ê²©: {interval_minutes}ë¶„)")
        
        def monitoring_loop():
            while self.monitoring_active:
                try:
                    report = self.monitor_cycle()
                    print(f"\n=== ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')}) ===")
                    print(f"ì „ì²´ ìƒíƒœ: {report['overall_status']}")
                    
                    if report.get('alerts'):
                        print("\nğŸš¨ ì•Œë¦¼:")
                        for alert in report['alerts']:
                            print(f"  {alert}")
                    
                    print(f"\nğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
                    health = report['system_health']
                    print(f"  CPU: {health['cpu_percent']}%")
                    print(f"  ë©”ëª¨ë¦¬: {health['memory_percent']}%")
                    print(f"  ë””ìŠ¤í¬: {health['disk_percent']}%")
                    
                    print(f"\nğŸŒ ì„œë¹„ìŠ¤ ìƒíƒœ:")
                    service = report['service_health']
                    print(f"  Streamlit: {service['streamlit_status']} ({service['streamlit_response_time']})")
                    print(f"  API ì„œë²„: {service['api_server_status']} ({service['api_server_response_time']})")
                    print(f"  ë°ì´í„°ë² ì´ìŠ¤: {service['database_status']} ({service['database_response_time']})")
                    
                    if 'ai_pipeline_health' in report:
                        print(f"\nğŸ¤– AI íŒŒì´í”„ë¼ì¸:")
                        ai = report['ai_pipeline_health']
                        print(f"  GPU: {'ì‚¬ìš© ê°€ëŠ¥' if ai['gpu_available'] else 'ì‚¬ìš© ë¶ˆê°€'} ({ai['gpu_utilization']})")
                        print(f"  Gemini API: {ai['gemini_api_status']}")
                        print(f"  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜: {ai['memory_leak_detected']}")
                    
                    if 'data_processing_health' in report:
                        print(f"\nğŸ“ˆ ë°ì´í„° ì²˜ë¦¬:")
                        data = report['data_processing_health']
                        print(f"  RSS ìˆ˜ì§‘ ì„±ê³µë¥ : {data['rss_success_rate']}")
                        print(f"  ì±„ë„ íƒìƒ‰ ì„±ê³µë¥ : {data['channel_discovery_success_rate']}")
                        print(f"  PPL í•„í„°ë§ ì •í™•ë„: {data['ppl_filtering_accuracy']}")
                        print(f"  ë°ì´í„°ë² ì´ìŠ¤ í¬ê¸°: {data['database_size']}")
                    
                    print("=" * 80)
                    
                except Exception as e:
                    self.logger.error(f"ëª¨ë‹ˆí„°ë§ ì‚¬ì´í´ ì˜¤ë¥˜: {e}")
                
                # ë‹¤ìŒ ëª¨ë‹ˆí„°ë§ê¹Œì§€ ëŒ€ê¸°
                time.sleep(interval_minutes * 60)
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        monitoring_thread = threading.Thread(target=monitoring_loop, daemon=True)
        monitoring_thread.start()
        
        return monitoring_thread
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        self.logger.info("ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    monitor = SystemMonitor()
    
    if len(sys.argv) > 1 and sys.argv[1] == "continuous":
        # ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ëª¨ë“œ
        print("ğŸ” ì§€ì†ì  ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        print("Ctrl+Cë¡œ ì¤‘ì§€í•˜ì„¸ìš”")
        
        try:
            monitoring_thread = monitor.start_monitoring(interval_minutes=10)
            
            # ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ëŒ€ê¸°
            while True:
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\nëª¨ë‹ˆí„°ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤...")
            monitor.stop_monitoring()
            sys.exit(0)
    else:
        # ë‹¨ì¼ ì‹¤í–‰ ëª¨ë“œ
        print("ğŸ” ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì¤‘...")
        report = monitor.monitor_cycle()
        
        print(f"\n=== ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸ ===")
        print(json.dumps(report, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    main()