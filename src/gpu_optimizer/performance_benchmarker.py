"""
ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

GPU ìµœì í™” ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ë¹„êµí•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
"""

import time
import logging
import statistics
from typing import List, Dict, Any, Callable, Optional
from dataclasses import dataclass
from contextlib import contextmanager
import psutil

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    np = None
    HAS_NUMPY = False

from config.config import Config
from .gpu_optimizer import GPUOptimizer


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    test_name: str
    iterations: int
    total_items: int
    processing_times: List[float]
    throughput_rates: List[float]
    memory_usage: List[Dict[str, float]]
    gpu_utilization: List[float]
    cpu_utilization: List[float]
    
    @property
    def avg_processing_time(self) -> float:
        """í‰ê·  ì²˜ë¦¬ ì‹œê°„"""
        return statistics.mean(self.processing_times) if self.processing_times else 0.0
    
    @property
    def avg_throughput(self) -> float:
        """í‰ê·  ì²˜ë¦¬ëŸ‰"""
        return statistics.mean(self.throughput_rates) if self.throughput_rates else 0.0
    
    @property
    def throughput_std(self) -> float:
        """ì²˜ë¦¬ëŸ‰ í‘œì¤€í¸ì°¨"""
        return statistics.stdev(self.throughput_rates) if len(self.throughput_rates) > 1 else 0.0
    
    @property
    def avg_memory_usage(self) -> float:
        """í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ """
        if not self.memory_usage:
            return 0.0
        memory_values = [usage.get('memory_utilization', 0) for usage in self.memory_usage]
        return statistics.mean(memory_values)
    
    @property
    def avg_gpu_utilization(self) -> float:
        """í‰ê·  GPU ì‚¬ìš©ë¥ """
        return statistics.mean(self.gpu_utilization) if self.gpu_utilization else 0.0


class PerformanceBenchmarker:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Config] = None):
        """
        ë²¤ì¹˜ë§ˆì»¤ ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config or Config
        self.logger = self._setup_logger()
        self.gpu_optimizer = None
        self.baseline_results = {}
        
        # ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘
        self.system_info = self._collect_system_info()
        
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger(__name__)
        logger.setLevel(getattr(logging, self.config.LOG_LEVEL))
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
    
    def _collect_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        info = {
            'cpu_count': psutil.cpu_count(),
            'memory_total': psutil.virtual_memory().total,
            'platform': psutil.os.name,
        }
        
        # GPU ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
        try:
            import torch
            if torch.cuda.is_available():
                info['gpu_count'] = torch.cuda.device_count()
                info['gpu_name'] = torch.cuda.get_device_name(0)
                info['gpu_memory'] = torch.cuda.get_device_properties(0).total_memory
            else:
                info['gpu_available'] = False
        except ImportError:
            info['gpu_available'] = False
        
        return info
    
    @contextmanager
    def _performance_monitor(self, gpu_optimizer: Optional[GPUOptimizer] = None):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì»¨í…ìŠ¤íŠ¸"""
        start_time = time.time()
        start_cpu = psutil.cpu_percent()
        start_memory = psutil.virtual_memory().percent
        
        gpu_util_start = 0.0
        if gpu_optimizer and gpu_optimizer.has_cuda:
            gpu_util_start = gpu_optimizer.monitor_gpu_utilization().get('gpu_utilization', 0.0)
        
        try:
            yield
        finally:
            end_time = time.time()
            end_cpu = psutil.cpu_percent()
            end_memory = psutil.virtual_memory().percent
            
            gpu_util_end = 0.0
            if gpu_optimizer and gpu_optimizer.has_cuda:
                gpu_util_end = gpu_optimizer.monitor_gpu_utilization().get('gpu_utilization', 0.0)
            
            # ëª¨ë‹ˆí„°ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ìœ„ì¹˜ê°€ í•„ìš”í•˜ë‹¤ë©´ ì—¬ê¸°ì„œ ì²˜ë¦¬
    
    def benchmark_processing_function(
        self,
        process_fn: Callable,
        test_data: List[Any],
        test_name: str = "benchmark_test",
        iterations: int = 5,
        use_gpu_optimization: bool = True
    ) -> BenchmarkResult:
        """
        ì²˜ë¦¬ í•¨ìˆ˜ ë²¤ì¹˜ë§ˆí‚¹
        
        Args:
            process_fn: ë²¤ì¹˜ë§ˆí‚¹í•  ì²˜ë¦¬ í•¨ìˆ˜
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_name: í…ŒìŠ¤íŠ¸ ì´ë¦„
            iterations: ë°˜ë³µ íšŸìˆ˜
            use_gpu_optimization: GPU ìµœì í™” ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        self.logger.info(f"ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘: {test_name} (ë°˜ë³µ: {iterations}íšŒ)")
        
        # GPU ìµœì í™” ì„¤ì •
        gpu_optimizer = None
        if use_gpu_optimization:
            try:
                gpu_optimizer = GPUOptimizer(self.config)
                self.logger.info("GPU ìµœì í™” ëª¨ë“œë¡œ ë²¤ì¹˜ë§ˆí‚¹")
            except Exception as e:
                self.logger.warning(f"GPU ìµœì í™” ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
        processing_times = []
        throughput_rates = []
        memory_usage = []
        gpu_utilization = []
        cpu_utilization = []
        
        # ì›œì—… ì‹¤í–‰ (ê²°ê³¼ì— í¬í•¨í•˜ì§€ ì•ŠìŒ)
        self.logger.debug("ì›œì—… ì‹¤í–‰ ì¤‘...")
        try:
            if gpu_optimizer:
                gpu_optimizer.process_with_optimization(test_data[:2], process_fn)
            else:
                process_fn(test_data[:2])
        except Exception as e:
            self.logger.warning(f"ì›œì—… ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        
        # ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰
        for i in range(iterations):
            self.logger.debug(f"ë²¤ì¹˜ë§ˆí‚¹ ë°˜ë³µ {i+1}/{iterations}")
            
            # ì‹œì‘ ì‹œì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            start_time = time.time()
            start_cpu = psutil.cpu_percent(interval=None)
            
            if gpu_optimizer:
                start_memory = gpu_optimizer.get_memory_info()
                start_gpu_util = gpu_optimizer.monitor_gpu_utilization()
            else:
                start_memory = {'memory_utilization': psutil.virtual_memory().percent}
                start_gpu_util = {'gpu_utilization': 0.0}
            
            try:
                # ì²˜ë¦¬ ì‹¤í–‰
                if gpu_optimizer:
                    results = gpu_optimizer.process_with_optimization(test_data, process_fn)
                else:
                    results = process_fn(test_data)
                
                # ì¢…ë£Œ ì‹œì  ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                end_time = time.time()
                end_cpu = psutil.cpu_percent(interval=None)
                
                if gpu_optimizer:
                    end_memory = gpu_optimizer.get_memory_info()
                    end_gpu_util = gpu_optimizer.monitor_gpu_utilization()
                else:
                    end_memory = {'memory_utilization': psutil.virtual_memory().percent}
                    end_gpu_util = {'gpu_utilization': 0.0}
                
                # ê²°ê³¼ ê³„ì‚°
                processing_time = end_time - start_time
                throughput = len(test_data) / processing_time if processing_time > 0 else 0.0
                
                # ê²°ê³¼ ì €ì¥
                processing_times.append(processing_time)
                throughput_rates.append(throughput)
                memory_usage.append({
                    'start': start_memory.get('memory_utilization', 0),
                    'end': end_memory.get('memory_utilization', 0),
                    'memory_utilization': end_memory.get('memory_utilization', 0)
                })
                gpu_utilization.append(end_gpu_util.get('gpu_utilization', 0.0))
                cpu_utilization.append(max(start_cpu, end_cpu))
                
                self.logger.debug(f"ë°˜ë³µ {i+1} ì™„ë£Œ - ì²˜ë¦¬ì‹œê°„: {processing_time:.3f}s, ì²˜ë¦¬ëŸ‰: {throughput:.1f} items/s")
                
            except Exception as e:
                self.logger.error(f"ë²¤ì¹˜ë§ˆí‚¹ ë°˜ë³µ {i+1} ì‹¤íŒ¨: {str(e)}")
                # ì‹¤íŒ¨í•œ ë°˜ë³µì€ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬
                processing_times.append(float('inf'))
                throughput_rates.append(0.0)
                memory_usage.append({'memory_utilization': 0})
                gpu_utilization.append(0.0)
                cpu_utilization.append(0.0)
        
        # ê²°ê³¼ ê°ì²´ ìƒì„±
        result = BenchmarkResult(
            test_name=test_name,
            iterations=iterations,
            total_items=len(test_data),
            processing_times=processing_times,
            throughput_rates=throughput_rates,
            memory_usage=memory_usage,
            gpu_utilization=gpu_utilization,
            cpu_utilization=cpu_utilization
        )
        
        self.logger.info(f"ë²¤ì¹˜ë§ˆí‚¹ ì™„ë£Œ: {test_name}")
        self.logger.info(f"í‰ê·  ì²˜ë¦¬ì‹œê°„: {result.avg_processing_time:.3f}s")
        self.logger.info(f"í‰ê·  ì²˜ë¦¬ëŸ‰: {result.avg_throughput:.1f} items/s")
        
        return result
    
    def compare_optimization_modes(
        self,
        process_fn: Callable,
        test_data: List[Any],
        iterations: int = 5
    ) -> Dict[str, BenchmarkResult]:
        """
        GPU ìµœì í™” ëª¨ë“œì™€ ì¼ë°˜ ëª¨ë“œ ë¹„êµ
        
        Args:
            process_fn: ì²˜ë¦¬ í•¨ìˆ˜
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            iterations: ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            ëª¨ë“œë³„ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        self.logger.info("GPU ìµœì í™” ëª¨ë“œ vs ì¼ë°˜ ëª¨ë“œ ë¹„êµ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘")
        
        results = {}
        
        # ì¼ë°˜ ëª¨ë“œ (GPU ìµœì í™” ì—†ìŒ)
        results['normal'] = self.benchmark_processing_function(
            process_fn, test_data, "Normal Mode", iterations, use_gpu_optimization=False
        )
        
        # GPU ìµœì í™” ëª¨ë“œ
        results['optimized'] = self.benchmark_processing_function(
            process_fn, test_data, "GPU Optimized Mode", iterations, use_gpu_optimization=True
        )
        
        # ë¹„êµ ê²°ê³¼ ì¶œë ¥
        self._print_comparison_results(results)
        
        return results
    
    def _print_comparison_results(self, results: Dict[str, BenchmarkResult]) -> None:
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        if 'normal' not in results or 'optimized' not in results:
            return
        
        normal = results['normal']
        optimized = results['optimized']
        
        self.logger.info("\n=== ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
        self.logger.info(f"{'ëª¨ë“œ':<15} {'ì²˜ë¦¬ì‹œê°„(s)':<12} {'ì²˜ë¦¬ëŸ‰(items/s)':<15} {'GPU ì‚¬ìš©ë¥ (%)':<12} {'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ (%)':<15}")
        self.logger.info("-" * 80)
        self.logger.info(f"{'ì¼ë°˜ ëª¨ë“œ':<15} {normal.avg_processing_time:<12.3f} {normal.avg_throughput:<15.1f} {normal.avg_gpu_utilization:<12.1f} {normal.avg_memory_usage:<15.1f}")
        self.logger.info(f"{'ìµœì í™” ëª¨ë“œ':<15} {optimized.avg_processing_time:<12.3f} {optimized.avg_throughput:<15.1f} {optimized.avg_gpu_utilization:<12.1f} {optimized.avg_memory_usage:<15.1f}")
        
        # ê°œì„ ë¥  ê³„ì‚°
        if normal.avg_processing_time > 0:
            time_improvement = ((normal.avg_processing_time - optimized.avg_processing_time) / normal.avg_processing_time) * 100
            throughput_improvement = ((optimized.avg_throughput - normal.avg_throughput) / normal.avg_throughput) * 100 if normal.avg_throughput > 0 else 0
            
            self.logger.info("\n=== ê°œì„ ë¥  ===")
            self.logger.info(f"ì²˜ë¦¬ì‹œê°„ ê°œì„ : {time_improvement:+.1f}%")
            self.logger.info(f"ì²˜ë¦¬ëŸ‰ ê°œì„ : {throughput_improvement:+.1f}%")
            
            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
            target_improvement = 30.0  # 30% ê°œì„  ëª©í‘œ
            if throughput_improvement >= target_improvement:
                self.logger.info(f"âœ… ëª©í‘œ ë‹¬ì„±: {throughput_improvement:.1f}% >= {target_improvement}%")
            else:
                self.logger.warning(f"âŒ ëª©í‘œ ë¯¸ë‹¬ì„±: {throughput_improvement:.1f}% < {target_improvement}%")
    
    def benchmark_batch_sizes(
        self,
        process_fn: Callable,
        test_data: List[Any],
        batch_sizes: List[int] = None
    ) -> Dict[int, BenchmarkResult]:
        """
        ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
        
        Args:
            process_fn: ì²˜ë¦¬ í•¨ìˆ˜
            test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
            batch_sizes: í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ í¬ê¸°ë³„ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        if batch_sizes is None:
            batch_sizes = [1, 2, 4, 8, 16, 32]
        
        self.logger.info(f"ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘: {batch_sizes}")
        
        results = {}
        
        for batch_size in batch_sizes:
            self.logger.info(f"ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ì„ì‹œ ì„¤ì •ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ë³€ê²½
            original_batch_size = getattr(self.config, 'GPU_BATCH_SIZE', 8)
            self.config.GPU_BATCH_SIZE = batch_size
            
            try:
                result = self.benchmark_processing_function(
                    process_fn, test_data, f"Batch Size {batch_size}", iterations=3
                )
                results[batch_size] = result
                
            except Exception as e:
                self.logger.error(f"ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                
            finally:
                # ì›ë˜ ë°°ì¹˜ í¬ê¸° ë³µì›
                self.config.GPU_BATCH_SIZE = original_batch_size
        
        # ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°
        self._find_optimal_batch_size(results)
        
        return results
    
    def _find_optimal_batch_size(self, results: Dict[int, BenchmarkResult]) -> None:
        """ìµœì  ë°°ì¹˜ í¬ê¸° ì°¾ê¸°"""
        if not results:
            return
        
        best_batch_size = max(results.keys(), key=lambda k: results[k].avg_throughput)
        best_result = results[best_batch_size]
        
        self.logger.info("\n=== ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ ê²°ê³¼ ===")
        self.logger.info(f"{'ë°°ì¹˜ í¬ê¸°':<10} {'ì²˜ë¦¬ëŸ‰(items/s)':<15} {'ì²˜ë¦¬ì‹œê°„(s)':<12} {'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ (%)':<15}")
        self.logger.info("-" * 60)
        
        for batch_size in sorted(results.keys()):
            result = results[batch_size]
            marker = " â­" if batch_size == best_batch_size else ""
            self.logger.info(f"{batch_size:<10} {result.avg_throughput:<15.1f} {result.avg_processing_time:<12.3f} {result.avg_memory_usage:<15.1f}{marker}")
        
        self.logger.info(f"\nğŸ† ìµœì  ë°°ì¹˜ í¬ê¸°: {best_batch_size} (ì²˜ë¦¬ëŸ‰: {best_result.avg_throughput:.1f} items/s)")
    
    def benchmark_memory_efficiency(
        self,
        process_fn: Callable,
        data_sizes: List[int] = None
    ) -> Dict[int, BenchmarkResult]:
        """
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí‚¹
        
        Args:
            process_fn: ì²˜ë¦¬ í•¨ìˆ˜
            data_sizes: í…ŒìŠ¤íŠ¸í•  ë°ì´í„° í¬ê¸° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°ì´í„° í¬ê¸°ë³„ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        if data_sizes is None:
            data_sizes = [10, 50, 100, 200, 500]
        
        self.logger.info(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí‚¹ ì‹œì‘: {data_sizes}")
        
        results = {}
        
        for size in data_sizes:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_data = [f"test_item_{i}" for i in range(size)]
            
            self.logger.info(f"ë°ì´í„° í¬ê¸° {size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            try:
                result = self.benchmark_processing_function(
                    process_fn, test_data, f"Data Size {size}", iterations=3
                )
                results[size] = result
                
            except Exception as e:
                self.logger.error(f"ë°ì´í„° í¬ê¸° {size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„
        self._analyze_memory_efficiency(results)
        
        return results
    
    def _analyze_memory_efficiency(self, results: Dict[int, BenchmarkResult]) -> None:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„"""
        if not results:
            return
        
        self.logger.info("\n=== ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¶„ì„ ===")
        self.logger.info(f"{'ë°ì´í„° í¬ê¸°':<12} {'ì²˜ë¦¬ëŸ‰(items/s)':<15} {'ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ (%)':<15} {'íš¨ìœ¨ì„± ì§€ìˆ˜':<12}")
        self.logger.info("-" * 60)
        
        for size in sorted(results.keys()):
            result = results[size]
            # íš¨ìœ¨ì„± ì§€ìˆ˜ = ì²˜ë¦¬ëŸ‰ / ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
            efficiency = result.avg_throughput / max(result.avg_memory_usage, 1.0)
            
            self.logger.info(f"{size:<12} {result.avg_throughput:<15.1f} {result.avg_memory_usage:<15.1f} {efficiency:<12.2f}")
    
    def generate_report(self, results: Dict[str, BenchmarkResult]) -> Dict[str, Any]:
        """
        ì¢…í•© ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        
        Args:
            results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë“¤
            
        Returns:
            ì¢…í•© ë¦¬í¬íŠ¸
        """
        report = {
            'system_info': self.system_info,
            'timestamp': time.time(),
            'results_summary': {},
            'performance_metrics': {},
            'recommendations': []
        }
        
        # ê²°ê³¼ ìš”ì•½
        for test_name, result in results.items():
            report['results_summary'][test_name] = {
                'avg_processing_time': result.avg_processing_time,
                'avg_throughput': result.avg_throughput,
                'throughput_std': result.throughput_std,
                'avg_memory_usage': result.avg_memory_usage,
                'avg_gpu_utilization': result.avg_gpu_utilization
            }
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        if 'optimized' in results and 'normal' in results:
            optimized = results['optimized']
            normal = results['normal']
            
            if normal.avg_throughput > 0:
                improvement = ((optimized.avg_throughput - normal.avg_throughput) / normal.avg_throughput) * 100
                report['performance_metrics']['throughput_improvement'] = improvement
                report['performance_metrics']['meets_target'] = improvement >= 30.0
        
        # ì¶”ì²œì‚¬í•­ ìƒì„±
        report['recommendations'] = self._generate_recommendations(results)
        
        return report
    
    def _generate_recommendations(self, results: Dict[str, BenchmarkResult]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for test_name, result in results.items():
            # GPU ì‚¬ìš©ë¥ ì´ ë‚®ì€ ê²½ìš°
            if result.avg_gpu_utilization < 30.0:
                recommendations.append(f"{test_name}: GPU ì‚¬ìš©ë¥ ì´ ë‚®ìŠµë‹ˆë‹¤ ({result.avg_gpu_utilization:.1f}%). ë°°ì¹˜ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ GPU í™œìš©ë„ë¥¼ ë†’ì´ëŠ” ë°©ì•ˆì„ ê²€í† í•˜ì„¸ìš”.")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ì€ ê²½ìš°
            if result.avg_memory_usage > 80.0:
                recommendations.append(f"{test_name}: ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({result.avg_memory_usage:.1f}%). ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”.")
            
            # ì²˜ë¦¬ëŸ‰ ë³€ë™ì´ í° ê²½ìš°
            if result.throughput_std > result.avg_throughput * 0.2:
                recommendations.append(f"{test_name}: ì²˜ë¦¬ëŸ‰ ë³€ë™ì´ í½ë‹ˆë‹¤ (í‘œì¤€í¸ì°¨: {result.throughput_std:.1f}). ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ê²€í† í•˜ì„¸ìš”.")
        
        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        
        return recommendations


def create_test_process_function() -> Callable:
    """í…ŒìŠ¤íŠ¸ìš© ì²˜ë¦¬ í•¨ìˆ˜ ìƒì„±"""
    def test_process(data_batch):
        """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜"""
        # ì‹¤ì œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        time.sleep(0.001 * len(data_batch))  # 1ms per item
        return [f"processed_{item}" for item in data_batch]
    
    return test_process


if __name__ == "__main__":
    # ë²¤ì¹˜ë§ˆí‚¹ ì˜ˆì œ ì‹¤í–‰
    config = Config()
    benchmarker = PerformanceBenchmarker(config)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = [f"test_item_{i}" for i in range(50)]
    process_fn = create_test_process_function()
    
    # ë¹„êµ ë²¤ì¹˜ë§ˆí‚¹ ì‹¤í–‰
    results = benchmarker.compare_optimization_modes(process_fn, test_data)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = benchmarker.generate_report(results)
    print(f"\nì„±ëŠ¥ ê°œì„ ë¥ : {report['performance_metrics'].get('throughput_improvement', 0):.1f}%")
    print(f"ëª©í‘œ ë‹¬ì„±: {'âœ…' if report['performance_metrics'].get('meets_target', False) else 'âŒ'}")