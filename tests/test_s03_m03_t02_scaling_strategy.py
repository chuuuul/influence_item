#!/usr/bin/env python3
"""
S03_M03_T02 í†µí•© í…ŒìŠ¤íŠ¸: í™•ì¥ì„± ë° ìŠ¤ì¼€ì¼ë§ ì „ëµ
ë¶€í•˜ ë¶„ì‚°, ìë™ ìŠ¤ì¼€ì¼ë§, ë¦¬ì†ŒìŠ¤ ìµœì í™” ì „ëµ ê²€ì¦
"""

import sys
import os
import time
import json
import threading
import queue
import psutil
from typing import Dict, List, Any
from datetime import datetime, timedelta
import concurrent.futures
import asyncio

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append('/Users/chul/Documents/claude/influence_item')

def test_load_balancing():
    """ë¶€í•˜ ë¶„ì‚° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë¶€í•˜ ë¶„ì‚° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        import random
        import hashlib
        from enum import Enum
        
        # ë¶€í•˜ ë¶„ì‚° ì•Œê³ ë¦¬ì¦˜ ì •ì˜
        class LoadBalancingAlgorithm(Enum):
            ROUND_ROBIN = "round_robin"
            LEAST_CONNECTIONS = "least_connections"
            WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
            HASH_BASED = "hash_based"
            RANDOM = "random"
        
        # ì„œë²„ ë…¸ë“œ í´ë˜ìŠ¤
        class ServerNode:
            def __init__(self, node_id, capacity=100, weight=1):
                self.node_id = node_id
                self.capacity = capacity
                self.weight = weight
                self.current_load = 0
                self.active_connections = 0
                self.total_requests = 0
                self.failed_requests = 0
                self.is_healthy = True
                self.last_health_check = datetime.now()
                self.response_times = []
            
            def process_request(self, request_complexity=1):
                """ìš”ì²­ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
                if not self.is_healthy:
                    self.failed_requests += 1
                    raise Exception(f"Node {self.node_id} is unhealthy")
                
                if self.current_load + request_complexity > self.capacity:
                    self.failed_requests += 1
                    raise Exception(f"Node {self.node_id} is overloaded")
                
                # ìš”ì²­ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                self.active_connections += 1
                self.current_load += request_complexity
                self.total_requests += 1
                
                # ì‘ë‹µ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ë¶€í•˜ì— ë”°ë¼ ì¦ê°€)
                base_response_time = 0.1
                load_factor = self.current_load / self.capacity
                response_time = base_response_time * (1 + load_factor)
                
                time.sleep(response_time)  # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                
                self.response_times.append(response_time)
                if len(self.response_times) > 100:  # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
                    self.response_times.pop(0)
                
                # ìš”ì²­ ì™„ë£Œ
                self.active_connections -= 1
                self.current_load -= request_complexity
                
                return {
                    'node_id': self.node_id,
                    'response_time': response_time,
                    'current_load': self.current_load
                }
            
            def health_check(self):
                """í—¬ìŠ¤ ì²´í¬"""
                # ê°„ë‹¨í•œ í—¬ìŠ¤ ì²´í¬ ë¡œì§
                load_ratio = self.current_load / self.capacity
                avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0
                
                self.is_healthy = (
                    load_ratio < 0.95 and  # 95% ì´í•˜ ë¶€í•˜
                    avg_response_time < 2.0 and  # 2ì´ˆ ì´í•˜ ì‘ë‹µì‹œê°„
                    self.failed_requests / max(self.total_requests, 1) < 0.1  # 10% ì´í•˜ ì‹¤íŒ¨ìœ¨
                )
                
                self.last_health_check = datetime.now()
                return self.is_healthy
            
            def get_stats(self):
                """ë…¸ë“œ í†µê³„"""
                success_rate = (self.total_requests - self.failed_requests) / max(self.total_requests, 1) * 100
                avg_response_time = sum(self.response_times) / len(self.response_times) if self.response_times else 0
                
                return {
                    'node_id': self.node_id,
                    'capacity': self.capacity,
                    'weight': self.weight,
                    'current_load': self.current_load,
                    'load_percentage': (self.current_load / self.capacity) * 100,
                    'active_connections': self.active_connections,
                    'total_requests': self.total_requests,
                    'failed_requests': self.failed_requests,
                    'success_rate': success_rate,
                    'avg_response_time': avg_response_time,
                    'is_healthy': self.is_healthy,
                    'last_health_check': self.last_health_check.isoformat()
                }
        
        # ë¡œë“œ ë°¸ëŸ°ì„œ í´ë˜ìŠ¤
        class LoadBalancer:
            def __init__(self, algorithm=LoadBalancingAlgorithm.ROUND_ROBIN):
                self.algorithm = algorithm
                self.nodes = {}
                self.current_index = 0
                self.request_count = 0
                self.failed_requests = 0
                self.health_check_interval = 30  # 30ì´ˆ
                self.last_health_check = datetime.now()
            
            def add_node(self, node: ServerNode):
                """ë…¸ë“œ ì¶”ê°€"""
                self.nodes[node.node_id] = node
                print(f"    â• ë…¸ë“œ ì¶”ê°€: {node.node_id} (ìš©ëŸ‰: {node.capacity}, ê°€ì¤‘ì¹˜: {node.weight})")
            
            def remove_node(self, node_id):
                """ë…¸ë“œ ì œê±°"""
                if node_id in self.nodes:
                    del self.nodes[node_id]
                    print(f"    â– ë…¸ë“œ ì œê±°: {node_id}")
            
            def select_node(self, request_key=None):
                """ë…¸ë“œ ì„ íƒ"""
                healthy_nodes = {nid: node for nid, node in self.nodes.items() if node.is_healthy}
                
                if not healthy_nodes:
                    raise Exception("No healthy nodes available")
                
                if self.algorithm == LoadBalancingAlgorithm.ROUND_ROBIN:
                    return self._round_robin(healthy_nodes)
                elif self.algorithm == LoadBalancingAlgorithm.LEAST_CONNECTIONS:
                    return self._least_connections(healthy_nodes)
                elif self.algorithm == LoadBalancingAlgorithm.WEIGHTED_ROUND_ROBIN:
                    return self._weighted_round_robin(healthy_nodes)
                elif self.algorithm == LoadBalancingAlgorithm.HASH_BASED:
                    return self._hash_based(healthy_nodes, request_key)
                elif self.algorithm == LoadBalancingAlgorithm.RANDOM:
                    return self._random(healthy_nodes)
                else:
                    return self._round_robin(healthy_nodes)
            
            def _round_robin(self, nodes):
                """ë¼ìš´ë“œ ë¡œë¹ˆ ì•Œê³ ë¦¬ì¦˜"""
                node_list = list(nodes.values())
                selected_node = node_list[self.current_index % len(node_list)]
                self.current_index += 1
                return selected_node
            
            def _least_connections(self, nodes):
                """ìµœì†Œ ì—°ê²° ì•Œê³ ë¦¬ì¦˜"""
                return min(nodes.values(), key=lambda n: n.active_connections)
            
            def _weighted_round_robin(self, nodes):
                """ê°€ì¤‘ì¹˜ ë¼ìš´ë“œ ë¡œë¹ˆ ì•Œê³ ë¦¬ì¦˜"""
                # ê°€ì¤‘ì¹˜ì— ë”°ë¥¸ ì„ íƒ í™•ë¥  ê³„ì‚°
                total_weight = sum(node.weight for node in nodes.values())
                random_value = random.uniform(0, total_weight)
                
                cumulative_weight = 0
                for node in nodes.values():
                    cumulative_weight += node.weight
                    if random_value <= cumulative_weight:
                        return node
                
                # ê¸°ë³¸ê°’ìœ¼ë¡œ ì²« ë²ˆì§¸ ë…¸ë“œ ë°˜í™˜
                return next(iter(nodes.values()))
            
            def _hash_based(self, nodes, request_key):
                """í•´ì‹œ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜"""
                if not request_key:
                    request_key = str(self.request_count)
                
                hash_value = int(hashlib.md5(str(request_key).encode()).hexdigest(), 16)
                node_list = list(nodes.values())
                selected_node = node_list[hash_value % len(node_list)]
                return selected_node
            
            def _random(self, nodes):
                """ëœë¤ ì•Œê³ ë¦¬ì¦˜"""
                return random.choice(list(nodes.values()))
            
            def process_request(self, request_complexity=1, request_key=None):
                """ìš”ì²­ ì²˜ë¦¬"""
                self.request_count += 1
                
                try:
                    # í—¬ìŠ¤ ì²´í¬ (ì£¼ê¸°ì ìœ¼ë¡œ)
                    if (datetime.now() - self.last_health_check).seconds > self.health_check_interval:
                        self.perform_health_checks()
                    
                    # ë…¸ë“œ ì„ íƒ
                    selected_node = self.select_node(request_key)
                    
                    # ìš”ì²­ ì²˜ë¦¬
                    result = selected_node.process_request(request_complexity)
                    result['algorithm'] = self.algorithm.value
                    result['request_id'] = self.request_count
                    
                    return result
                
                except Exception as e:
                    self.failed_requests += 1
                    return {
                        'error': str(e),
                        'request_id': self.request_count,
                        'algorithm': self.algorithm.value
                    }
            
            def perform_health_checks(self):
                """ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ í—¬ìŠ¤ ì²´í¬ ìˆ˜í–‰"""
                for node in self.nodes.values():
                    node.health_check()
                self.last_health_check = datetime.now()
            
            def get_cluster_stats(self):
                """í´ëŸ¬ìŠ¤í„° í†µê³„"""
                total_capacity = sum(node.capacity for node in self.nodes.values())
                total_load = sum(node.current_load for node in self.nodes.values())
                healthy_nodes = sum(1 for node in self.nodes.values() if node.is_healthy)
                
                cluster_load_percentage = (total_load / total_capacity * 100) if total_capacity > 0 else 0
                success_rate = (self.request_count - self.failed_requests) / max(self.request_count, 1) * 100
                
                return {
                    'algorithm': self.algorithm.value,
                    'total_nodes': len(self.nodes),
                    'healthy_nodes': healthy_nodes,
                    'total_capacity': total_capacity,
                    'total_load': total_load,
                    'cluster_load_percentage': cluster_load_percentage,
                    'total_requests': self.request_count,
                    'failed_requests': self.failed_requests,
                    'success_rate': success_rate,
                    'nodes': {nid: node.get_stats() for nid, node in self.nodes.items()}
                }
        
        # ë¡œë“œ ë°¸ëŸ°ì„œ í…ŒìŠ¤íŠ¸
        lb = LoadBalancer(LoadBalancingAlgorithm.LEAST_CONNECTIONS)
        
        # ë…¸ë“œ ì¶”ê°€
        nodes = [
            ServerNode('node-1', capacity=100, weight=1),
            ServerNode('node-2', capacity=150, weight=2),
            ServerNode('node-3', capacity=80, weight=1)
        ]
        
        for node in nodes:
            lb.add_node(node)
        
        # ë™ì‹œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
        def simulate_requests(lb, num_requests=50):
            results = []
            for i in range(num_requests):
                complexity = random.randint(1, 5)
                result = lb.process_request(complexity, f"user_{i}")
                results.append(result)
                time.sleep(0.01)  # ì‘ì€ ì§€ì—°
            return results
        
        # ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¡œ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            future_results = [
                executor.submit(simulate_requests, lb, 20)
                for _ in range(3)
            ]
            
            all_results = []
            for future in concurrent.futures.as_completed(future_results):
                all_results.extend(future.result())
        
        # ê²°ê³¼ ê²€ì¦
        cluster_stats = lb.get_cluster_stats()
        
        assert cluster_stats['total_requests'] > 0, "ìš”ì²­ì´ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        assert cluster_stats['healthy_nodes'] >= 2, "ì¶©ë¶„í•œ ìˆ˜ì˜ ë…¸ë“œê°€ healthy ìƒíƒœê°€ ì•„ë‹™ë‹ˆë‹¤."
        assert cluster_stats['success_rate'] > 80, f"ì„±ê³µë¥ ì´ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {cluster_stats['success_rate']:.1f}%"
        
        # ë¶€í•˜ ë¶„ì‚° ê²€ì¦ (ê° ë…¸ë“œê°€ ìš”ì²­ì„ ë°›ì•˜ëŠ”ì§€)
        successful_results = [r for r in all_results if 'node_id' in r]
        nodes_used = set(r['node_id'] for r in successful_results)
        assert len(nodes_used) >= 2, "ë¶€í•˜ ë¶„ì‚°ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"  âš–ï¸ ë¶€í•˜ ë¶„ì‚° ê²°ê³¼: {cluster_stats['total_requests']}ê°œ ìš”ì²­, "
              f"ì„±ê³µë¥  {cluster_stats['success_rate']:.1f}%")
        print(f"  ğŸ–¥ï¸ ì‚¬ìš©ëœ ë…¸ë“œ: {len(nodes_used)}ê°œ, "
              f"í´ëŸ¬ìŠ¤í„° ë¶€í•˜: {cluster_stats['cluster_load_percentage']:.1f}%")
        
        print("âœ… ë¶€í•˜ ë¶„ì‚° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ë¶€í•˜ ë¶„ì‚° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_auto_scaling():
    """ìë™ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ìë™ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from enum import Enum
        import threading
        import time
        
        # ìŠ¤ì¼€ì¼ë§ ë™ì‘ ì •ì˜
        class ScalingAction(Enum):
            SCALE_UP = "scale_up"
            SCALE_DOWN = "scale_down"
            NO_ACTION = "no_action"
        
        # ìŠ¤ì¼€ì¼ë§ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤
        class ScalingMetrics:
            def __init__(self):
                self.cpu_percent = 0.0
                self.memory_percent = 0.0
                self.active_requests = 0
                self.response_time = 0.0
                self.error_rate = 0.0
                self.timestamp = datetime.now()
            
            def update(self, cpu=None, memory=None, requests=None, response_time=None, error_rate=None):
                """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
                if cpu is not None:
                    self.cpu_percent = cpu
                if memory is not None:
                    self.memory_percent = memory
                if requests is not None:
                    self.active_requests = requests
                if response_time is not None:
                    self.response_time = response_time
                if error_rate is not None:
                    self.error_rate = error_rate
                self.timestamp = datetime.now()
            
            def to_dict(self):
                return {
                    'cpu_percent': self.cpu_percent,
                    'memory_percent': self.memory_percent,
                    'active_requests': self.active_requests,
                    'response_time': self.response_time,
                    'error_rate': self.error_rate,
                    'timestamp': self.timestamp.isoformat()
                }
        
        # ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ í´ë˜ìŠ¤
        class AutoScaler:
            def __init__(self):
                self.instances = {}
                self.min_instances = 1
                self.max_instances = 10
                self.target_cpu_percent = 70.0
                self.target_response_time = 2.0
                self.scale_up_threshold = 80.0
                self.scale_down_threshold = 30.0
                self.cooldown_period = 300  # 5ë¶„
                self.last_scaling_action = None
                self.last_scaling_time = datetime.now() - timedelta(minutes=10)
                self.scaling_history = []
                self.metrics_history = []
            
            def add_instance(self, instance_id, capacity=100):
                """ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€"""
                self.instances[instance_id] = {
                    'instance_id': instance_id,
                    'capacity': capacity,
                    'current_load': 0,
                    'created_at': datetime.now(),
                    'status': 'running'
                }
                print(f"    â• ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€: {instance_id}")
                return instance_id
            
            def remove_instance(self, instance_id):
                """ì¸ìŠ¤í„´ìŠ¤ ì œê±°"""
                if instance_id in self.instances:
                    del self.instances[instance_id]
                    print(f"    â– ì¸ìŠ¤í„´ìŠ¤ ì œê±°: {instance_id}")
                    return True
                return False
            
            def evaluate_scaling(self, metrics: ScalingMetrics):
                """ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± í‰ê°€"""
                # ì¿¨ë‹¤ìš´ ê¸°ê°„ í™•ì¸
                time_since_last_scaling = (datetime.now() - self.last_scaling_time).total_seconds()
                if time_since_last_scaling < self.cooldown_period:
                    return ScalingAction.NO_ACTION, "Cooling down"
                
                current_instances = len(self.instances)
                
                # ìŠ¤ì¼€ì¼ ì—… ì¡°ê±´ í™•ì¸
                scale_up_conditions = [
                    metrics.cpu_percent > self.scale_up_threshold,
                    metrics.response_time > self.target_response_time,
                    metrics.error_rate > 5.0,  # 5% ì´ìƒ ì—ëŸ¬ìœ¨
                    current_instances < self.max_instances
                ]
                
                if all([scale_up_conditions[0] or scale_up_conditions[1] or scale_up_conditions[2], scale_up_conditions[3]]):
                    return ScalingAction.SCALE_UP, f"High resource usage: CPU {metrics.cpu_percent}%, RT {metrics.response_time}s"
                
                # ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì¡°ê±´ í™•ì¸
                scale_down_conditions = [
                    metrics.cpu_percent < self.scale_down_threshold,
                    metrics.response_time < self.target_response_time * 0.5,
                    metrics.error_rate < 1.0,  # 1% ì´í•˜ ì—ëŸ¬ìœ¨
                    current_instances > self.min_instances
                ]
                
                if all(scale_down_conditions):
                    return ScalingAction.SCALE_DOWN, f"Low resource usage: CPU {metrics.cpu_percent}%, RT {metrics.response_time}s"
                
                return ScalingAction.NO_ACTION, "No scaling needed"
            
            def execute_scaling(self, action: ScalingAction, reason: str):
                """ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
                if action == ScalingAction.NO_ACTION:
                    return None
                
                current_instances = len(self.instances)
                
                if action == ScalingAction.SCALE_UP:
                    # ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
                    new_instance_id = f"instance-{current_instances + 1}-{int(time.time())}"
                    self.add_instance(new_instance_id)
                    
                    scaling_event = {
                        'action': action.value,
                        'instance_id': new_instance_id,
                        'reason': reason,
                        'timestamp': datetime.now(),
                        'instances_before': current_instances,
                        'instances_after': current_instances + 1
                    }
                
                elif action == ScalingAction.SCALE_DOWN:
                    # ê°€ì¥ ìµœê·¼ì— ìƒì„±ëœ ì¸ìŠ¤í„´ìŠ¤ ì œê±°
                    if self.instances:
                        oldest_instance = min(
                            self.instances.items(),
                            key=lambda x: x[1]['created_at']
                        )
                        removed_instance_id = oldest_instance[0]
                        self.remove_instance(removed_instance_id)
                        
                        scaling_event = {
                            'action': action.value,
                            'instance_id': removed_instance_id,
                            'reason': reason,
                            'timestamp': datetime.now(),
                            'instances_before': current_instances,
                            'instances_after': current_instances - 1
                        }
                    else:
                        return None
                
                # ìŠ¤ì¼€ì¼ë§ ê¸°ë¡
                self.scaling_history.append(scaling_event)
                self.last_scaling_action = action
                self.last_scaling_time = datetime.now()
                
                print(f"    ğŸ”„ ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰: {action.value} - {reason}")
                
                return scaling_event
            
            def monitor_and_scale(self, metrics: ScalingMetrics):
                """ëª¨ë‹ˆí„°ë§ ë° ìë™ ìŠ¤ì¼€ì¼ë§"""
                # ë©”íŠ¸ë¦­ ê¸°ë¡
                self.metrics_history.append(metrics.to_dict())
                
                # ìµœê·¼ ë©”íŠ¸ë¦­ë§Œ ìœ ì§€ (ìµœëŒ€ 1000ê°œ)
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]
                
                # ìŠ¤ì¼€ì¼ë§ í‰ê°€ ë° ì‹¤í–‰
                action, reason = self.evaluate_scaling(metrics)
                scaling_event = self.execute_scaling(action, reason)
                
                return {
                    'action': action.value,
                    'reason': reason,
                    'scaling_event': scaling_event,
                    'current_instances': len(self.instances),
                    'metrics': metrics.to_dict()
                }
            
            def get_scaling_stats(self):
                """ìŠ¤ì¼€ì¼ë§ í†µê³„"""
                scale_up_count = len([h for h in self.scaling_history if h['action'] == 'scale_up'])
                scale_down_count = len([h for h in self.scaling_history if h['action'] == 'scale_down'])
                
                return {
                    'current_instances': len(self.instances),
                    'min_instances': self.min_instances,
                    'max_instances': self.max_instances,
                    'total_scaling_events': len(self.scaling_history),
                    'scale_up_events': scale_up_count,
                    'scale_down_events': scale_down_count,
                    'last_scaling_time': self.last_scaling_time.isoformat() if self.last_scaling_time else None,
                    'metrics_collected': len(self.metrics_history),
                    'instances': list(self.instances.keys())
                }
        
        # ì˜¤í† ìŠ¤ì¼€ì¼ëŸ¬ í…ŒìŠ¤íŠ¸
        autoscaler = AutoScaler()
        autoscaler.min_instances = 2
        autoscaler.max_instances = 5
        autoscaler.cooldown_period = 5  # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 5ì´ˆë¡œ ë‹¨ì¶•
        
        # ì´ˆê¸° ì¸ìŠ¤í„´ìŠ¤ ì¶”ê°€
        autoscaler.add_instance("initial-instance-1")
        autoscaler.add_instance("initial-instance-2")
        
        # ìŠ¤ì¼€ì¼ë§ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
        scenarios = [
            # ì‹œë‚˜ë¦¬ì˜¤ 1: ë†’ì€ CPU ì‚¬ìš©ë¥  (ìŠ¤ì¼€ì¼ ì—… ìœ ë°œ)
            ScalingMetrics(),
            ScalingMetrics(),
            ScalingMetrics(),
        ]
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
        scenarios[0].update(cpu=85.0, memory=70.0, requests=150, response_time=2.5, error_rate=2.0)
        scenarios[1].update(cpu=20.0, memory=30.0, requests=20, response_time=0.5, error_rate=0.1)
        scenarios[2].update(cpu=90.0, memory=85.0, requests=200, response_time=4.0, error_rate=8.0)
        
        results = []
        
        # ê° ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
        for i, metrics in enumerate(scenarios):
            print(f"    ğŸ“Š ì‹œë‚˜ë¦¬ì˜¤ {i+1} ì‹¤í–‰: CPU {metrics.cpu_percent}%, RT {metrics.response_time}s")
            
            result = autoscaler.monitor_and_scale(metrics)
            results.append(result)
            
            # ì¿¨ë‹¤ìš´ ê¸°ê°„ ëŒ€ê¸° (í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì§§ê²Œ)
            time.sleep(6)
        
        # ê²°ê³¼ ê²€ì¦
        stats = autoscaler.get_scaling_stats()
        
        assert stats['current_instances'] >= autoscaler.min_instances, "ìµœì†Œ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ë¯¸ë‹¬"
        assert stats['current_instances'] <= autoscaler.max_instances, "ìµœëŒ€ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ì´ˆê³¼"
        assert stats['total_scaling_events'] > 0, "ìŠ¤ì¼€ì¼ë§ ì´ë²¤íŠ¸ê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        assert len(stats['instances']) == stats['current_instances'], "ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ ë¶ˆì¼ì¹˜"
        
        # ìŠ¤ì¼€ì¼ ì—…ì´ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸ (ë†’ì€ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ)
        high_load_results = [r for r in results if r['action'] == 'scale_up']
        assert len(high_load_results) > 0, "ë†’ì€ ë¶€í•˜ ìƒí™©ì—ì„œ ìŠ¤ì¼€ì¼ ì—…ì´ ë°œìƒí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"  ğŸ“ˆ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼: {stats['total_scaling_events']}ê°œ ì´ë²¤íŠ¸")
        print(f"  ğŸ–¥ï¸ ìµœì¢… ì¸ìŠ¤í„´ìŠ¤: {stats['current_instances']}ê°œ "
              f"(ì—… {stats['scale_up_events']}íšŒ, ë‹¤ìš´ {stats['scale_down_events']}íšŒ)")
        
        print("âœ… ìë™ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ìë™ ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_resource_optimization():
    """ë¦¬ì†ŒìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë¦¬ì†ŒìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        import psutil
        from collections import deque
        import threading
        
        # ë¦¬ì†ŒìŠ¤ ìµœì í™” ê´€ë¦¬ì í´ë˜ìŠ¤
        class ResourceOptimizer:
            def __init__(self):
                self.resource_pools = {}
                self.cache_systems = {}
                self.optimization_rules = []
                self.resource_usage_history = deque(maxlen=1000)
                self.optimization_events = []
                self.monitoring_active = False
                self.monitor_thread = None
            
            def create_resource_pool(self, pool_name, pool_type, initial_size=5, max_size=20):
                """ë¦¬ì†ŒìŠ¤ í’€ ìƒì„±"""
                self.resource_pools[pool_name] = {
                    'type': pool_type,
                    'pool': queue.Queue(),
                    'active_resources': set(),
                    'initial_size': initial_size,
                    'max_size': max_size,
                    'created_count': 0,
                    'acquired_count': 0,
                    'released_count': 0,
                    'peak_usage': 0
                }
                
                # ì´ˆê¸° ë¦¬ì†ŒìŠ¤ ìƒì„±
                for i in range(initial_size):
                    resource = self._create_resource(pool_type, f"{pool_name}_{i}")
                    self.resource_pools[pool_name]['pool'].put(resource)
                    self.resource_pools[pool_name]['created_count'] += 1
                
                print(f"    ğŸŠ ë¦¬ì†ŒìŠ¤ í’€ ìƒì„±: {pool_name} ({pool_type}, ì´ˆê¸° í¬ê¸°: {initial_size})")
            
            def _create_resource(self, resource_type, resource_id):
                """ë¦¬ì†ŒìŠ¤ ìƒì„±"""
                if resource_type == 'database_connection':
                    return {
                        'id': resource_id,
                        'type': resource_type,
                        'created_at': datetime.now(),
                        'last_used': None,
                        'usage_count': 0,
                        'connection': f"mock_db_connection_{resource_id}"
                    }
                elif resource_type == 'worker_thread':
                    return {
                        'id': resource_id,
                        'type': resource_type,
                        'created_at': datetime.now(),
                        'last_used': None,
                        'usage_count': 0,
                        'thread': f"mock_thread_{resource_id}"
                    }
                elif resource_type == 'gpu_context':
                    return {
                        'id': resource_id,
                        'type': resource_type,
                        'created_at': datetime.now(),
                        'last_used': None,
                        'usage_count': 0,
                        'context': f"mock_gpu_context_{resource_id}"
                    }
                else:
                    return {
                        'id': resource_id,
                        'type': resource_type,
                        'created_at': datetime.now(),
                        'last_used': None,
                        'usage_count': 0
                    }
            
            def acquire_resource(self, pool_name, timeout=5.0):
                """ë¦¬ì†ŒìŠ¤ íšë“"""
                if pool_name not in self.resource_pools:
                    raise ValueError(f"Resource pool '{pool_name}' not found")
                
                pool_info = self.resource_pools[pool_name]
                
                try:
                    # í’€ì—ì„œ ë¦¬ì†ŒìŠ¤ íšë“ ì‹œë„
                    resource = pool_info['pool'].get(timeout=timeout)
                    
                    # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
                    resource['last_used'] = datetime.now()
                    resource['usage_count'] += 1
                    
                    pool_info['active_resources'].add(resource['id'])
                    pool_info['acquired_count'] += 1
                    
                    # í”¼í¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
                    current_usage = len(pool_info['active_resources'])
                    if current_usage > pool_info['peak_usage']:
                        pool_info['peak_usage'] = current_usage
                    
                    return resource
                
                except queue.Empty:
                    # í’€ì´ ë¹„ì–´ìˆê³  ìµœëŒ€ í¬ê¸°ì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ìƒˆ ë¦¬ì†ŒìŠ¤ ìƒì„±
                    if pool_info['created_count'] < pool_info['max_size']:
                        resource = self._create_resource(
                            pool_info['type'], 
                            f"{pool_name}_{pool_info['created_count']}"
                        )
                        pool_info['created_count'] += 1
                        pool_info['acquired_count'] += 1
                        pool_info['active_resources'].add(resource['id'])
                        
                        resource['last_used'] = datetime.now()
                        resource['usage_count'] += 1
                        
                        print(f"    â• ìƒˆ ë¦¬ì†ŒìŠ¤ ìƒì„±: {resource['id']}")
                        return resource
                    else:
                        raise Exception(f"Resource pool '{pool_name}' exhausted")
            
            def release_resource(self, pool_name, resource):
                """ë¦¬ì†ŒìŠ¤ ë°˜í™˜"""
                if pool_name not in self.resource_pools:
                    raise ValueError(f"Resource pool '{pool_name}' not found")
                
                pool_info = self.resource_pools[pool_name]
                
                if resource['id'] in pool_info['active_resources']:
                    pool_info['active_resources'].remove(resource['id'])
                    pool_info['released_count'] += 1
                    
                    # ë¦¬ì†ŒìŠ¤ë¥¼ í’€ì— ë°˜í™˜
                    pool_info['pool'].put(resource)
            
            def add_cache_system(self, cache_name, max_size=1000, ttl=3600):
                """ìºì‹œ ì‹œìŠ¤í…œ ì¶”ê°€"""
                self.cache_systems[cache_name] = {
                    'cache': {},
                    'access_times': {},
                    'max_size': max_size,
                    'ttl': ttl,
                    'hit_count': 0,
                    'miss_count': 0,
                    'eviction_count': 0
                }
                print(f"    ğŸ’¾ ìºì‹œ ì‹œìŠ¤í…œ ìƒì„±: {cache_name} (ìµœëŒ€ í¬ê¸°: {max_size}, TTL: {ttl}ì´ˆ)")
            
            def cache_get(self, cache_name, key):
                """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
                if cache_name not in self.cache_systems:
                    return None
                
                cache_info = self.cache_systems[cache_name]
                current_time = datetime.now()
                
                if key in cache_info['cache']:
                    # TTL í™•ì¸
                    access_time = cache_info['access_times'][key]
                    if (current_time - access_time).total_seconds() < cache_info['ttl']:
                        cache_info['hit_count'] += 1
                        cache_info['access_times'][key] = current_time  # ì•¡ì„¸ìŠ¤ ì‹œê°„ ì—…ë°ì´íŠ¸
                        return cache_info['cache'][key]
                    else:
                        # TTL ë§Œë£Œëœ í‚¤ ì œê±°
                        del cache_info['cache'][key]
                        del cache_info['access_times'][key]
                
                cache_info['miss_count'] += 1
                return None
            
            def cache_set(self, cache_name, key, value):
                """ìºì‹œì— ê°’ ì €ì¥"""
                if cache_name not in self.cache_systems:
                    return False
                
                cache_info = self.cache_systems[cache_name]
                current_time = datetime.now()
                
                # ìºì‹œ í¬ê¸° í™•ì¸ ë° LRU ì œê±°
                if len(cache_info['cache']) >= cache_info['max_size']:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = min(cache_info['access_times'], key=cache_info['access_times'].get)
                    del cache_info['cache'][oldest_key]
                    del cache_info['access_times'][oldest_key]
                    cache_info['eviction_count'] += 1
                
                cache_info['cache'][key] = value
                cache_info['access_times'][key] = current_time
                return True
            
            def add_optimization_rule(self, rule):
                """ìµœì í™” ê·œì¹™ ì¶”ê°€"""
                self.optimization_rules.append(rule)
            
            def collect_resource_metrics(self):
                """ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
                current_time = datetime.now()
                
                metrics = {
                    'timestamp': current_time,
                    'system_cpu': psutil.cpu_percent(),
                    'system_memory': psutil.virtual_memory().percent,
                    'resource_pools': {},
                    'cache_systems': {}
                }
                
                # ë¦¬ì†ŒìŠ¤ í’€ ë©”íŠ¸ë¦­
                for pool_name, pool_info in self.resource_pools.items():
                    metrics['resource_pools'][pool_name] = {
                        'active_resources': len(pool_info['active_resources']),
                        'available_resources': pool_info['pool'].qsize(),
                        'total_created': pool_info['created_count'],
                        'total_acquired': pool_info['acquired_count'],
                        'total_released': pool_info['released_count'],
                        'peak_usage': pool_info['peak_usage'],
                        'utilization': (len(pool_info['active_resources']) / pool_info['max_size']) * 100
                    }
                
                # ìºì‹œ ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
                for cache_name, cache_info in self.cache_systems.items():
                    total_requests = cache_info['hit_count'] + cache_info['miss_count']
                    hit_rate = (cache_info['hit_count'] / total_requests * 100) if total_requests > 0 else 0
                    
                    metrics['cache_systems'][cache_name] = {
                        'cache_size': len(cache_info['cache']),
                        'max_size': cache_info['max_size'],
                        'hit_count': cache_info['hit_count'],
                        'miss_count': cache_info['miss_count'],
                        'hit_rate': hit_rate,
                        'eviction_count': cache_info['eviction_count'],
                        'utilization': (len(cache_info['cache']) / cache_info['max_size']) * 100
                    }
                
                self.resource_usage_history.append(metrics)
                return metrics
            
            def optimize_resources(self):
                """ë¦¬ì†ŒìŠ¤ ìµœì í™” ì‹¤í–‰"""
                if not self.resource_usage_history:
                    return []
                
                optimizations = []
                latest_metrics = self.resource_usage_history[-1]
                
                # ë¦¬ì†ŒìŠ¤ í’€ ìµœì í™”
                for pool_name, pool_metrics in latest_metrics['resource_pools'].items():
                    # ë†’ì€ ì‚¬ìš©ë¥  -> í’€ í¬ê¸° ì¦ê°€ ê¶Œì¥
                    if pool_metrics['utilization'] > 90:
                        optimizations.append({
                            'type': 'resource_pool_expansion',
                            'target': pool_name,
                            'current_size': self.resource_pools[pool_name]['max_size'],
                            'recommended_size': min(self.resource_pools[pool_name]['max_size'] * 2, 50),
                            'reason': f"High utilization: {pool_metrics['utilization']:.1f}%"
                        })
                    
                    # ë‚®ì€ ì‚¬ìš©ë¥  -> í’€ í¬ê¸° ì¶•ì†Œ ê¶Œì¥
                    elif pool_metrics['utilization'] < 20 and self.resource_pools[pool_name]['max_size'] > 5:
                        optimizations.append({
                            'type': 'resource_pool_reduction',
                            'target': pool_name,
                            'current_size': self.resource_pools[pool_name]['max_size'],
                            'recommended_size': max(self.resource_pools[pool_name]['max_size'] // 2, 5),
                            'reason': f"Low utilization: {pool_metrics['utilization']:.1f}%"
                        })
                
                # ìºì‹œ ìµœì í™”
                for cache_name, cache_metrics in latest_metrics['cache_systems'].items():
                    # ë‚®ì€ ì ì¤‘ë¥  -> ìºì‹œ í¬ê¸° ì¦ê°€ ë˜ëŠ” TTL ì¡°ì • ê¶Œì¥
                    if cache_metrics['hit_rate'] < 50:
                        optimizations.append({
                            'type': 'cache_optimization',
                            'target': cache_name,
                            'current_hit_rate': cache_metrics['hit_rate'],
                            'current_size': cache_metrics['max_size'],
                            'recommended_size': cache_metrics['max_size'] * 2,
                            'reason': f"Low hit rate: {cache_metrics['hit_rate']:.1f}%"
                        })
                    
                    # ë†’ì€ ì œê±°ìœ¨ -> ìºì‹œ í¬ê¸° ì¦ê°€ ê¶Œì¥
                    if cache_metrics['eviction_count'] > 100:
                        optimizations.append({
                            'type': 'cache_expansion',
                            'target': cache_name,
                            'current_size': cache_metrics['max_size'],
                            'recommended_size': cache_metrics['max_size'] * 1.5,
                            'eviction_count': cache_metrics['eviction_count'],
                            'reason': f"High eviction count: {cache_metrics['eviction_count']}"
                        })
                
                # ìµœì í™” ì´ë²¤íŠ¸ ê¸°ë¡
                if optimizations:
                    self.optimization_events.append({
                        'timestamp': datetime.now(),
                        'optimizations': optimizations
                    })
                
                return optimizations
            
            def get_optimization_summary(self):
                """ìµœì í™” ìš”ì•½ ì •ë³´"""
                if not self.resource_usage_history:
                    return {}
                
                latest_metrics = self.resource_usage_history[-1]
                
                # ì „ì²´ ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ê³„ì‚°
                total_pool_utilization = []
                total_cache_hit_rates = []
                
                for pool_metrics in latest_metrics['resource_pools'].values():
                    total_pool_utilization.append(pool_metrics['utilization'])
                
                for cache_metrics in latest_metrics['cache_systems'].values():
                    if cache_metrics['hit_rate'] > 0:
                        total_cache_hit_rates.append(cache_metrics['hit_rate'])
                
                avg_pool_utilization = sum(total_pool_utilization) / len(total_pool_utilization) if total_pool_utilization else 0
                avg_cache_hit_rate = sum(total_cache_hit_rates) / len(total_cache_hit_rates) if total_cache_hit_rates else 0
                
                return {
                    'system_efficiency': {
                        'avg_pool_utilization': avg_pool_utilization,
                        'avg_cache_hit_rate': avg_cache_hit_rate,
                        'system_cpu': latest_metrics['system_cpu'],
                        'system_memory': latest_metrics['system_memory']
                    },
                    'resource_pools_count': len(self.resource_pools),
                    'cache_systems_count': len(self.cache_systems),
                    'optimization_events': len(self.optimization_events),
                    'metrics_collected': len(self.resource_usage_history)
                }
        
        # ë¦¬ì†ŒìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸
        optimizer = ResourceOptimizer()
        
        # ë¦¬ì†ŒìŠ¤ í’€ ìƒì„±
        optimizer.create_resource_pool('db_connections', 'database_connection', initial_size=3, max_size=10)
        optimizer.create_resource_pool('worker_threads', 'worker_thread', initial_size=2, max_size=8)
        optimizer.create_resource_pool('gpu_contexts', 'gpu_context', initial_size=1, max_size=4)
        
        # ìºì‹œ ì‹œìŠ¤í…œ ìƒì„±
        optimizer.add_cache_system('api_cache', max_size=100, ttl=300)
        optimizer.add_cache_system('result_cache', max_size=50, ttl=600)
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
        def simulate_resource_usage():
            """ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜"""
            try:
                # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‚¬ìš©
                db_conn = optimizer.acquire_resource('db_connections')
                time.sleep(0.1)  # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                optimizer.release_resource('db_connections', db_conn)
                
                # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‚¬ìš©
                worker = optimizer.acquire_resource('worker_threads')
                time.sleep(0.05)
                optimizer.release_resource('worker_threads', worker)
                
                # ìºì‹œ ì‚¬ìš©
                cache_key = f"key_{int(time.time() * 1000) % 50}"
                cached_value = optimizer.cache_get('api_cache', cache_key)
                if cached_value is None:
                    # ìºì‹œ ë¯¸ìŠ¤ - ìƒˆ ê°’ ì €ì¥
                    optimizer.cache_set('api_cache', cache_key, f"value_{cache_key}")
                
            except Exception as e:
                print(f"    âš ï¸ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {str(e)}")
        
        # ë™ì‹œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© í…ŒìŠ¤íŠ¸
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¡œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ì‹œë®¬ë ˆì´ì…˜
            futures = [executor.submit(simulate_resource_usage) for _ in range(20)]
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ë°±ê·¸ë¼ìš´ë“œ)
            for i in range(5):
                metrics = optimizer.collect_resource_metrics()
                time.sleep(0.2)
            
            # ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            for future in concurrent.futures.as_completed(futures):
                future.result()
        
        # ìµœì í™” ì‹¤í–‰
        optimizations = optimizer.optimize_resources()
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        final_metrics = optimizer.collect_resource_metrics()
        summary = optimizer.get_optimization_summary()
        
        # ê²°ê³¼ ê²€ì¦
        assert len(optimizer.resource_pools) == 3, "ë¦¬ì†ŒìŠ¤ í’€ ìˆ˜ ë¶ˆì¼ì¹˜"
        assert len(optimizer.cache_systems) == 2, "ìºì‹œ ì‹œìŠ¤í…œ ìˆ˜ ë¶ˆì¼ì¹˜"
        assert len(optimizer.resource_usage_history) > 0, "ë©”íŠ¸ë¦­ì´ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        assert summary['metrics_collected'] > 0, "ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹¤íŒ¨"
        
        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš© ê²€ì¦
        for pool_name, pool_info in optimizer.resource_pools.items():
            assert pool_info['acquired_count'] > 0, f"{pool_name} í’€ì´ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            assert pool_info['released_count'] > 0, f"{pool_name} ë¦¬ì†ŒìŠ¤ê°€ ë°˜í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ìºì‹œ ì‚¬ìš© ê²€ì¦
        for cache_name, cache_info in optimizer.cache_systems.items():
            total_requests = cache_info['hit_count'] + cache_info['miss_count']
            assert total_requests > 0, f"{cache_name} ìºì‹œê°€ ì‚¬ìš©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        print(f"  ğŸ”§ ìµœì í™” ê²°ê³¼: {len(optimizations)}ê°œ ê¶Œì¥ì‚¬í•­")
        print(f"  ğŸ“Š ì‹œìŠ¤í…œ íš¨ìœ¨ì„±: í’€ í™œìš©ë¥  {summary['system_efficiency']['avg_pool_utilization']:.1f}%, "
              f"ìºì‹œ ì ì¤‘ë¥  {summary['system_efficiency']['avg_cache_hit_rate']:.1f}%")
        
        print("âœ… ë¦¬ì†ŒìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ë¦¬ì†ŒìŠ¤ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """S03_M03_T02 í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ S03_M03_T02 í™•ì¥ì„± ë° ìŠ¤ì¼€ì¼ë§ ì „ëµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    test_results = []
    start_time = time.time()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("ë¶€í•˜ ë¶„ì‚° ì‹œìŠ¤í…œ", test_load_balancing),
        ("ìë™ ìŠ¤ì¼€ì¼ë§", test_auto_scaling),
        ("ë¦¬ì†ŒìŠ¤ ìµœì í™”", test_resource_optimization)
    ]
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        try:
            result = test_func()
            test_results.append((test_name, result))
            if result:
                print(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            test_results.append((test_name, False))
    
    # ê²°ê³¼ ìš”ì•½
    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ¯ S03_M03_T02 í™•ì¥ì„± ë° ìŠ¤ì¼€ì¼ë§ ì „ëµ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        print(f"  {status}: {test_name}")
    
    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: {passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼ ({passed_tests/total_tests*100:.1f}%)")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! S03_M03_T02 ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)