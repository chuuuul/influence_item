#!/usr/bin/env python3
"""
S03_M03_T01 í†µí•© í…ŒìŠ¤íŠ¸: ìš´ì˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§, ì•Œë¦¼ ì‹œìŠ¤í…œ, ë¡œê·¸ ë¶„ì„ ê¸°ëŠ¥ ê²€ì¦
"""

import sys
import os
import time
import json
import sqlite3
import threading
from typing import Dict, List, Any
from datetime import datetime, timedelta
import queue

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append('/Users/chul/Documents/claude/influence_item')

def test_real_time_metrics():
    """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        import psutil
        from collections import deque
        import threading
        import time
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° í´ë˜ìŠ¤
        class RealTimeMetricsCollector:
            def __init__(self, buffer_size=100):
                self.buffer_size = buffer_size
                self.metrics_buffer = {
                    'cpu_percent': deque(maxlen=buffer_size),
                    'memory_percent': deque(maxlen=buffer_size),
                    'disk_io': deque(maxlen=buffer_size),
                    'network_io': deque(maxlen=buffer_size),
                    'process_count': deque(maxlen=buffer_size),
                    'timestamp': deque(maxlen=buffer_size)
                }
                self.collection_interval = 1.0  # 1ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
                self.collecting = False
                self.collector_thread = None
                self.callbacks = []
            
            def start_collection(self):
                """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘"""
                if self.collecting:
                    return
                
                self.collecting = True
                self.collector_thread = threading.Thread(target=self._collection_loop)
                self.collector_thread.daemon = True
                self.collector_thread.start()
                print("  ğŸ“Š ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘")
            
            def stop_collection(self):
                """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€"""
                self.collecting = False
                if self.collector_thread:
                    self.collector_thread.join(timeout=2.0)
                print("  â¹ï¸ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì¤‘ì§€")
            
            def _collection_loop(self):
                """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë£¨í”„"""
                last_disk_io = psutil.disk_io_counters()
                last_network_io = psutil.net_io_counters()
                
                while self.collecting:
                    try:
                        timestamp = datetime.now()
                        
                        # CPU ì‚¬ìš©ë¥ 
                        cpu_percent = psutil.cpu_percent(interval=None)
                        
                        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
                        memory = psutil.virtual_memory()
                        memory_percent = memory.percent
                        
                        # ë””ìŠ¤í¬ I/O (ì´ˆë‹¹ ì½ê¸°/ì“°ê¸° ë°”ì´íŠ¸)
                        current_disk_io = psutil.disk_io_counters()
                        disk_io_rate = 0
                        if last_disk_io:
                            read_diff = current_disk_io.read_bytes - last_disk_io.read_bytes
                            write_diff = current_disk_io.write_bytes - last_disk_io.write_bytes
                            disk_io_rate = (read_diff + write_diff) / self.collection_interval
                        last_disk_io = current_disk_io
                        
                        # ë„¤íŠ¸ì›Œí¬ I/O (ì´ˆë‹¹ ì†¡ìˆ˜ì‹  ë°”ì´íŠ¸)
                        current_network_io = psutil.net_io_counters()
                        network_io_rate = 0
                        if last_network_io:
                            sent_diff = current_network_io.bytes_sent - last_network_io.bytes_sent
                            recv_diff = current_network_io.bytes_recv - last_network_io.bytes_recv
                            network_io_rate = (sent_diff + recv_diff) / self.collection_interval
                        last_network_io = current_network_io
                        
                        # í”„ë¡œì„¸ìŠ¤ ìˆ˜
                        process_count = len(psutil.pids())
                        
                        # ë©”íŠ¸ë¦­ ì €ì¥
                        self.metrics_buffer['cpu_percent'].append(cpu_percent)
                        self.metrics_buffer['memory_percent'].append(memory_percent)
                        self.metrics_buffer['disk_io'].append(disk_io_rate)
                        self.metrics_buffer['network_io'].append(network_io_rate)
                        self.metrics_buffer['process_count'].append(process_count)
                        self.metrics_buffer['timestamp'].append(timestamp)
                        
                        # ì½œë°± ì‹¤í–‰
                        metrics_data = {
                            'timestamp': timestamp,
                            'cpu_percent': cpu_percent,
                            'memory_percent': memory_percent,
                            'disk_io': disk_io_rate,
                            'network_io': network_io_rate,
                            'process_count': process_count
                        }
                        
                        for callback in self.callbacks:
                            try:
                                callback(metrics_data)
                            except Exception as e:
                                print(f"    âš ï¸ ë©”íŠ¸ë¦­ ì½œë°± ì˜¤ë¥˜: {str(e)}")
                        
                        time.sleep(self.collection_interval)
                        
                    except Exception as e:
                        print(f"    âš ï¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
                        time.sleep(self.collection_interval)
            
            def add_callback(self, callback):
                """ë©”íŠ¸ë¦­ ì½œë°± ì¶”ê°€"""
                self.callbacks.append(callback)
            
            def get_latest_metrics(self):
                """ìµœì‹  ë©”íŠ¸ë¦­ ë°˜í™˜"""
                if not self.metrics_buffer['timestamp']:
                    return None
                
                return {
                    'timestamp': self.metrics_buffer['timestamp'][-1],
                    'cpu_percent': self.metrics_buffer['cpu_percent'][-1],
                    'memory_percent': self.metrics_buffer['memory_percent'][-1],
                    'disk_io': self.metrics_buffer['disk_io'][-1],
                    'network_io': self.metrics_buffer['network_io'][-1],
                    'process_count': self.metrics_buffer['process_count'][-1]
                }
            
            def get_metrics_history(self, duration_minutes=5):
                """ì§€ì •ëœ ê¸°ê°„ì˜ ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ë°˜í™˜"""
                if not self.metrics_buffer['timestamp']:
                    return []
                
                cutoff_time = datetime.now() - timedelta(minutes=duration_minutes)
                history = []
                
                timestamps = list(self.metrics_buffer['timestamp'])
                for i, timestamp in enumerate(timestamps):
                    if timestamp > cutoff_time:
                        history.append({
                            'timestamp': timestamp,
                            'cpu_percent': list(self.metrics_buffer['cpu_percent'])[i],
                            'memory_percent': list(self.metrics_buffer['memory_percent'])[i],
                            'disk_io': list(self.metrics_buffer['disk_io'])[i],
                            'network_io': list(self.metrics_buffer['network_io'])[i],
                            'process_count': list(self.metrics_buffer['process_count'])[i]
                        })
                
                return history
            
            def get_aggregated_stats(self, duration_minutes=5):
                """ì§‘ê³„ í†µê³„ ë°˜í™˜"""
                history = self.get_metrics_history(duration_minutes)
                
                if not history:
                    return None
                
                cpu_values = [item['cpu_percent'] for item in history]
                memory_values = [item['memory_percent'] for item in history]
                
                return {
                    'duration_minutes': duration_minutes,
                    'sample_count': len(history),
                    'cpu_avg': sum(cpu_values) / len(cpu_values),
                    'cpu_max': max(cpu_values),
                    'cpu_min': min(cpu_values),
                    'memory_avg': sum(memory_values) / len(memory_values),
                    'memory_max': max(memory_values),
                    'memory_min': min(memory_values)
                }
        
        # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸
        collector = RealTimeMetricsCollector(buffer_size=10)
        
        # ì½œë°± í…ŒìŠ¤íŠ¸
        callback_called = False
        def test_callback(metrics_data):
            nonlocal callback_called
            callback_called = True
            print(f"    ğŸ“Š ë©”íŠ¸ë¦­ ì½œë°±: CPU {metrics_data['cpu_percent']:.1f}%, "
                  f"ë©”ëª¨ë¦¬ {metrics_data['memory_percent']:.1f}%")
        
        collector.add_callback(test_callback)
        
        # ìˆ˜ì§‘ ì‹œì‘
        collector.start_collection()
        
        # ì ì‹œ ëŒ€ê¸°í•˜ì—¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        time.sleep(3.0)
        
        # ìµœì‹  ë©”íŠ¸ë¦­ í™•ì¸
        latest_metrics = collector.get_latest_metrics()
        assert latest_metrics is not None, "ìµœì‹  ë©”íŠ¸ë¦­ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        assert 'cpu_percent' in latest_metrics, "CPU ë©”íŠ¸ë¦­ ëˆ„ë½"
        assert 'memory_percent' in latest_metrics, "ë©”ëª¨ë¦¬ ë©”íŠ¸ë¦­ ëˆ„ë½"
        
        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ í™•ì¸
        history = collector.get_metrics_history(duration_minutes=1)
        assert len(history) > 0, "ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ì§‘ê³„ í†µê³„ í™•ì¸
        stats = collector.get_aggregated_stats(duration_minutes=1)
        assert stats is not None, "ì§‘ê³„ í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        assert stats['sample_count'] > 0, "ìƒ˜í”Œ ìˆ˜ê°€ 0ì…ë‹ˆë‹¤."
        
        # ì½œë°± í˜¸ì¶œ í™•ì¸
        assert callback_called == True, "ë©”íŠ¸ë¦­ ì½œë°±ì´ í˜¸ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ìˆ˜ì§‘ ì¤‘ì§€
        collector.stop_collection()
        
        print(f"  ğŸ“ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì™„ë£Œ: {stats['sample_count']}ê°œ ìƒ˜í”Œ")
        print(f"  ğŸ’» í‰ê·  CPU: {stats['cpu_avg']:.1f}%, í‰ê·  ë©”ëª¨ë¦¬: {stats['memory_avg']:.1f}%")
        
        print("âœ… ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_alert_system():
    """ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from enum import Enum
        from datetime import datetime, timedelta
        
        # ì•Œë¦¼ ì‹¬ê°ë„ ì •ì˜
        class AlertSeverity(Enum):
            INFO = "info"
            WARNING = "warning"
            ERROR = "error"
            CRITICAL = "critical"
        
        # ì•Œë¦¼ ìƒíƒœ ì •ì˜
        class AlertStatus(Enum):
            ACTIVE = "active"
            ACKNOWLEDGED = "acknowledged"
            RESOLVED = "resolved"
            SUPPRESSED = "suppressed"
        
        # ì•Œë¦¼ í´ë˜ìŠ¤
        class Alert:
            def __init__(self, alert_id, title, message, severity=AlertSeverity.INFO, source="system"):
                self.alert_id = alert_id
                self.title = title
                self.message = message
                self.severity = severity
                self.source = source
                self.status = AlertStatus.ACTIVE
                self.created_at = datetime.now()
                self.updated_at = self.created_at
                self.acknowledged_at = None
                self.resolved_at = None
                self.acknowledged_by = None
                self.tags = []
                self.metadata = {}
            
            def acknowledge(self, user="system"):
                """ì•Œë¦¼ í™•ì¸"""
                self.status = AlertStatus.ACKNOWLEDGED
                self.acknowledged_at = datetime.now()
                self.updated_at = self.acknowledged_at
                self.acknowledged_by = user
            
            def resolve(self):
                """ì•Œë¦¼ í•´ê²°"""
                self.status = AlertStatus.RESOLVED
                self.resolved_at = datetime.now()
                self.updated_at = self.resolved_at
            
            def suppress(self):
                """ì•Œë¦¼ ì–µì œ"""
                self.status = AlertStatus.SUPPRESSED
                self.updated_at = datetime.now()
            
            def add_tag(self, tag):
                """íƒœê·¸ ì¶”ê°€"""
                if tag not in self.tags:
                    self.tags.append(tag)
            
            def to_dict(self):
                """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
                return {
                    'alert_id': self.alert_id,
                    'title': self.title,
                    'message': self.message,
                    'severity': self.severity.value,
                    'source': self.source,
                    'status': self.status.value,
                    'created_at': self.created_at.isoformat(),
                    'updated_at': self.updated_at.isoformat(),
                    'acknowledged_at': self.acknowledged_at.isoformat() if self.acknowledged_at else None,
                    'resolved_at': self.resolved_at.isoformat() if self.resolved_at else None,
                    'acknowledged_by': self.acknowledged_by,
                    'tags': self.tags,
                    'metadata': self.metadata
                }
        
        # ì•Œë¦¼ ê´€ë¦¬ì í´ë˜ìŠ¤
        class AlertManager:
            def __init__(self):
                self.alerts = {}
                self.alert_rules = {}
                self.notification_channels = {}
                self.suppression_rules = []
                self.alert_history = []
                self.stats = {
                    'total_alerts': 0,
                    'active_alerts': 0,
                    'critical_alerts': 0,
                    'acknowledged_alerts': 0,
                    'resolved_alerts': 0
                }
            
            def create_alert(self, title, message, severity=AlertSeverity.INFO, source="system", metadata=None):
                """ì•Œë¦¼ ìƒì„±"""
                alert_id = f"alert_{int(time.time() * 1000)}"
                alert = Alert(alert_id, title, message, severity, source)
                
                if metadata:
                    alert.metadata = metadata
                
                # ì–µì œ ê·œì¹™ í™•ì¸
                if self._is_suppressed(alert):
                    alert.suppress()
                
                self.alerts[alert_id] = alert
                self.alert_history.append(alert.to_dict())
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self._update_stats()
                
                # ì•Œë¦¼ ì „ì†¡
                self._send_notifications(alert)
                
                print(f"    ğŸš¨ ì•Œë¦¼ ìƒì„±: {alert.title} ({alert.severity.value})")
                
                return alert_id
            
            def acknowledge_alert(self, alert_id, user="system"):
                """ì•Œë¦¼ í™•ì¸"""
                if alert_id in self.alerts:
                    self.alerts[alert_id].acknowledge(user)
                    self._update_stats()
                    print(f"    âœ… ì•Œë¦¼ í™•ì¸: {alert_id}")
                    return True
                return False
            
            def resolve_alert(self, alert_id):
                """ì•Œë¦¼ í•´ê²°"""
                if alert_id in self.alerts:
                    self.alerts[alert_id].resolve()
                    self._update_stats()
                    print(f"    âœ… ì•Œë¦¼ í•´ê²°: {alert_id}")
                    return True
                return False
            
            def add_suppression_rule(self, rule):
                """ì–µì œ ê·œì¹™ ì¶”ê°€"""
                self.suppression_rules.append(rule)
            
            def _is_suppressed(self, alert):
                """ì•Œë¦¼ ì–µì œ ì—¬ë¶€ í™•ì¸"""
                for rule in self.suppression_rules:
                    if self._match_suppression_rule(alert, rule):
                        return True
                return False
            
            def _match_suppression_rule(self, alert, rule):
                """ì–µì œ ê·œì¹™ ë§¤ì¹­"""
                if 'severity' in rule and alert.severity != rule['severity']:
                    return False
                if 'source' in rule and alert.source != rule['source']:
                    return False
                if 'title_pattern' in rule and rule['title_pattern'] not in alert.title:
                    return False
                return True
            
            def _send_notifications(self, alert):
                """ì•Œë¦¼ ì „ì†¡"""
                for channel_name, channel_config in self.notification_channels.items():
                    try:
                        # ì‹¬ê°ë„ë³„ ì±„ë„ í•„í„°ë§
                        if 'min_severity' in channel_config:
                            min_severity = channel_config['min_severity']
                            if alert.severity.value < min_severity.value:
                                continue
                        
                        # ì‹¤ì œë¡œëŠ” ê° ì±„ë„ë¡œ ì•Œë¦¼ ì „ì†¡
                        print(f"      ğŸ“¤ {channel_name}ë¡œ ì•Œë¦¼ ì „ì†¡: {alert.title}")
                        
                    except Exception as e:
                        print(f"      âš ï¸ {channel_name} ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {str(e)}")
            
            def _update_stats(self):
                """í†µê³„ ì—…ë°ì´íŠ¸"""
                self.stats['total_alerts'] = len(self.alerts)
                self.stats['active_alerts'] = len([a for a in self.alerts.values() if a.status == AlertStatus.ACTIVE])
                self.stats['critical_alerts'] = len([a for a in self.alerts.values() if a.severity == AlertSeverity.CRITICAL and a.status == AlertStatus.ACTIVE])
                self.stats['acknowledged_alerts'] = len([a for a in self.alerts.values() if a.status == AlertStatus.ACKNOWLEDGED])
                self.stats['resolved_alerts'] = len([a for a in self.alerts.values() if a.status == AlertStatus.RESOLVED])
            
            def get_active_alerts(self):
                """í™œì„± ì•Œë¦¼ ì¡°íšŒ"""
                return [alert.to_dict() for alert in self.alerts.values() if alert.status == AlertStatus.ACTIVE]
            
            def get_alerts_by_severity(self, severity):
                """ì‹¬ê°ë„ë³„ ì•Œë¦¼ ì¡°íšŒ"""
                return [alert.to_dict() for alert in self.alerts.values() if alert.severity == severity]
            
            def get_alert_stats(self):
                """ì•Œë¦¼ í†µê³„ ë°˜í™˜"""
                return self.stats.copy()
            
            def add_notification_channel(self, name, config):
                """ì•Œë¦¼ ì±„ë„ ì¶”ê°€"""
                self.notification_channels[name] = config
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
        alert_manager = AlertManager()
        
        # ì•Œë¦¼ ì±„ë„ ì„¤ì •
        alert_manager.add_notification_channel('slack', {
            'type': 'slack',
            'webhook_url': 'https://hooks.slack.com/test',
            'min_severity': AlertSeverity.WARNING
        })
        
        alert_manager.add_notification_channel('email', {
            'type': 'email',
            'recipients': ['admin@example.com'],
            'min_severity': AlertSeverity.ERROR
        })
        
        # ì–µì œ ê·œì¹™ ì¶”ê°€
        alert_manager.add_suppression_rule({
            'title_pattern': 'test_suppressed',
            'severity': AlertSeverity.INFO
        })
        
        # ë‹¤ì–‘í•œ ì•Œë¦¼ ìƒì„±
        test_alerts = [
            ("ë†’ì€ CPU ì‚¬ìš©ë¥ ", "CPU ì‚¬ìš©ë¥ ì´ 85%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.", AlertSeverity.WARNING),
            ("ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±", "ë””ìŠ¤í¬ ì‚¬ìš©ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.", AlertSeverity.CRITICAL),
            ("API ì‘ë‹µ ì§€ì—°", "Gemini API ì‘ë‹µ ì‹œê°„ì´ 5ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.", AlertSeverity.ERROR),
            ("ì •ë³´ ì•Œë¦¼", "ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", AlertSeverity.INFO),
            ("test_suppressed", "ì´ ì•Œë¦¼ì€ ì–µì œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.", AlertSeverity.INFO)
        ]
        
        alert_ids = []
        for title, message, severity in test_alerts:
            alert_id = alert_manager.create_alert(title, message, severity)
            alert_ids.append(alert_id)
        
        # ì•Œë¦¼ ìƒíƒœ í™•ì¸
        stats = alert_manager.get_alert_stats()
        assert stats['total_alerts'] == len(test_alerts), f"ì´ ì•Œë¦¼ ìˆ˜ ë¶ˆì¼ì¹˜: {stats['total_alerts']}"
        assert stats['critical_alerts'] >= 1, "í¬ë¦¬í‹°ì»¬ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # í™œì„± ì•Œë¦¼ í™•ì¸
        active_alerts = alert_manager.get_active_alerts()
        assert len(active_alerts) > 0, "í™œì„± ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ì‹¬ê°ë„ë³„ ì•Œë¦¼ í™•ì¸
        critical_alerts = alert_manager.get_alerts_by_severity(AlertSeverity.CRITICAL)
        assert len(critical_alerts) >= 1, "í¬ë¦¬í‹°ì»¬ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ì•Œë¦¼ í™•ì¸ ë° í•´ê²°
        if alert_ids:
            # ì²« ë²ˆì§¸ ì•Œë¦¼ í™•ì¸
            alert_manager.acknowledge_alert(alert_ids[0], "test_user")
            
            # ë‘ ë²ˆì§¸ ì•Œë¦¼ í•´ê²°
            if len(alert_ids) > 1:
                alert_manager.resolve_alert(alert_ids[1])
        
        # ì–µì œëœ ì•Œë¦¼ í™•ì¸
        suppressed_alerts = [a for a in alert_manager.alerts.values() if a.status == AlertStatus.SUPPRESSED]
        assert len(suppressed_alerts) >= 1, "ì–µì œëœ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ìµœì¢… í†µê³„ í™•ì¸
        final_stats = alert_manager.get_alert_stats()
        assert final_stats['acknowledged_alerts'] >= 1, "í™•ì¸ëœ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        assert final_stats['resolved_alerts'] >= 1, "í•´ê²°ëœ ì•Œë¦¼ì´ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"  ğŸ“Š ì•Œë¦¼ í†µê³„: ì´ {final_stats['total_alerts']}ê°œ, "
              f"í™œì„± {final_stats['active_alerts']}ê°œ, "
              f"í¬ë¦¬í‹°ì»¬ {final_stats['critical_alerts']}ê°œ")
        
        print("âœ… ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_log_analysis():
    """ë¡œê·¸ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë¡œê·¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        import re
        import gzip
        from collections import defaultdict, Counter
        
        # ë¡œê·¸ ì—”íŠ¸ë¦¬ í´ë˜ìŠ¤
        class LogEntry:
            def __init__(self, timestamp, level, message, source=None, metadata=None):
                self.timestamp = timestamp
                self.level = level
                self.message = message
                self.source = source or "system"
                self.metadata = metadata or {}
            
            def to_dict(self):
                return {
                    'timestamp': self.timestamp.isoformat(),
                    'level': self.level,
                    'message': self.message,
                    'source': self.source,
                    'metadata': self.metadata
                }
        
        # ë¡œê·¸ ë¶„ì„ê¸° í´ë˜ìŠ¤
        class LogAnalyzer:
            def __init__(self):
                self.log_patterns = {
                    'error_patterns': [
                        r'ERROR.*',
                        r'CRITICAL.*',
                        r'Exception.*',
                        r'Failed.*',
                        r'Timeout.*'
                    ],
                    'warning_patterns': [
                        r'WARNING.*',
                        r'WARN.*',
                        r'Deprecated.*',
                        r'Slow.*'
                    ],
                    'api_patterns': [
                        r'API.*request.*',
                        r'HTTP.*\d{3}.*',
                        r'Response time.*'
                    ],
                    'performance_patterns': [
                        r'Processing time.*',
                        r'Memory usage.*',
                        r'CPU.*percent.*'
                    ]
                }
                
                self.processed_logs = []
                self.analysis_cache = {}
            
            def parse_log_line(self, log_line):
                """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
                # ì¼ë°˜ì ì¸ ë¡œê·¸ í˜•ì‹: 2025-06-23 10:30:45 [ERROR] message
                pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s*\[(\w+)\]\s*(.*)'
                match = re.match(pattern, log_line.strip())
                
                if match:
                    timestamp_str, level, message = match.groups()
                    try:
                        timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                        return LogEntry(timestamp, level.upper(), message)
                    except ValueError:
                        pass
                
                # íŒ¨í„´ì´ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬
                return LogEntry(datetime.now(), 'INFO', log_line.strip())
            
            def analyze_logs(self, log_entries):
                """ë¡œê·¸ ë¶„ì„"""
                analysis = {
                    'total_entries': len(log_entries),
                    'level_distribution': Counter(),
                    'source_distribution': Counter(),
                    'error_analysis': defaultdict(list),
                    'performance_metrics': [],
                    'api_requests': [],
                    'time_analysis': {
                        'start_time': None,
                        'end_time': None,
                        'duration_hours': 0
                    },
                    'patterns_found': defaultdict(int),
                    'anomalies': []
                }
                
                if not log_entries:
                    return analysis
                
                # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
                timestamps = [entry.timestamp for entry in log_entries]
                analysis['time_analysis']['start_time'] = min(timestamps)
                analysis['time_analysis']['end_time'] = max(timestamps)
                duration = max(timestamps) - min(timestamps)
                analysis['time_analysis']['duration_hours'] = duration.total_seconds() / 3600
                
                # ë¡œê·¸ ì—”íŠ¸ë¦¬ë³„ ë¶„ì„
                for entry in log_entries:
                    # ë ˆë²¨ë³„ ë¶„í¬
                    analysis['level_distribution'][entry.level] += 1
                    
                    # ì†ŒìŠ¤ë³„ ë¶„í¬
                    analysis['source_distribution'][entry.source] += 1
                    
                    # íŒ¨í„´ ë§¤ì¹­
                    self._analyze_patterns(entry, analysis)
                    
                    # ì—ëŸ¬ ë¶„ì„
                    if entry.level in ['ERROR', 'CRITICAL']:
                        analysis['error_analysis'][entry.level].append({
                            'timestamp': entry.timestamp.isoformat(),
                            'message': entry.message,
                            'source': entry.source
                        })
                    
                    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                    self._extract_performance_metrics(entry, analysis)
                    
                    # API ìš”ì²­ ë¶„ì„
                    self._analyze_api_requests(entry, analysis)
                
                # ì´ìƒ íƒì§€
                analysis['anomalies'] = self._detect_anomalies(log_entries, analysis)
                
                return analysis
            
            def _analyze_patterns(self, entry, analysis):
                """íŒ¨í„´ ë¶„ì„"""
                for pattern_type, patterns in self.log_patterns.items():
                    for pattern in patterns:
                        if re.search(pattern, entry.message, re.IGNORECASE):
                            analysis['patterns_found'][pattern_type] += 1
            
            def _extract_performance_metrics(self, entry, analysis):
                """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
                # ì²˜ë¦¬ ì‹œê°„ ì¶”ì¶œ
                time_pattern = r'Processing time[:\s]+(\d+\.?\d*)\s*(seconds?|ms|milliseconds?)'
                match = re.search(time_pattern, entry.message, re.IGNORECASE)
                if match:
                    time_value = float(match.group(1))
                    time_unit = match.group(2).lower()
                    
                    # ì´ˆ ë‹¨ìœ„ë¡œ ì •ê·œí™”
                    if 'ms' in time_unit or 'millisecond' in time_unit:
                        time_value /= 1000
                    
                    analysis['performance_metrics'].append({
                        'timestamp': entry.timestamp.isoformat(),
                        'metric_type': 'processing_time',
                        'value': time_value,
                        'unit': 'seconds'
                    })
                
                # CPU ì‚¬ìš©ë¥  ì¶”ì¶œ
                cpu_pattern = r'CPU[:\s]+(\d+\.?\d*)%'
                match = re.search(cpu_pattern, entry.message, re.IGNORECASE)
                if match:
                    cpu_value = float(match.group(1))
                    analysis['performance_metrics'].append({
                        'timestamp': entry.timestamp.isoformat(),
                        'metric_type': 'cpu_percent',
                        'value': cpu_value,
                        'unit': 'percent'
                    })
            
            def _analyze_api_requests(self, entry, analysis):
                """API ìš”ì²­ ë¶„ì„"""
                # HTTP ì‘ë‹µ ì½”ë“œ ì¶”ì¶œ
                http_pattern = r'HTTP.*(\d{3}).*'
                match = re.search(http_pattern, entry.message)
                if match:
                    status_code = int(match.group(1))
                    analysis['api_requests'].append({
                        'timestamp': entry.timestamp.isoformat(),
                        'status_code': status_code,
                        'success': 200 <= status_code < 400
                    })
            
            def _detect_anomalies(self, log_entries, analysis):
                """ì´ìƒ íƒì§€"""
                anomalies = []
                
                # ì—ëŸ¬ìœ¨ ì´ìƒ íƒì§€
                total_entries = len(log_entries)
                error_entries = analysis['level_distribution'].get('ERROR', 0) + analysis['level_distribution'].get('CRITICAL', 0)
                error_rate = (error_entries / total_entries * 100) if total_entries > 0 else 0
                
                if error_rate > 10:  # 10% ì´ìƒ ì—ëŸ¬ìœ¨
                    anomalies.append({
                        'type': 'high_error_rate',
                        'description': f'ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€: {error_rate:.1f}%',
                        'severity': 'warning' if error_rate < 20 else 'critical',
                        'value': error_rate
                    })
                
                # ì„±ëŠ¥ ì´ìƒ íƒì§€
                performance_metrics = analysis['performance_metrics']
                processing_times = [m['value'] for m in performance_metrics if m['metric_type'] == 'processing_time']
                
                if processing_times:
                    avg_time = sum(processing_times) / len(processing_times)
                    max_time = max(processing_times)
                    
                    if max_time > avg_time * 3:  # í‰ê· ì˜ 3ë°° ì´ìƒ
                        anomalies.append({
                            'type': 'slow_processing',
                            'description': f'ë¹„ì •ìƒì ìœ¼ë¡œ ëŠë¦° ì²˜ë¦¬ ì‹œê°„: {max_time:.2f}ì´ˆ (í‰ê· : {avg_time:.2f}ì´ˆ)',
                            'severity': 'warning',
                            'value': max_time
                        })
                
                return anomalies
            
            def generate_summary_report(self, analysis):
                """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
                report = {
                    'analysis_timestamp': datetime.now().isoformat(),
                    'summary': {
                        'total_log_entries': analysis['total_entries'],
                        'time_range': {
                            'start': analysis['time_analysis']['start_time'].isoformat() if analysis['time_analysis']['start_time'] else None,
                            'end': analysis['time_analysis']['end_time'].isoformat() if analysis['time_analysis']['end_time'] else None,
                            'duration_hours': analysis['time_analysis']['duration_hours']
                        },
                        'error_count': analysis['level_distribution'].get('ERROR', 0) + analysis['level_distribution'].get('CRITICAL', 0),
                        'warning_count': analysis['level_distribution'].get('WARNING', 0),
                        'anomaly_count': len(analysis['anomalies'])
                    },
                    'key_findings': [],
                    'recommendations': []
                }
                
                # ì£¼ìš” ë°œê²¬ì‚¬í•­
                if analysis['anomalies']:
                    for anomaly in analysis['anomalies']:
                        report['key_findings'].append(f"{anomaly['type']}: {anomaly['description']}")
                
                # ê¶Œì¥ì‚¬í•­
                if report['summary']['error_count'] > 0:
                    report['recommendations'].append("ì˜¤ë¥˜ ë¡œê·¸ë¥¼ ê²€í† í•˜ì—¬ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.")
                
                if any(a['type'] == 'high_error_rate' for a in analysis['anomalies']):
                    report['recommendations'].append("ë†’ì€ ì˜¤ë¥˜ìœ¨ë¡œ ì¸í•´ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ì ê²€í•˜ì„¸ìš”.")
                
                if any(a['type'] == 'slow_processing' for a in analysis['anomalies']):
                    report['recommendations'].append("ì„±ëŠ¥ ìµœì í™”ë¥¼ ê²€í† í•˜ì„¸ìš”.")
                
                return report
        
        # ë¡œê·¸ ë¶„ì„ í…ŒìŠ¤íŠ¸
        analyzer = LogAnalyzer()
        
        # í…ŒìŠ¤íŠ¸ ë¡œê·¸ ë°ì´í„° ìƒì„±
        test_logs = [
            "2025-06-23 10:30:45 [INFO] ì‹œìŠ¤í…œ ì‹œì‘ë¨",
            "2025-06-23 10:31:00 [INFO] API ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ",
            "2025-06-23 10:31:15 [ERROR] Gemini API ì—°ê²° ì‹¤íŒ¨: timeout",
            "2025-06-23 10:31:30 [WARNING] ë†’ì€ CPU ì‚¬ìš©ë¥ : CPU 85%",
            "2025-06-23 10:31:45 [INFO] ì˜ìƒ ë¶„ì„ ì‹œì‘: video_123",
            "2025-06-23 10:32:00 [INFO] Processing time: 2.5 seconds",
            "2025-06-23 10:32:15 [ERROR] ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ ë°œìƒ",
            "2025-06-23 10:32:30 [INFO] HTTP 200 ì‘ë‹µ ìˆ˜ì‹ ",
            "2025-06-23 10:32:45 [WARNING] API ì‘ë‹µ ì§€ì—°: Response time 4.2 seconds",
            "2025-06-23 10:33:00 [CRITICAL] ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±",
            "2025-06-23 10:33:15 [INFO] Processing time: 15.8 seconds"  # ë¹„ì •ìƒì ìœ¼ë¡œ ê¸´ ì‹œê°„
        ]
        
        # ë¡œê·¸ íŒŒì‹±
        log_entries = []
        for log_line in test_logs:
            entry = analyzer.parse_log_line(log_line)
            log_entries.append(entry)
        
        # ë¡œê·¸ ë¶„ì„ ì‹¤í–‰
        analysis = analyzer.analyze_logs(log_entries)
        
        # ë¶„ì„ ê²°ê³¼ ê²€ì¦
        assert analysis['total_entries'] == len(test_logs), "ì´ ë¡œê·¸ ì—”íŠ¸ë¦¬ ìˆ˜ ë¶ˆì¼ì¹˜"
        assert analysis['level_distribution']['ERROR'] >= 1, "ì—ëŸ¬ ë¡œê·¸ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ"
        assert analysis['level_distribution']['CRITICAL'] >= 1, "í¬ë¦¬í‹°ì»¬ ë¡œê·¸ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ"
        assert len(analysis['error_analysis']['ERROR']) >= 1, "ì—ëŸ¬ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
        assert len(analysis['performance_metrics']) >= 1, "ì„±ëŠ¥ ë©”íŠ¸ë¦­ì´ ì¶”ì¶œë˜ì§€ ì•ŠìŒ"
        assert len(analysis['anomalies']) >= 1, "ì´ìƒ í˜„ìƒì´ ê°ì§€ë˜ì§€ ì•ŠìŒ"
        
        # ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        report = analyzer.generate_summary_report(analysis)
        
        assert 'summary' in report, "ìš”ì•½ ì •ë³´ ëˆ„ë½"
        assert 'key_findings' in report, "ì£¼ìš” ë°œê²¬ì‚¬í•­ ëˆ„ë½"
        assert 'recommendations' in report, "ê¶Œì¥ì‚¬í•­ ëˆ„ë½"
        assert report['summary']['error_count'] >= 2, "ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¶ˆì¼ì¹˜"
        
        print(f"  ğŸ“Š ë¡œê·¸ ë¶„ì„ ì™„ë£Œ: {analysis['total_entries']}ê°œ ì—”íŠ¸ë¦¬")
        print(f"  ğŸš¨ ì—ëŸ¬: {report['summary']['error_count']}ê°œ, "
              f"ê²½ê³ : {report['summary']['warning_count']}ê°œ, "
              f"ì´ìƒ: {report['summary']['anomaly_count']}ê°œ")
        
        print("âœ… ë¡œê·¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ë¡œê·¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_dashboard_integration():
    """ëŒ€ì‹œë³´ë“œ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ëŒ€ì‹œë³´ë“œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        import json
        import sqlite3
        from datetime import datetime, timedelta
        
        # ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í´ë˜ìŠ¤
        class MonitoringDashboard:
            def __init__(self, db_path=None):
                self.db_path = db_path or '/tmp/monitoring_test.db'
                self.widgets = {}
                self.data_sources = {}
                self.refresh_intervals = {}
                self.init_database()
            
            def init_database(self):
                """ëª¨ë‹ˆí„°ë§ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                # ë©”íŠ¸ë¦­ í…Œì´ë¸” ìƒì„±
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    metric_name TEXT NOT NULL,
                    metric_value REAL NOT NULL,
                    metric_unit TEXT,
                    source TEXT DEFAULT 'system',
                    metadata TEXT
                )
                ''')
                
                # ì•Œë¦¼ í…Œì´ë¸” ìƒì„±
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS alerts (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    alert_id TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    message TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    status TEXT DEFAULT 'active',
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    source TEXT DEFAULT 'system'
                )
                ''')
                
                conn.commit()
                conn.close()
            
            def add_widget(self, widget_id, widget_config):
                """ìœ„ì ¯ ì¶”ê°€"""
                self.widgets[widget_id] = widget_config
                self.refresh_intervals[widget_id] = widget_config.get('refresh_interval', 60)  # ê¸°ë³¸ 60ì´ˆ
            
            def add_data_source(self, source_id, source_config):
                """ë°ì´í„° ì†ŒìŠ¤ ì¶”ê°€"""
                self.data_sources[source_id] = source_config
            
            def store_metric(self, metric_name, value, unit=None, source='system', metadata=None):
                """ë©”íŠ¸ë¦­ ì €ì¥"""
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                metadata_json = json.dumps(metadata) if metadata else None
                
                cursor.execute('''
                INSERT INTO metrics (metric_name, metric_value, metric_unit, source, metadata)
                VALUES (?, ?, ?, ?, ?)
                ''', (metric_name, value, unit, source, metadata_json))
                
                conn.commit()
                conn.close()
            
            def get_metrics(self, metric_name, hours_back=1, source=None):
                """ë©”íŠ¸ë¦­ ì¡°íšŒ"""
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                cutoff_time = datetime.now() - timedelta(hours=hours_back)
                
                query = '''
                SELECT timestamp, metric_value, metric_unit, source, metadata
                FROM metrics
                WHERE metric_name = ? AND timestamp > ?
                '''
                params = [metric_name, cutoff_time.isoformat()]
                
                if source:
                    query += ' AND source = ?'
                    params.append(source)
                
                query += ' ORDER BY timestamp DESC'
                
                cursor.execute(query, params)
                results = cursor.fetchall()
                conn.close()
                
                return [
                    {
                        'timestamp': row[0],
                        'value': row[1],
                        'unit': row[2],
                        'source': row[3],
                        'metadata': json.loads(row[4]) if row[4] else None
                    }
                    for row in results
                ]
            
            def get_widget_data(self, widget_id):
                """ìœ„ì ¯ ë°ì´í„° ì¡°íšŒ"""
                if widget_id not in self.widgets:
                    return None
                
                widget_config = self.widgets[widget_id]
                widget_type = widget_config['type']
                
                if widget_type == 'metric_chart':
                    return self._get_metric_chart_data(widget_config)
                elif widget_type == 'stat_card':
                    return self._get_stat_card_data(widget_config)
                elif widget_type == 'alert_list':
                    return self._get_alert_list_data(widget_config)
                elif widget_type == 'system_status':
                    return self._get_system_status_data(widget_config)
                else:
                    return {'error': f'Unknown widget type: {widget_type}'}
            
            def _get_metric_chart_data(self, config):
                """ë©”íŠ¸ë¦­ ì°¨íŠ¸ ë°ì´í„°"""
                metric_name = config['metric_name']
                hours_back = config.get('hours_back', 1)
                
                metrics = self.get_metrics(metric_name, hours_back)
                
                return {
                    'type': 'metric_chart',
                    'title': config.get('title', metric_name),
                    'data_points': len(metrics),
                    'latest_value': metrics[0]['value'] if metrics else None,
                    'unit': metrics[0]['unit'] if metrics else None,
                    'metrics': metrics[:100]  # ìµœëŒ€ 100ê°œ í¬ì¸íŠ¸
                }
            
            def _get_stat_card_data(self, config):
                """í†µê³„ ì¹´ë“œ ë°ì´í„°"""
                metric_name = config['metric_name']
                hours_back = config.get('hours_back', 1)
                
                metrics = self.get_metrics(metric_name, hours_back)
                
                if not metrics:
                    return {
                        'type': 'stat_card',
                        'title': config.get('title', metric_name),
                        'value': 'N/A',
                        'unit': None
                    }
                
                values = [m['value'] for m in metrics]
                
                stat_type = config.get('stat_type', 'latest')
                if stat_type == 'latest':
                    value = values[0]
                elif stat_type == 'average':
                    value = sum(values) / len(values)
                elif stat_type == 'max':
                    value = max(values)
                elif stat_type == 'min':
                    value = min(values)
                else:
                    value = values[0]
                
                return {
                    'type': 'stat_card',
                    'title': config.get('title', metric_name),
                    'value': value,
                    'unit': metrics[0]['unit'],
                    'change': self._calculate_change(values) if len(values) > 1 else None
                }
            
            def _get_alert_list_data(self, config):
                """ì•Œë¦¼ ëª©ë¡ ë°ì´í„°"""
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                limit = config.get('limit', 10)
                severity_filter = config.get('severity_filter')
                
                query = 'SELECT * FROM alerts WHERE status = "active"'
                params = []
                
                if severity_filter:
                    query += ' AND severity = ?'
                    params.append(severity_filter)
                
                query += ' ORDER BY created_at DESC LIMIT ?'
                params.append(limit)
                
                cursor.execute(query, params)
                results = cursor.fetchall()
                conn.close()
                
                columns = ['id', 'alert_id', 'title', 'message', 'severity', 'status', 'created_at', 'updated_at', 'source']
                alerts = [dict(zip(columns, row)) for row in results]
                
                return {
                    'type': 'alert_list',
                    'title': config.get('title', 'Active Alerts'),
                    'alert_count': len(alerts),
                    'alerts': alerts
                }
            
            def _get_system_status_data(self, config):
                """ì‹œìŠ¤í…œ ìƒíƒœ ë°ì´í„°"""
                # ìµœê·¼ ë©”íŠ¸ë¦­ë“¤ë¡œë¶€í„° ì‹œìŠ¤í…œ ìƒíƒœ íŒë‹¨
                status_metrics = ['cpu_percent', 'memory_percent', 'disk_percent']
                system_status = {
                    'type': 'system_status',
                    'title': config.get('title', 'System Status'),
                    'overall_status': 'healthy',
                    'components': {}
                }
                
                for metric_name in status_metrics:
                    metrics = self.get_metrics(metric_name, hours_back=0.1)  # ìµœê·¼ 6ë¶„
                    
                    if metrics:
                        latest_value = metrics[0]['value']
                        
                        # ì„ê³„ê°’ ê¸°ë°˜ ìƒíƒœ íŒë‹¨
                        if metric_name == 'cpu_percent':
                            status = 'healthy' if latest_value < 80 else 'warning' if latest_value < 95 else 'critical'
                        elif metric_name in ['memory_percent', 'disk_percent']:
                            status = 'healthy' if latest_value < 85 else 'warning' if latest_value < 95 else 'critical'
                        else:
                            status = 'healthy'
                        
                        system_status['components'][metric_name] = {
                            'status': status,
                            'value': latest_value,
                            'unit': metrics[0]['unit']
                        }
                        
                        # ì „ì²´ ìƒíƒœ ì—…ë°ì´íŠ¸
                        if status == 'critical':
                            system_status['overall_status'] = 'critical'
                        elif status == 'warning' and system_status['overall_status'] != 'critical':
                            system_status['overall_status'] = 'warning'
                    
                    else:
                        system_status['components'][metric_name] = {
                            'status': 'unknown',
                            'value': None,
                            'unit': None
                        }
                
                return system_status
            
            def _calculate_change(self, values):
                """ë³€í™”ìœ¨ ê³„ì‚°"""
                if len(values) < 2:
                    return None
                
                current = values[0]
                previous = values[-1]
                
                if previous == 0:
                    return None
                
                change_percent = ((current - previous) / previous) * 100
                return {
                    'percent': change_percent,
                    'direction': 'up' if change_percent > 0 else 'down' if change_percent < 0 else 'stable'
                }
            
            def get_dashboard_layout(self):
                """ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ ë°˜í™˜"""
                layout = {
                    'title': 'Influence Item Monitoring Dashboard',
                    'last_updated': datetime.now().isoformat(),
                    'widgets': {}
                }
                
                for widget_id, widget_config in self.widgets.items():
                    widget_data = self.get_widget_data(widget_id)
                    layout['widgets'][widget_id] = {
                        'config': widget_config,
                        'data': widget_data,
                        'refresh_interval': self.refresh_intervals[widget_id]
                    }
                
                return layout
        
        # ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸
        dashboard = MonitoringDashboard()
        
        # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ì €ì¥
        test_metrics = [
            ('cpu_percent', 75.5, 'percent'),
            ('memory_percent', 68.2, 'percent'),
            ('disk_percent', 45.8, 'percent'),
            ('api_response_time', 1.2, 'seconds'),
            ('active_users', 125, 'count')
        ]
        
        for metric_name, value, unit in test_metrics:
            dashboard.store_metric(metric_name, value, unit)
        
        # ìœ„ì ¯ ì„¤ì •
        dashboard.add_widget('cpu_chart', {
            'type': 'metric_chart',
            'title': 'CPU Usage',
            'metric_name': 'cpu_percent',
            'hours_back': 1
        })
        
        dashboard.add_widget('memory_stat', {
            'type': 'stat_card',
            'title': 'Memory Usage',
            'metric_name': 'memory_percent',
            'stat_type': 'latest'
        })
        
        dashboard.add_widget('system_overview', {
            'type': 'system_status',
            'title': 'System Overview'
        })
        
        dashboard.add_widget('recent_alerts', {
            'type': 'alert_list',
            'title': 'Recent Alerts',
            'limit': 5
        })
        
        # ìœ„ì ¯ ë°ì´í„° í…ŒìŠ¤íŠ¸
        cpu_chart_data = dashboard.get_widget_data('cpu_chart')
        assert cpu_chart_data is not None, "CPU ì°¨íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        assert cpu_chart_data['type'] == 'metric_chart', "ì˜ëª»ëœ ìœ„ì ¯ íƒ€ì…"
        assert cpu_chart_data['latest_value'] == 75.5, "CPU ê°’ ë¶ˆì¼ì¹˜"
        
        memory_stat_data = dashboard.get_widget_data('memory_stat')
        assert memory_stat_data is not None, "ë©”ëª¨ë¦¬ í†µê³„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        assert memory_stat_data['type'] == 'stat_card', "ì˜ëª»ëœ ìœ„ì ¯ íƒ€ì…"
        assert memory_stat_data['value'] == 68.2, "ë©”ëª¨ë¦¬ ê°’ ë¶ˆì¼ì¹˜"
        
        system_status_data = dashboard.get_widget_data('system_overview')
        assert system_status_data is not None, "ì‹œìŠ¤í…œ ìƒíƒœ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        assert system_status_data['type'] == 'system_status', "ì˜ëª»ëœ ìœ„ì ¯ íƒ€ì…"
        assert 'components' in system_status_data, "ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì •ë³´ ëˆ„ë½"
        
        # ëŒ€ì‹œë³´ë“œ ë ˆì´ì•„ì›ƒ í…ŒìŠ¤íŠ¸
        layout = dashboard.get_dashboard_layout()
        assert 'widgets' in layout, "ìœ„ì ¯ ì •ë³´ ëˆ„ë½"
        assert len(layout['widgets']) == 4, f"ìœ„ì ¯ ìˆ˜ ë¶ˆì¼ì¹˜: {len(layout['widgets'])}"
        assert 'last_updated' in layout, "ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ëˆ„ë½"
        
        # ê° ìœ„ì ¯ì´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
        for widget_id, widget_info in layout['widgets'].items():
            assert 'data' in widget_info, f"{widget_id} ìœ„ì ¯ì— ë°ì´í„° ì—†ìŒ"
            assert 'config' in widget_info, f"{widget_id} ìœ„ì ¯ì— ì„¤ì • ì—†ìŒ"
        
        # ì •ë¦¬
        if os.path.exists(dashboard.db_path):
            os.remove(dashboard.db_path)
        
        print(f"  ğŸ“Š ëŒ€ì‹œë³´ë“œ ìœ„ì ¯: {len(layout['widgets'])}ê°œ")
        print(f"  ğŸ’¾ ì €ì¥ëœ ë©”íŠ¸ë¦­: {len(test_metrics)}ê°œ")
        
        print("âœ… ëŒ€ì‹œë³´ë“œ í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ëŒ€ì‹œë³´ë“œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """S03_M03_T01 í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ S03_M03_T01 ìš´ì˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    test_results = []
    start_time = time.time()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘", test_real_time_metrics),
        ("ì•Œë¦¼ ì‹œìŠ¤í…œ", test_alert_system),
        ("ë¡œê·¸ ë¶„ì„", test_log_analysis),
        ("ëŒ€ì‹œë³´ë“œ í†µí•©", test_dashboard_integration)
    ]
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        try:
            result = test_func()
            test_results.append((test_name, result))
            if result:
                print(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            test_results.append((test_name, False))
    
    # ê²°ê³¼ ìš”ì•½
    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ¯ S03_M03_T01 ìš´ì˜ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        print(f"  {status}: {test_name}")
    
    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: {passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼ ({passed_tests/total_tests*100:.1f}%)")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! S03_M03_T01 ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)