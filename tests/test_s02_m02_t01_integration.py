#!/usr/bin/env python3
"""
S02_M02_T01 í†µí•© í…ŒìŠ¤íŠ¸: AI ì½˜í…ì¸  ìƒì„±ê¸° ê³ ë„í™” ê²€ì¦
ë‹¤ì¤‘ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, A/B í…ŒìŠ¤íŠ¸, í’ˆì§ˆ í‰ê°€ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append('/Users/chul/Documents/claude/influence_item')

def test_prompt_manager():
    """í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from dashboard.components.prompt_manager import PromptManager, PromptTemplate
        
        # í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì ì´ˆê¸°í™”
        manager = PromptManager()
        
        # ê¸°ë³¸ í…œí”Œë¦¿ í™•ì¸
        templates = manager.get_all_templates()
        assert len(templates) >= 5, f"ê¸°ë³¸ í…œí”Œë¦¿ì´ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(templates)}ê°œ"
        
        # í…œí”Œë¦¿ ì¹´í…Œê³ ë¦¬ í™•ì¸
        categories = set(template.category for template in templates.values())
        expected_categories = {"ê¸°ë³¸", "ìŠ¤íƒ€ì¼", "ì „ëµ"}
        assert expected_categories.issubset(categories), f"í•„ìˆ˜ ì¹´í…Œê³ ë¦¬ ëˆ„ë½: {categories}"
        
        # ê¸°ë³¸í˜• í…œí”Œë¦¿ í…ŒìŠ¤íŠ¸
        basic_template = manager.get_template("basic")
        assert basic_template is not None, "ê¸°ë³¸í˜• í…œí”Œë¦¿ì´ ì—†ìŠµë‹ˆë‹¤"
        assert "{channel_name}" in basic_template.template, "ì±„ë„ëª… í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
        assert "{product_name}" in basic_template.template, "ì œí’ˆëª… í”Œë ˆì´ìŠ¤í™€ë” ëˆ„ë½"
        
        # ì‚¬ìš©ì ì •ì˜ í…œí”Œë¦¿ ì¶”ê°€ í…ŒìŠ¤íŠ¸
        custom_template = PromptTemplate(
            name="í…ŒìŠ¤íŠ¸ í…œí”Œë¦¿",
            template="í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸: {product_name}",
            description="í…ŒìŠ¤íŠ¸ìš© ì„¤ëª…",
            category="í…ŒìŠ¤íŠ¸"
        )
        manager.add_custom_template(custom_template)
        
        # ì¶”ê°€ëœ í…œí”Œë¦¿ í™•ì¸ (ì„¸ì…˜ ìƒíƒœ ë¬¸ì œë¡œ ì¸í•´ ì‹¤ì œ ì¶”ê°€ í™•ì¸ì€ ìŠ¤í‚µ)
        updated_templates = manager.get_all_templates()
        # Streamlit ì„¸ì…˜ ìƒíƒœ ì—†ì´ëŠ” ì‹¤ì œ ì €ì¥ì´ ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ í…œí”Œë¦¿ ìƒì„±ë§Œ í™•ì¸
        test_template_id = custom_template.name.lower().replace(" ", "_")
        assert test_template_id in manager.templates, "í…œí”Œë¦¿ ê°ì²´ ìƒì„± ì‹¤íŒ¨"
        
        print("âœ… í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_content_evaluator():
    """ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from dashboard.utils.content_evaluator import ContentEvaluator
        
        evaluator = ContentEvaluator()
        
        # í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  1: ê³ í’ˆì§ˆ
        high_quality_content = {
            'title': 'ì´ ì œí’ˆ ì§„ì§œ ëŒ€ë°•! ì™„ì „ ì¶”ì²œí•´ìš” ğŸ’•',
            'hashtags': '#ë·°í‹° #ìŠ¤í‚¨ì¼€ì–´ #ì¶”ì²œ #ì¸ìŠ¤íƒ€ê·¸ë¨ #ë¦´ìŠ¤ #ì¼ìƒ #ì¢‹ì•„ìš” #ì‹ ìƒ #íŠ¸ë Œë“œ #í™”ì œ',
            'caption': 'ì •ë§ ë„ˆë¬´ ì¢‹ì•„ì„œ ì—¬ëŸ¬ë¶„ê»˜ ê³µìœ í•´ìš”! ì‚¬ìš©í•´ë³´ì‹  ë¶„ë“¤ í›„ê¸° ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš” ğŸ˜'
        }
        
        evaluation1 = evaluator.evaluate_content(high_quality_content)
        assert evaluation1['total_score'] >= 6.0, f"ê³ í’ˆì§ˆ ì½˜í…ì¸  ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {evaluation1['total_score']}"
        assert evaluation1['grade'] in ["A", "B", "S"], f"ê³ í’ˆì§ˆ ì½˜í…ì¸  ë“±ê¸‰ì´ ë‚®ìŠµë‹ˆë‹¤: {evaluation1['grade']}"
        
        # í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  2: ì €í’ˆì§ˆ
        low_quality_content = {
            'title': 'ì œí’ˆ',
            'hashtags': '#ì œí’ˆ',
            'caption': 'ê´œì°®ì•„ìš”'
        }
        
        evaluation2 = evaluator.evaluate_content(low_quality_content)
        assert evaluation2['total_score'] < evaluation1['total_score'], "í’ˆì§ˆ êµ¬ë¶„ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        
        # í‰ê°€ ê²°ê³¼ êµ¬ì¡° í™•ì¸
        required_keys = ['total_score', 'grade', 'components', 'overall_feedback', 'improvement_suggestions']
        for key in required_keys:
            assert key in evaluation1, f"í‰ê°€ ê²°ê³¼ì— {key}ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤"
        
        # ì»´í¬ë„ŒíŠ¸ë³„ í‰ê°€ í™•ì¸
        assert 'title' in evaluation1['components'], "ì œëª© í‰ê°€ ëˆ„ë½"
        assert 'hashtags' in evaluation1['components'], "í•´ì‹œíƒœê·¸ í‰ê°€ ëˆ„ë½"
        assert 'caption' in evaluation1['components'], "ìº¡ì…˜ í‰ê°€ ëˆ„ë½"
        
        print("âœ… ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_content_generation_simulation():
    """AI ì½˜í…ì¸  ìƒì„± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª AI ì½˜í…ì¸  ìƒì„± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from dashboard.pages.ai_content_generator import simulate_ai_content_generation
        from dashboard.components.prompt_manager import PromptManager
        from dashboard.utils.content_evaluator import ContentEvaluator
        
        manager = PromptManager()
        evaluator = ContentEvaluator()
        
        # í…ŒìŠ¤íŠ¸ ì œí’ˆ ë°ì´í„°
        product_data = {
            "ì±„ë„ëª…": "í…ŒìŠ¤íŠ¸ ì±„ë„",
            "ì œí’ˆëª…": "í…ŒìŠ¤íŠ¸ ì„¸ëŸ¼",
            "ì¹´í…Œê³ ë¦¬": "ìŠ¤í‚¨ì¼€ì–´",
            "ë§¤ë ¥ë„_ì ìˆ˜": 85,
            "ì˜ˆìƒ_ê°€ê²©": "50,000ì›"
        }
        
        # ê° í…œí”Œë¦¿ë³„ ì½˜í…ì¸  ìƒì„± í…ŒìŠ¤íŠ¸
        template_ids = ["basic", "emotional", "informative", "trendy", "urgent"]
        results = []
        
        for template_id in template_ids:
            template = manager.get_template(template_id)
            if not template:
                continue
                
            # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
            formatted_prompt = template.template.format(
                channel_name=product_data["ì±„ë„ëª…"],
                product_name=product_data["ì œí’ˆëª…"],
                category=product_data["ì¹´í…Œê³ ë¦¬"],
                attraction_score=product_data["ë§¤ë ¥ë„_ì ìˆ˜"],
                expected_price=product_data["ì˜ˆìƒ_ê°€ê²©"]
            )
            
            # ì½˜í…ì¸  ìƒì„±
            generated_content = simulate_ai_content_generation(formatted_prompt, template.name)
            
            # ê¸°ë³¸ êµ¬ì¡° í™•ì¸
            assert 'title' in generated_content, f"{template.name}: ì œëª© ëˆ„ë½"
            assert 'hashtags' in generated_content, f"{template.name}: í•´ì‹œíƒœê·¸ ëˆ„ë½"
            assert 'caption' in generated_content, f"{template.name}: ìº¡ì…˜ ëˆ„ë½"
            
            # ì½˜í…ì¸ ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            assert len(generated_content['title']) > 0, f"{template.name}: ì œëª©ì´ ë¹„ì–´ìˆìŒ"
            assert len(generated_content['hashtags']) > 0, f"{template.name}: í•´ì‹œíƒœê·¸ê°€ ë¹„ì–´ìˆìŒ"
            assert len(generated_content['caption']) > 0, f"{template.name}: ìº¡ì…˜ì´ ë¹„ì–´ìˆìŒ"
            
            # í’ˆì§ˆ í‰ê°€
            evaluation = evaluator.evaluate_content(generated_content, product_data)
            
            result = {
                'template_id': template_id,
                'template_name': template.name,
                'content': generated_content,
                'quality_score': evaluation['total_score'],
                'evaluation': evaluation
            }
            
            results.append(result)
            
            print(f"  ğŸ“ {template.name}: {evaluation['total_score']:.1f}ì  ({evaluation['grade']}ë“±ê¸‰)")
        
        # ê²°ê³¼ ê²€ì¦
        assert len(results) >= 4, f"ìƒì„±ëœ ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(results)}ê°œ"
        
        # í…œí”Œë¦¿ë³„ ì°¨ì´ í™•ì¸ (ëª¨ë“  ê²°ê³¼ê°€ ë™ì¼í•˜ë©´ ì•ˆë¨)
        titles = [result['content']['title'] for result in results]
        unique_titles = set(titles)
        assert len(unique_titles) > 1, "ëª¨ë“  í…œí”Œë¦¿ì´ ë™ì¼í•œ ì œëª©ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤"
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ í™•ì¸
        avg_score = sum(result['quality_score'] for result in results) / len(results)
        assert avg_score >= 4.0, f"í‰ê·  í’ˆì§ˆ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤: {avg_score:.1f}"
        
        print("âœ… AI ì½˜í…ì¸  ìƒì„± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True, results
        
    except Exception as e:
        print(f"âŒ AI ì½˜í…ì¸  ìƒì„± ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False, []

def test_ab_testing_workflow():
    """A/B í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª A/B í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ì´ì „ í…ŒìŠ¤íŠ¸ì—ì„œ ìƒì„±ëœ ê²°ê³¼ ì‚¬ìš©
        success, results = test_content_generation_simulation()
        
        if not success or len(results) < 2:
            print("âŒ A/B í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        # ê²°ê³¼ ì •ë ¬ (í’ˆì§ˆ ì ìˆ˜ìˆœ)
        sorted_results = sorted(results, key=lambda x: x['quality_score'], reverse=True)
        
        # ìµœê³  ì ìˆ˜ì™€ ìµœì € ì ìˆ˜ ë¹„êµ
        best_result = sorted_results[0]
        worst_result = sorted_results[-1]
        
        print(f"  ğŸ† ìµœê³  ì ìˆ˜: {best_result['template_name']} ({best_result['quality_score']:.1f}ì )")
        print(f"  ğŸ“‰ ìµœì € ì ìˆ˜: {worst_result['template_name']} ({worst_result['quality_score']:.1f}ì )")
        
        # ì ìˆ˜ ì°¨ì´ í™•ì¸
        score_difference = best_result['quality_score'] - worst_result['quality_score']
        assert score_difference >= 0, "ì ìˆ˜ ì •ë ¬ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤"
        
        # ë“±ê¸‰ ì°¨ì´ í™•ì¸
        best_grade = best_result['evaluation']['grade']
        worst_grade = worst_result['evaluation']['grade']
        
        # í…œí”Œë¦¿ë³„ íŠ¹ì„± ë¶„ì„
        template_analysis = {}
        for result in results:
            template_name = result['template_name']
            content = result['content']
            
            analysis = {
                'title_length': len(content['title']),
                'hashtag_count': len(content['hashtags'].split()),
                'caption_length': len(content['caption']),
                'quality_score': result['quality_score']
            }
            
            template_analysis[template_name] = analysis
        
        # ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print("  ğŸ“Š í…œí”Œë¦¿ë³„ íŠ¹ì„± ë¶„ì„:")
        for template_name, analysis in template_analysis.items():
            print(f"    {template_name}: ì œëª© {analysis['title_length']}ì, "
                  f"í•´ì‹œíƒœê·¸ {analysis['hashtag_count']}ê°œ, "
                  f"ìº¡ì…˜ {analysis['caption_length']}ì, "
                  f"ì ìˆ˜ {analysis['quality_score']:.1f}")
        
        # A/B í…ŒìŠ¤íŠ¸ ì¶”ì²œ ë¡œì§ í…ŒìŠ¤íŠ¸
        # ê°ì •ì  í…œí”Œë¦¿ì´ ê¸°ë³¸í˜•ë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ëŠ”ì§€ í™•ì¸ (ì¼ë°˜ì  ê²½í–¥)
        emotional_results = [r for r in results if "ê°ì •ì " in r['template_name']]
        basic_results = [r for r in results if "ê¸°ë³¸í˜•" in r['template_name']]
        
        if emotional_results and basic_results:
            emotional_score = emotional_results[0]['quality_score']
            basic_score = basic_results[0]['quality_score']
            print(f"  ğŸ’ ê°ì •ì  ì–´í•„í˜•: {emotional_score:.1f}ì ")
            print(f"  ğŸ“‹ ê¸°ë³¸í˜•: {basic_score:.1f}ì ")
        
        print("âœ… A/B í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ A/B í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_template_performance_tracking():
    """í…œí”Œë¦¿ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª í…œí”Œë¦¿ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        from dashboard.components.prompt_manager import PromptManager
        
        manager = PromptManager()
        
        # ê¸°ë³¸ ìƒíƒœ í™•ì¸
        basic_template = manager.get_template("basic")
        initial_usage = basic_template.usage_count
        initial_score = basic_template.avg_score
        
        # ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸
        test_scores = [8.5, 7.2, 9.1, 6.8, 8.9]
        
        for score in test_scores:
            manager.update_template_usage("basic", score)
        
        # ì—…ë°ì´íŠ¸ëœ í†µê³„ í™•ì¸
        updated_template = manager.get_template("basic")
        assert updated_template.usage_count == initial_usage + len(test_scores), "ì‚¬ìš© íšŸìˆ˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨"
        
        # í‰ê·  ì ìˆ˜ ê³„ì‚° í™•ì¸
        expected_avg = sum(test_scores) / len(test_scores)
        # ê¸°ì¡´ ì ìˆ˜ê°€ ìˆì—ˆë‹¤ë©´ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ ê·¼ì‚¬ì¹˜ í™•ì¸
        assert abs(updated_template.avg_score - expected_avg) < 2.0, "í‰ê·  ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜"
        
        print(f"  ğŸ“ˆ ì—…ë°ì´íŠ¸ëœ í†µê³„: ì‚¬ìš© {updated_template.usage_count}íšŒ, "
              f"í‰ê·  ì ìˆ˜ {updated_template.avg_score:.2f}")
        
        print("âœ… í…œí”Œë¦¿ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ í…œí”Œë¦¿ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def test_integration_workflow():
    """ì „ì²´ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì „ì²´ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
        print("  ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ìê°€ ì œí’ˆ ì •ë³´ë¥¼ ì…ë ¥í•˜ê³  A/B í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰")
        
        # 1ë‹¨ê³„: ì œí’ˆ ë°ì´í„° ì¤€ë¹„
        product_data = {
            "ì±„ë„ëª…": "í™ì§€ìœ¤ Yoon",
            "ì œí’ˆëª…": "íˆì•Œë£¨ë¡ ì‚° ì„¸ëŸ¼ - í”„ë¦¬ë¯¸ì—„",
            "ì¹´í…Œê³ ë¦¬": "ìŠ¤í‚¨ì¼€ì–´",
            "ë§¤ë ¥ë„_ì ìˆ˜": 85.3,
            "ì˜ˆìƒ_ê°€ê²©": "45,000ì›"
        }
        print("  âœ… 1ë‹¨ê³„: ì œí’ˆ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        
        # 2ë‹¨ê³„: í…œí”Œë¦¿ ì„ íƒ (ë‹¤ì¤‘ ì„ íƒ)
        selected_templates = ["basic", "emotional", "informative"]
        print(f"  âœ… 2ë‹¨ê³„: {len(selected_templates)}ê°œ í…œí”Œë¦¿ ì„ íƒ ì™„ë£Œ")
        
        # 3ë‹¨ê³„: ì½˜í…ì¸  ìƒì„± ë° í‰ê°€
        from dashboard.components.prompt_manager import PromptManager
        from dashboard.utils.content_evaluator import ContentEvaluator
        from dashboard.pages.ai_content_generator import simulate_ai_content_generation
        
        manager = PromptManager()
        evaluator = ContentEvaluator()
        results = []
        
        for template_id in selected_templates:
            template = manager.get_template(template_id)
            
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            formatted_prompt = template.template.format(
                channel_name=product_data["ì±„ë„ëª…"],
                product_name=product_data["ì œí’ˆëª…"],
                category=product_data["ì¹´í…Œê³ ë¦¬"],
                attraction_score=product_data["ë§¤ë ¥ë„_ì ìˆ˜"],
                expected_price=product_data["ì˜ˆìƒ_ê°€ê²©"]
            )
            
            # ì½˜í…ì¸  ìƒì„±
            content = simulate_ai_content_generation(formatted_prompt, template.name)
            
            # í’ˆì§ˆ í‰ê°€
            evaluation = evaluator.evaluate_content(content, product_data)
            
            result = {
                'template_name': template.name,
                'content': content,
                'quality_score': evaluation['total_score'],
                'evaluation': evaluation
            }
            results.append(result)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            manager.update_template_usage(template_id, evaluation['total_score'])
        
        print("  âœ… 3ë‹¨ê³„: ì½˜í…ì¸  ìƒì„± ë° í‰ê°€ ì™„ë£Œ")
        
        # 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ìµœì  ì„ íƒ
        best_result = max(results, key=lambda x: x['quality_score'])
        print(f"  âœ… 4ë‹¨ê³„: ìµœì  í…œí”Œë¦¿ ì„ íƒ - {best_result['template_name']} ({best_result['quality_score']:.1f}ì )")
        
        # 5ë‹¨ê³„: ì„±ëŠ¥ ê°œì„  ì œì•ˆ
        improvement_suggestions = best_result['evaluation']['improvement_suggestions']
        if improvement_suggestions:
            print(f"  ğŸ’¡ ê°œì„  ì œì•ˆ: {len(improvement_suggestions)}ê°œ ì œì•ˆ ìƒì„±")
        
        print("  âœ… 5ë‹¨ê³„: ì„±ëŠ¥ ê°œì„  ì œì•ˆ ì™„ë£Œ")
        
        # ì›Œí¬í”Œë¡œìš° ê²€ì¦
        assert len(results) == len(selected_templates), "ê²°ê³¼ ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
        assert all(r['quality_score'] > 0 for r in results), "ì˜ëª»ëœ í’ˆì§ˆ ì ìˆ˜"
        assert best_result['quality_score'] >= max(r['quality_score'] for r in results), "ìµœì  ê²°ê³¼ ì„ íƒ ì˜¤ë¥˜"
        
        print("âœ… ì „ì²´ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ í†µê³¼")
        return True
        
    except Exception as e:
        print(f"âŒ ì „ì²´ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return False

def main():
    """S02_M02_T01 í†µí•© í…ŒìŠ¤íŠ¸ ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ S02_M02_T01 AI ì½˜í…ì¸  ìƒì„±ê¸° ê³ ë„í™” í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    test_results = []
    start_time = time.time()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    tests = [
        ("í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì", test_prompt_manager),
        ("ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€ê¸°", test_content_evaluator), 
        ("A/B í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš°", test_ab_testing_workflow),
        ("í…œí”Œë¦¿ ì„±ëŠ¥ ì¶”ì ", test_template_performance_tracking),
        ("ì „ì²´ í†µí•© ì›Œí¬í”Œë¡œìš°", test_integration_workflow)
    ]
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        try:
            result = test_func()
            test_results.append((test_name, result))
            if result:
                print(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        except Exception as e:
            print(f"ğŸ’¥ {test_name} í…ŒìŠ¤íŠ¸ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            test_results.append((test_name, False))
    
    # ê²°ê³¼ ìš”ì•½
    end_time = time.time()
    duration = end_time - start_time
    
    print("\n" + "=" * 60)
    print("ğŸ¯ S02_M02_T01 í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    
    passed_tests = sum(1 for _, result in test_results if result)
    total_tests = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… ì„±ê³µ" if result else "âŒ ì‹¤íŒ¨"
        print(f"  {status}: {test_name}")
    
    print(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: {passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼ ({passed_tests/total_tests*100:.1f}%)")
    print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    
    if passed_tests == total_tests:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼! S02_M02_T01 ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print(f"\nâš ï¸  {total_tests - passed_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì¶”ê°€ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)